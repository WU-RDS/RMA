<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>6 Supervised learning | Retail Marketing Analytics 2025</title>
  <meta name="description" content="An Introduction to Data Analytics Using R" />
  <meta name="generator" content="bookdown 0.39 and GitBook 2.6.7" />

  <meta property="og:title" content="6 Supervised learning | Retail Marketing Analytics 2025" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="An Introduction to Data Analytics Using R" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="6 Supervised learning | Retail Marketing Analytics 2025" />
  
  <meta name="twitter:description" content="An Introduction to Data Analytics Using R" />
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  <link rel="shortcut icon" href="favicon.ico" type="image/x-icon" />
<link rel="prev" href="statistical-inference.html"/>
<link rel="next" href="unsupervised-learning.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/pagedtable-1.1/css/pagedtable.css" rel="stylesheet" />
<script src="libs/pagedtable-1.1/js/pagedtable.js"></script>
<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><strong><a href="./">RMA 2025</a></strong></li>

<li class="divider"></li>
<li class="part"><span><b>I Course outline</b></span></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome!</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a>
<ul>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#course-structure"><i class="fa fa-check"></i>Course structure</a>
<ul>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#contents"><i class="fa fa-check"></i>Contents</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#grading"><i class="fa fa-check"></i>Grading</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#course-materials"><i class="fa fa-check"></i>Course materials</a>
<ul>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#main-reference"><i class="fa fa-check"></i>Main reference</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#further-readings"><i class="fa fa-check"></i>Further readings</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#discussion-forum"><i class="fa fa-check"></i>Discussion forum</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#other-web-resources"><i class="fa fa-check"></i>Other web-resources</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#contact"><i class="fa fa-check"></i>Contact</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>II Lecture notes</b></span></li>
<li class="chapter" data-level="1" data-path="preliminaries.html"><a href="preliminaries.html"><i class="fa fa-check"></i><b>1</b> Preliminaries</a>
<ul>
<li class="chapter" data-level="1.1" data-path="preliminaries.html"><a href="preliminaries.html#marketing-foundations-recap"><i class="fa fa-check"></i><b>1.1</b> Marketing foundations (recap)</a></li>
<li class="chapter" data-level="1.2" data-path="preliminaries.html"><a href="preliminaries.html#the-research-process"><i class="fa fa-check"></i><b>1.2</b> The research process</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="preliminaries.html"><a href="preliminaries.html#research-question-and-hypothesis"><i class="fa fa-check"></i><b>1.2.1</b> Research question and hypothesis</a></li>
<li class="chapter" data-level="1.2.2" data-path="preliminaries.html"><a href="preliminaries.html#choosing-a-research-design"><i class="fa fa-check"></i><b>1.2.2</b> Choosing a research design</a></li>
<li class="chapter" data-level="1.2.3" data-path="preliminaries.html"><a href="preliminaries.html#collecting-data"><i class="fa fa-check"></i><b>1.2.3</b> Collecting data</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="preliminaries.html"><a href="preliminaries.html#learning-check"><i class="fa fa-check"></i>Learning check</a></li>
<li class="chapter" data-level="" data-path="preliminaries.html"><a href="preliminaries.html#references"><i class="fa fa-check"></i>References</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="getting-started-with-r.html"><a href="getting-started-with-r.html"><i class="fa fa-check"></i><b>2</b> Getting started with R</a>
<ul>
<li class="chapter" data-level="2.1" data-path="getting-started-with-r.html"><a href="getting-started-with-r.html#how-to-download-and-install-r-and-rstudio"><i class="fa fa-check"></i><b>2.1</b> How to download and install R and RStudio</a></li>
<li class="chapter" data-level="2.2" data-path="getting-started-with-r.html"><a href="getting-started-with-r.html#the-r-studio-interface"><i class="fa fa-check"></i><b>2.2</b> The R Studio interface</a></li>
<li class="chapter" data-level="2.3" data-path="getting-started-with-r.html"><a href="getting-started-with-r.html#functions"><i class="fa fa-check"></i><b>2.3</b> Functions</a></li>
<li class="chapter" data-level="2.4" data-path="getting-started-with-r.html"><a href="getting-started-with-r.html#packages"><i class="fa fa-check"></i><b>2.4</b> Packages</a></li>
<li class="chapter" data-level="2.5" data-path="getting-started-with-r.html"><a href="getting-started-with-r.html#a-typical-r-session"><i class="fa fa-check"></i><b>2.5</b> A typical R session</a></li>
<li class="chapter" data-level="2.6" data-path="getting-started-with-r.html"><a href="getting-started-with-r.html#getting-help"><i class="fa fa-check"></i><b>2.6</b> Getting help</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="data-handling.html"><a href="data-handling.html"><i class="fa fa-check"></i><b>3</b> Data handling</a>
<ul>
<li class="chapter" data-level="3.1" data-path="data-handling.html"><a href="data-handling.html#basic-data-handling"><i class="fa fa-check"></i><b>3.1</b> Basic data handling</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="data-handling.html"><a href="data-handling.html#creating-objects"><i class="fa fa-check"></i><b>3.1.1</b> Creating objects</a></li>
<li class="chapter" data-level="3.1.2" data-path="data-handling.html"><a href="data-handling.html#data-types"><i class="fa fa-check"></i><b>3.1.2</b> Data types</a></li>
<li class="chapter" data-level="3.1.3" data-path="data-handling.html"><a href="data-handling.html#data-structures"><i class="fa fa-check"></i><b>3.1.3</b> Data structures</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="data-handling.html"><a href="data-handling.html#data-import-and-export"><i class="fa fa-check"></i><b>3.2</b> Data import and export</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="data-handling.html"><a href="data-handling.html#getting-data-for-this-course"><i class="fa fa-check"></i><b>3.2.1</b> Getting data for this course</a></li>
<li class="chapter" data-level="3.2.2" data-path="data-handling.html"><a href="data-handling.html#export-data"><i class="fa fa-check"></i><b>3.2.2</b> Export data</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="data-handling.html"><a href="data-handling.html#learning-check-1"><i class="fa fa-check"></i>Learning check</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="summarizing-data.html"><a href="summarizing-data.html"><i class="fa fa-check"></i><b>4</b> Summarizing data</a>
<ul>
<li class="chapter" data-level="4.1" data-path="summarizing-data.html"><a href="summarizing-data.html#summary-statistics"><i class="fa fa-check"></i><b>4.1</b> Summary statistics</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="summarizing-data.html"><a href="summarizing-data.html#categorical-variables"><i class="fa fa-check"></i><b>4.1.1</b> Categorical variables</a></li>
<li class="chapter" data-level="4.1.2" data-path="summarizing-data.html"><a href="summarizing-data.html#continuous-variables"><i class="fa fa-check"></i><b>4.1.2</b> Continuous variables</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="summarizing-data.html"><a href="summarizing-data.html#data-visualization"><i class="fa fa-check"></i><b>4.2</b> Data visualization</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="summarizing-data.html"><a href="summarizing-data.html#categorical-variables-1"><i class="fa fa-check"></i><b>4.2.1</b> Categorical variables</a></li>
<li class="chapter" data-level="4.2.2" data-path="summarizing-data.html"><a href="summarizing-data.html#continuous-variables-1"><i class="fa fa-check"></i><b>4.2.2</b> Continuous variables</a></li>
<li class="chapter" data-level="4.2.3" data-path="summarizing-data.html"><a href="summarizing-data.html#saving-plots"><i class="fa fa-check"></i><b>4.2.3</b> Saving plots</a></li>
<li class="chapter" data-level="4.2.4" data-path="summarizing-data.html"><a href="summarizing-data.html#ggplot-extensions"><i class="fa fa-check"></i><b>4.2.4</b> ggplot extensions</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="summarizing-data.html"><a href="summarizing-data.html#learning-check-2"><i class="fa fa-check"></i>Learning check</a></li>
<li class="chapter" data-level="" data-path="summarizing-data.html"><a href="summarizing-data.html#references-1"><i class="fa fa-check"></i>References</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="statistical-inference.html"><a href="statistical-inference.html"><i class="fa fa-check"></i><b>5</b> Statistical inference</a>
<ul>
<li class="chapter" data-level="5.1" data-path="statistical-inference.html"><a href="statistical-inference.html#if-we-knew-it-all"><i class="fa fa-check"></i><b>5.1</b> If we knew it all</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="statistical-inference.html"><a href="statistical-inference.html#sampling-from-a-known-population"><i class="fa fa-check"></i><b>5.1.1</b> Sampling from a known population</a></li>
<li class="chapter" data-level="5.1.2" data-path="statistical-inference.html"><a href="statistical-inference.html#standard-error-of-the-mean"><i class="fa fa-check"></i><b>5.1.2</b> Standard error of the mean</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="statistical-inference.html"><a href="statistical-inference.html#the-central-limit-theorem"><i class="fa fa-check"></i><b>5.2</b> The Central Limit Theorem</a></li>
<li class="chapter" data-level="5.3" data-path="statistical-inference.html"><a href="statistical-inference.html#using-what-we-actually-know"><i class="fa fa-check"></i><b>5.3</b> Using what we actually know</a></li>
<li class="chapter" data-level="5.4" data-path="statistical-inference.html"><a href="statistical-inference.html#confidence-intervals-for-the-sample-mean"><i class="fa fa-check"></i><b>5.4</b> Confidence Intervals for the Sample Mean</a></li>
<li class="chapter" data-level="5.5" data-path="statistical-inference.html"><a href="statistical-inference.html#null-hypothesis-statistical-testing-nhst"><i class="fa fa-check"></i><b>5.5</b> Null Hypothesis Statistical Testing (NHST)</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="statistical-inference.html"><a href="statistical-inference.html#the-null-hypothesis"><i class="fa fa-check"></i><b>5.5.1</b> The null hypothesis</a></li>
<li class="chapter" data-level="5.5.2" data-path="statistical-inference.html"><a href="statistical-inference.html#statistical-inference-on-a-sample"><i class="fa fa-check"></i><b>5.5.2</b> Statistical inference on a sample</a></li>
<li class="chapter" data-level="5.5.3" data-path="statistical-inference.html"><a href="statistical-inference.html#nhst-considerations"><i class="fa fa-check"></i><b>5.5.3</b> NHST considerations</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="statistical-inference.html"><a href="statistical-inference.html#learning-check-3"><i class="fa fa-check"></i>Learning check</a></li>
<li class="chapter" data-level="" data-path="statistical-inference.html"><a href="statistical-inference.html#references-2"><i class="fa fa-check"></i>References</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="supervised-learning.html"><a href="supervised-learning.html"><i class="fa fa-check"></i><b>6</b> Supervised learning</a>
<ul>
<li class="chapter" data-level="6.1" data-path="supervised-learning.html"><a href="supervised-learning.html#linear-regression"><i class="fa fa-check"></i><b>6.1</b> Linear regression</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="supervised-learning.html"><a href="supervised-learning.html#correlation"><i class="fa fa-check"></i><b>6.1.1</b> Correlation</a></li>
<li class="chapter" data-level="6.1.2" data-path="supervised-learning.html"><a href="supervised-learning.html#regression-analysis"><i class="fa fa-check"></i><b>6.1.2</b> Regression analysis</a></li>
<li class="chapter" data-level="6.1.3" data-path="supervised-learning.html"><a href="supervised-learning.html#categorical-predictors"><i class="fa fa-check"></i><b>6.1.3</b> Categorical predictors</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="supervised-learning.html"><a href="supervised-learning.html#logistic-regression"><i class="fa fa-check"></i><b>6.2</b> Logistic regression</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="supervised-learning.html"><a href="supervised-learning.html#motivation-and-intuition"><i class="fa fa-check"></i><b>6.2.1</b> Motivation and intuition</a></li>
<li class="chapter" data-level="6.2.2" data-path="supervised-learning.html"><a href="supervised-learning.html#technical-details-of-the-model"><i class="fa fa-check"></i><b>6.2.2</b> Technical details of the model</a></li>
<li class="chapter" data-level="6.2.3" data-path="supervised-learning.html"><a href="supervised-learning.html#estimation-in-r"><i class="fa fa-check"></i><b>6.2.3</b> Estimation in R</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="supervised-learning.html"><a href="supervised-learning.html#learning-check-4"><i class="fa fa-check"></i>Learning check</a></li>
<li class="chapter" data-level="" data-path="supervised-learning.html"><a href="supervised-learning.html#references-3"><i class="fa fa-check"></i>References</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html"><i class="fa fa-check"></i><b>7</b> Unsupervised learning</a>
<ul>
<li class="chapter" data-level="7.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#principal-component-analysis"><i class="fa fa-check"></i><b>7.1</b> Principal component analysis</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#introduction-1"><i class="fa fa-check"></i><b>7.1.1</b> Introduction</a></li>
<li class="chapter" data-level="7.1.2" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#steps-in-factor-analysis"><i class="fa fa-check"></i><b>7.1.2</b> Steps in factor analysis</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#cluster-analysis"><i class="fa fa-check"></i><b>7.2</b> Cluster analysis</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#k-means"><i class="fa fa-check"></i><b>7.2.1</b> K-Means</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#learning-check-5"><i class="fa fa-check"></i>Learning check</a></li>
<li class="chapter" data-level="" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#references-4"><i class="fa fa-check"></i>References</a></li>
</ul></li>
<li class="part"><span><b>III Assignments</b></span></li>
<li class="chapter" data-level="8" data-path="r-markdown.html"><a href="r-markdown.html"><i class="fa fa-check"></i><b>8</b> R Markdown</a>
<ul>
<li class="chapter" data-level="8.1" data-path="r-markdown.html"><a href="r-markdown.html#introduction-to-r-markdown"><i class="fa fa-check"></i><b>8.1</b> Introduction to R Markdown</a>
<ul>
<li class="chapter" data-level="" data-path="r-markdown.html"><a href="r-markdown.html#creating-a-new-r-markdown-document"><i class="fa fa-check"></i>Creating a new R Markdown document</a></li>
<li class="chapter" data-level="" data-path="r-markdown.html"><a href="r-markdown.html#text-and-equations"><i class="fa fa-check"></i>Text and Equations</a></li>
<li class="chapter" data-level="" data-path="r-markdown.html"><a href="r-markdown.html#r-code"><i class="fa fa-check"></i>R-Code</a></li>
<li class="chapter" data-level="" data-path="r-markdown.html"><a href="r-markdown.html#latex-math"><i class="fa fa-check"></i>LaTeX Math</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>IV FAQ</b></span></li>
<li class="chapter" data-level="9" data-path="faq.html"><a href="faq.html"><i class="fa fa-check"></i><b>9</b> FAQ</a>
<ul>
<li class="chapter" data-level="9.1" data-path="faq.html"><a href="faq.html#common-error-messages"><i class="fa fa-check"></i><b>9.1</b> Common error messages</a>
<ul>
<li class="chapter" data-level="" data-path="faq.html"><a href="faq.html#a-general-note-on-error-messages"><i class="fa fa-check"></i>A general note on error messages</a></li>
<li class="chapter" data-level="" data-path="faq.html"><a href="faq.html#error-in-filefile-rt-cannot-open-the-connection"><i class="fa fa-check"></i>Error in file(file, “rt”): cannot open the connection</a></li>
<li class="chapter" data-level="" data-path="faq.html"><a href="faq.html#error-unexpected-symbol-in-call"><i class="fa fa-check"></i>Error: unexpected ‘SYMBOL’ in “CALL”</a></li>
<li class="chapter" data-level="" data-path="faq.html"><a href="faq.html#error-in-call-object-of-type-closure-is-not-subsettable"><i class="fa fa-check"></i>Error in CALL : object of type ‘closure’ is not subsettable</a></li>
<li class="chapter" data-level="" data-path="faq.html"><a href="faq.html#error-in-call_with_-operator-is-invalid-for-atomic-vectors"><i class="fa fa-check"></i>Error in CALL_WITH_$: $ operator is invalid for atomic vectors</a></li>
<li class="chapter" data-level="" data-path="faq.html"><a href="faq.html#error-in-call-object-name-not-found"><i class="fa fa-check"></i>Error in CALL: object ‘NAME’ not found</a></li>
<li class="chapter" data-level="" data-path="faq.html"><a href="faq.html#error-in-call-could-not-find-function-function"><i class="fa fa-check"></i>Error in CALL: could not find function “FUNCTION”</a></li>
<li class="chapter" data-level="" data-path="faq.html"><a href="faq.html#error-in-call-incorrect-number-of-dimension"><i class="fa fa-check"></i>Error in CALL: incorrect number of dimension</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="faq.html"><a href="faq.html#installation-of-r-packages"><i class="fa fa-check"></i><b>9.2</b> Installation of R packages</a>
<ul>
<li class="chapter" data-level="" data-path="faq.html"><a href="faq.html#what-are-the-different-ways-to-install-r-packages"><i class="fa fa-check"></i>What are the different ways to install R packages?</a></li>
<li class="chapter" data-level="" data-path="faq.html"><a href="faq.html#i-cannot-install-packages-due-to-error-in-contrib.urlrepossource-or-warning-message-package-packagename-is-not-available-for-this-version-of-r"><i class="fa fa-check"></i>I cannot install packages due to “Error in contrib.url(repos,”source”)” or “Warning message: package ‘PACKAGENAME’ is not available for this version of R”</a></li>
<li class="chapter" data-level="" data-path="faq.html"><a href="faq.html#some-libraries-with-graphical-output-e.g.-summarytools-magick-fail-to-installload-properly-on-macos"><i class="fa fa-check"></i>Some libraries with graphical output (e.g., summarytools, magick) fail to install/load properly on MacOS</a></li>
<li class="chapter" data-level="" data-path="faq.html"><a href="faq.html#i-cannot-install-some-packages-on-macos"><i class="fa fa-check"></i>I cannot install some packages on MacOS</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="faq.html"><a href="faq.html#issues-with-statistics-and-data"><i class="fa fa-check"></i><b>9.3</b> Issues with statistics and data</a>
<ul>
<li class="chapter" data-level="" data-path="faq.html"><a href="faq.html#why-does-a-multi-item-scale-lead-to-increased-reliability"><i class="fa fa-check"></i>Why does a multi-item scale lead to increased reliability?</a></li>
<li class="chapter" data-level="" data-path="faq.html"><a href="faq.html#why-can-demeaningstandardization-lead-to-missing-values"><i class="fa fa-check"></i>Why can demeaning/standardization lead to missing values?</a></li>
<li class="chapter" data-level="" data-path="faq.html"><a href="faq.html#the-confidence-interval-ci-of-the-mean-seems-very-small-compared-to-the-dispersion-of-my-sample.-can-this-be-correct"><i class="fa fa-check"></i>The confidence interval (CI) of the mean seems very small compared to the dispersion of my sample. Can this be correct?</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="faq.html"><a href="faq.html#errors-related-to-specific-methods"><i class="fa fa-check"></i><b>9.4</b> Errors related to specific methods</a>
<ul>
<li class="chapter" data-level="" data-path="faq.html"><a href="faq.html#logistic-regression-1"><i class="fa fa-check"></i>Logistic regression</a></li>
<li class="chapter" data-level="" data-path="faq.html"><a href="faq.html#when-using-logistic-regression-error-in-weights-y-non-numeric-argument-to-binary-operator"><i class="fa fa-check"></i>When using logistic regression: Error in weights * y : non-numeric argument to binary operator</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="faq.html"><a href="faq.html#general-settings-and-options"><i class="fa fa-check"></i><b>9.5</b> General settings and options</a>
<ul>
<li class="chapter" data-level="" data-path="faq.html"><a href="faq.html#numbers-are-formatted-weirdly"><i class="fa fa-check"></i>Numbers are formatted weirdly</a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="faq.html"><a href="faq.html#data-visualizationoutput-issues"><i class="fa fa-check"></i><b>9.6</b> Data visualization/output issues</a>
<ul>
<li class="chapter" data-level="" data-path="faq.html"><a href="faq.html#how-can-the-geom-colors-in-a-ggplot-be-changed"><i class="fa fa-check"></i>How can the <code>geom</code> colors in a <code>ggplot</code> be changed?</a></li>
<li class="chapter" data-level="" data-path="faq.html"><a href="faq.html#why-are-some-histograms-displayed-differently"><i class="fa fa-check"></i>Why are some histograms displayed differently?</a></li>
<li class="chapter" data-level="" data-path="faq.html"><a href="faq.html#some-labels-in-plots-are-cut-off.-how-can-i-extend-the-plot-margins"><i class="fa fa-check"></i>Some labels in plots are cut off. How can I extend the plot margins?</a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="faq.html"><a href="faq.html#issues-with-functions-and-function-arguments"><i class="fa fa-check"></i><b>9.7</b> Issues with functions and function arguments</a>
<ul>
<li class="chapter" data-level="" data-path="faq.html"><a href="faq.html#problems-with-factor-and-as.factor"><i class="fa fa-check"></i>Problems with <code>factor</code> and <code>as.factor</code></a></li>
<li class="chapter" data-level="" data-path="faq.html"><a href="faq.html#how-can-i-find-an-explanation-of-the-output-of-a-function"><i class="fa fa-check"></i>How can I find an explanation of the output of a function?</a></li>
<li class="chapter" data-level="" data-path="faq.html"><a href="faq.html#what-does-the-margin-argument-do"><i class="fa fa-check"></i>What does the <code>MARGIN</code> argument do?</a></li>
</ul></li>
<li class="chapter" data-level="9.8" data-path="faq.html"><a href="faq.html#issues-with-r-markdown"><i class="fa fa-check"></i><b>9.8</b> Issues with R Markdown</a>
<ul>
<li class="chapter" data-level="" data-path="faq.html"><a href="faq.html#i-get-an-error-when-knitting-to-pdf-but-it-works-for-html"><i class="fa fa-check"></i>I get an error when <code>knit</code>ting to PDF but it works for HTML</a></li>
<li class="chapter" data-level="" data-path="faq.html"><a href="faq.html#i-am-not-sure-where-r-code-latex-math-and-text-go"><i class="fa fa-check"></i>I am not sure where R-code, LaTeX math, and text go</a></li>
</ul></li>
<li class="chapter" data-level="9.9" data-path="faq.html"><a href="faq.html#new-questions"><i class="fa fa-check"></i><b>9.9</b> New questions</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Retail Marketing Analytics 2025</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="supervised-learning" class="section level1 hasAnchor" number="6">
<h1><span class="header-section-number">6</span> Supervised learning<a href="supervised-learning.html#supervised-learning" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="linear-regression" class="section level2 hasAnchor" number="6.1">
<h2><span class="header-section-number">6.1</span> Linear regression<a href="supervised-learning.html#linear-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="correlation" class="section level3 hasAnchor" number="6.1.1">
<h3><span class="header-section-number">6.1.1</span> Correlation<a href="supervised-learning.html#correlation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Before we start with regression analysis, we will review the basic concept of correlation first. Correlation helps us to determine the degree to which the variation in one variable, X, is related to the variation in another variable, Y.</p>
<div id="correlation-coefficient" class="section level4 hasAnchor" number="6.1.1.1">
<h4><span class="header-section-number">6.1.1.1</span> Correlation coefficient<a href="supervised-learning.html#correlation-coefficient" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The correlation coefficient summarizes the strength of the linear relationship between two metric (interval or ratio scaled) variables. Let’s consider a simple example. Say you conduct a survey to investigate the relationship between the attitude towards a shop and the duration of being of its customer. The “Attitude” variable can take values between 1 (very unfavorable) and 12 (very favorable), and the “Duration” is measured in months. Let’s further assume for this example that the attitude measurement represents an interval scale (although it is usually not realistic to assume that the scale points on an itemized rating scale have the same distance). To keep it simple, let’s further assume that you only asked 12 people. We can create a short data set like this:</p>
<div class="sourceCode" id="cb207"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb207-1"><a href="supervised-learning.html#cb207-1" tabindex="-1"></a><span class="fu">library</span>(psych)</span>
<span id="cb207-2"><a href="supervised-learning.html#cb207-2" tabindex="-1"></a>attitude <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">6</span>, <span class="dv">9</span>, <span class="dv">8</span>, <span class="dv">3</span>, <span class="dv">10</span>, <span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">2</span>, <span class="dv">11</span>, <span class="dv">9</span>, <span class="dv">10</span>, <span class="dv">2</span>)</span>
<span id="cb207-3"><a href="supervised-learning.html#cb207-3" tabindex="-1"></a>duration <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">10</span>, <span class="dv">12</span>, <span class="dv">12</span>, <span class="dv">4</span>, <span class="dv">12</span>, <span class="dv">6</span>, <span class="dv">8</span>, <span class="dv">2</span>, <span class="dv">18</span>, <span class="dv">9</span>, <span class="dv">17</span>,</span>
<span id="cb207-4"><a href="supervised-learning.html#cb207-4" tabindex="-1"></a>    <span class="dv">2</span>)</span>
<span id="cb207-5"><a href="supervised-learning.html#cb207-5" tabindex="-1"></a>att_data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(attitude, duration)</span>
<span id="cb207-6"><a href="supervised-learning.html#cb207-6" tabindex="-1"></a>att_data <span class="ot">&lt;-</span> att_data[<span class="fu">order</span>(<span class="sc">-</span>attitude), ]</span>
<span id="cb207-7"><a href="supervised-learning.html#cb207-7" tabindex="-1"></a>att_data<span class="sc">$</span>respodentID <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">12</span>)</span>
<span id="cb207-8"><a href="supervised-learning.html#cb207-8" tabindex="-1"></a><span class="fu">str</span>(att_data)</span></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    12 obs. of  3 variables:
##  $ attitude   : num  11 10 10 9 9 8 6 5 4 3 ...
##  $ duration   : num  18 12 17 12 9 12 10 8 6 4 ...
##  $ respodentID: int  1 2 3 4 5 6 7 8 9 10 ...</code></pre>
<div class="sourceCode" id="cb209"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb209-1"><a href="supervised-learning.html#cb209-1" tabindex="-1"></a>psych<span class="sc">::</span><span class="fu">describe</span>(att_data[, <span class="fu">c</span>(<span class="st">&quot;attitude&quot;</span>, <span class="st">&quot;duration&quot;</span>)])</span></code></pre></div>
<pre><code>##          vars  n mean   sd median trimmed  mad min max range  skew kurtosis
## attitude    1 12 6.58 3.32    7.0     6.6 4.45   2  11     9 -0.14    -1.74
## duration    2 12 9.33 5.26    9.5     9.2 4.45   2  18    16  0.10    -1.27
##            se
## attitude 0.96
## duration 1.52</code></pre>
<div class="sourceCode" id="cb211"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb211-1"><a href="supervised-learning.html#cb211-1" tabindex="-1"></a>att_data</span></code></pre></div>
<pre><code>##    attitude duration respodentID
## 9        11       18           1
## 5        10       12           2
## 11       10       17           3
## 2         9       12           4
## 10        9        9           5
## 3         8       12           6
## 1         6       10           7
## 7         5        8           8
## 6         4        6           9
## 4         3        4          10
## 8         2        2          11
## 12        2        2          12</code></pre>
<p>Let’s look at the data first. The following graph shows the individual data points for the “duration” variable, where the y-axis shows the duration of residency in years and the x-axis shows the respondent ID. The blue horizontal line represents the mean of the variable (9.33) and the vertical lines show the distance of the individual data points from the mean.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-4"></span>
<img src="07-supervised_learning_files/figure-html/unnamed-chunk-4-1.png" alt="Scores for duration variable" width="672" />
<p class="caption">
Figure 1.3: Scores for duration variable
</p>
</div>
<p>You can see that there are some respondents that have been the store’s customers longer than average and some - shorter than average. Let’s do the same for the second variable (“Attitude”). Again, the y-axis shows the observed scores for this variable and the x-axis shows the respondent ID.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-5"></span>
<img src="07-supervised_learning_files/figure-html/unnamed-chunk-5-1.png" alt="Scores for attitude variable" width="672" />
<p class="caption">
Figure 1.4: Scores for attitude variable
</p>
</div>
<p>Again, we can see that some respondents have an above average attitude towards the store (more favorable) and some respondents have a below average attitude. Let’s combine both variables in one graph now to see if there is some co-movement:</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-6"></span>
<img src="07-supervised_learning_files/figure-html/unnamed-chunk-6-1.png" alt="Scores for attitude and duration variables" width="672" />
<p class="caption">
Figure 5.1: Scores for attitude and duration variables
</p>
</div>
<p>We can see that there is indeed some co-movement here. The variables <b>covary</b> because respondents who have an above (below) average attitude towards the store also appear to have been its customers for an above (below) average amount of time and vice versa. Correlation helps us to quantify this relationship. Before you proceed to compute the correlation coefficient, you should first look at the data. We usually use a scatterplot to visualize the relationship between two metric variables:</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-7"></span>
<img src="07-supervised_learning_files/figure-html/unnamed-chunk-7-1.png" alt="Scatterplot for durationand attitute variables" width="672" />
<p class="caption">
Figure 1.5: Scatterplot for durationand attitute variables
</p>
</div>
<p>How can we compute the correlation coefficient? Remember that the variance measures the average deviation from the mean of a variable:</p>
<p><span class="math display" id="eq:variance">\[\begin{equation}
\begin{split}
s_x^2&amp;=\frac{\sum_{i=1}^{N} (X_i-\overline{X})^2}{N-1} \\
     &amp;= \frac{\sum_{i=1}^{N} (X_i-\overline{X})*(X_i-\overline{X})}{N-1}
\end{split}
\tag{6.1}
\end{equation}\]</span></p>
<p>When we consider two variables, we multiply the deviation for one variable by the respective deviation for the second variable:</p>
<p style="text-align:center;">
<span class="math inline">\((X_i-\overline{X})*(Y_i-\overline{Y})\)</span>
</p>
<p>This is called the cross-product deviation. Then we sum the cross-product deviations:</p>
<p style="text-align:center;">
<span class="math inline">\(\sum_{i=1}^{N}(X_i-\overline{X})*(Y_i-\overline{Y})\)</span>
</p>
<p>… and compute the average of the sum of all cross-product deviations to get the <b>covariance</b>:</p>
<p><span class="math display" id="eq:covariance">\[\begin{equation}
Cov(x, y) =\frac{\sum_{i=1}^{N}(X_i-\overline{X})*(Y_i-\overline{Y})}{N-1}
\tag{6.2}
\end{equation}\]</span></p>
<p>You can easily compute the covariance manually as follows</p>
<div class="sourceCode" id="cb213"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb213-1"><a href="supervised-learning.html#cb213-1" tabindex="-1"></a>x <span class="ot">&lt;-</span> att_data<span class="sc">$</span>duration</span>
<span id="cb213-2"><a href="supervised-learning.html#cb213-2" tabindex="-1"></a>x_bar <span class="ot">&lt;-</span> <span class="fu">mean</span>(att_data<span class="sc">$</span>duration)</span>
<span id="cb213-3"><a href="supervised-learning.html#cb213-3" tabindex="-1"></a>y <span class="ot">&lt;-</span> att_data<span class="sc">$</span>attitude</span>
<span id="cb213-4"><a href="supervised-learning.html#cb213-4" tabindex="-1"></a>y_bar <span class="ot">&lt;-</span> <span class="fu">mean</span>(att_data<span class="sc">$</span>attitude)</span>
<span id="cb213-5"><a href="supervised-learning.html#cb213-5" tabindex="-1"></a>N <span class="ot">&lt;-</span> <span class="fu">nrow</span>(att_data)</span>
<span id="cb213-6"><a href="supervised-learning.html#cb213-6" tabindex="-1"></a>cov <span class="ot">&lt;-</span> (<span class="fu">sum</span>((x <span class="sc">-</span> x_bar) <span class="sc">*</span> (y <span class="sc">-</span> y_bar)))<span class="sc">/</span>(N <span class="sc">-</span> <span class="dv">1</span>)</span>
<span id="cb213-7"><a href="supervised-learning.html#cb213-7" tabindex="-1"></a>cov</span></code></pre></div>
<pre><code>## [1] 16.333333</code></pre>
<p>Or you simply use the built-in <code>cov()</code> function:</p>
<div class="sourceCode" id="cb215"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb215-1"><a href="supervised-learning.html#cb215-1" tabindex="-1"></a><span class="fu">cov</span>(att_data<span class="sc">$</span>duration, att_data<span class="sc">$</span>attitude)  <span class="co"># apply the cov function </span></span></code></pre></div>
<pre><code>## [1] 16.333333</code></pre>
<p>A positive covariance indicates that as one variable deviates from the mean, the other variable deviates in the same direction. A negative covariance indicates that as one variable deviates from the mean (e.g., increases), the other variable deviates in the opposite direction (e.g., decreases).</p>
<p>However, the size of the covariance depends on the scale of measurement. Larger scale units will lead to larger covariance. To overcome the problem of dependence on measurement scale, we need to convert the covariance to a standard set of units through standardization by dividing the covariance by the standard deviation (similar to how we compute z-scores).</p>
<p>With two variables, there are two standard deviations. We simply multiply the two standard deviations. We then divide the covariance by the product of the two standard deviations to get the standardized covariance, which is known as a correlation coefficient r:</p>
<p><span class="math display" id="eq:corcoeff">\[\begin{equation}
r=\frac{Cov_{xy}}{s_x*s_y}
\tag{6.3}
\end{equation}\]</span></p>
<p>This is known as the product moment correlation (r) and it is straight-forward to compute:</p>
<div class="sourceCode" id="cb217"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb217-1"><a href="supervised-learning.html#cb217-1" tabindex="-1"></a>x_sd <span class="ot">&lt;-</span> <span class="fu">sd</span>(att_data<span class="sc">$</span>duration)</span>
<span id="cb217-2"><a href="supervised-learning.html#cb217-2" tabindex="-1"></a>y_sd <span class="ot">&lt;-</span> <span class="fu">sd</span>(att_data<span class="sc">$</span>attitude)</span>
<span id="cb217-3"><a href="supervised-learning.html#cb217-3" tabindex="-1"></a>r <span class="ot">&lt;-</span> cov<span class="sc">/</span>(x_sd <span class="sc">*</span> y_sd)</span>
<span id="cb217-4"><a href="supervised-learning.html#cb217-4" tabindex="-1"></a>r</span></code></pre></div>
<pre><code>## [1] 0.93607782</code></pre>
<p>Or you could just use the <code>cor()</code> function:</p>
<div class="sourceCode" id="cb219"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb219-1"><a href="supervised-learning.html#cb219-1" tabindex="-1"></a><span class="fu">cor</span>(att_data[, <span class="fu">c</span>(<span class="st">&quot;attitude&quot;</span>, <span class="st">&quot;duration&quot;</span>)], <span class="at">method =</span> <span class="st">&quot;pearson&quot;</span>,</span>
<span id="cb219-2"><a href="supervised-learning.html#cb219-2" tabindex="-1"></a>    <span class="at">use =</span> <span class="st">&quot;complete&quot;</span>)</span></code></pre></div>
<pre><code>##            attitude   duration
## attitude 1.00000000 0.93607782
## duration 0.93607782 1.00000000</code></pre>
<p>The properties of the correlation coefficient (‘r’) are:</p>
<ul>
<li>ranges from -1 to + 1</li>
<li>+1 indicates perfect linear relationship</li>
<li>-1 indicates perfect negative relationship</li>
<li>0 indicates no linear relationship</li>
<li>± .1 represents small effect</li>
<li>± .3 represents medium effect</li>
<li>± .5 represents large effect</li>
</ul>
</div>
<div id="significance-testing" class="section level4 hasAnchor" number="6.1.1.2">
<h4><span class="header-section-number">6.1.1.2</span> Significance testing<a href="supervised-learning.html#significance-testing" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>How can we determine if our two variables are significantly related? To test this, we denote the population moment correlation <em>ρ</em>. Then we test the null of no relationship between variables:</p>
<p><span class="math display">\[H_0:\rho=0\]</span>
<span class="math display">\[H_1:\rho\ne0\]</span></p>
<p>The test statistic is:</p>
<p><span class="math display" id="eq:cortest">\[\begin{equation}
t=\frac{r*\sqrt{N-2}}{\sqrt{1-r^2}}
\tag{6.4}
\end{equation}\]</span></p>
<p>It has a t distribution with n - 2 degrees of freedom. You can simply use the <code>cor.test()</code> function, which also produces the 95% confidence interval:</p>
<div class="sourceCode" id="cb221"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb221-1"><a href="supervised-learning.html#cb221-1" tabindex="-1"></a><span class="fu">cor.test</span>(att_data<span class="sc">$</span>attitude, att_data<span class="sc">$</span>duration, <span class="at">alternative =</span> <span class="st">&quot;two.sided&quot;</span>,</span>
<span id="cb221-2"><a href="supervised-learning.html#cb221-2" tabindex="-1"></a>    <span class="at">method =</span> <span class="st">&quot;pearson&quot;</span>, <span class="at">conf.level =</span> <span class="fl">0.95</span>)</span></code></pre></div>
<pre><code>## 
##  Pearson&#39;s product-moment correlation
## 
## data:  att_data$attitude and att_data$duration
## t = 8.4144, df = 10, p-value = 0.000007545
## alternative hypothesis: true correlation is not equal to 0
## 95 percent confidence interval:
##  0.7826041 0.9822815
## sample estimates:
##       cor 
## 0.9360778</code></pre>
<p>To determine the linear relationship between variables, the data only needs to be measured using interval scales. If you want to test the significance of the association, the sampling distribution needs to be normally distributed (we usually assume this when our data are normally distributed or when N is large). If parametric assumptions are violated, you should use non-parametric tests:</p>
<ul>
<li>Spearman’s correlation coefficient: requires ordinal data and ranks the data before applying Pearson’s equation.</li>
<li>Kendall’s tau: use when N is small or the number of tied ranks is large.</li>
</ul>
<div class="sourceCode" id="cb223"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb223-1"><a href="supervised-learning.html#cb223-1" tabindex="-1"></a><span class="fu">cor.test</span>(att_data<span class="sc">$</span>attitude, att_data<span class="sc">$</span>duration, <span class="at">alternative =</span> <span class="st">&quot;two.sided&quot;</span>,</span>
<span id="cb223-2"><a href="supervised-learning.html#cb223-2" tabindex="-1"></a>    <span class="at">method =</span> <span class="st">&quot;spearman&quot;</span>, <span class="at">conf.level =</span> <span class="fl">0.95</span>)</span></code></pre></div>
<pre><code>## 
##  Spearman&#39;s rank correlation rho
## 
## data:  att_data$attitude and att_data$duration
## S = 14.197, p-value = 0.000002183
## alternative hypothesis: true rho is not equal to 0
## sample estimates:
##       rho 
## 0.9503606</code></pre>
<div class="sourceCode" id="cb225"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb225-1"><a href="supervised-learning.html#cb225-1" tabindex="-1"></a><span class="fu">cor.test</span>(att_data<span class="sc">$</span>attitude, att_data<span class="sc">$</span>duration, <span class="at">alternative =</span> <span class="st">&quot;two.sided&quot;</span>,</span>
<span id="cb225-2"><a href="supervised-learning.html#cb225-2" tabindex="-1"></a>    <span class="at">method =</span> <span class="st">&quot;kendall&quot;</span>, <span class="at">conf.level =</span> <span class="fl">0.95</span>)</span></code></pre></div>
<pre><code>## 
##  Kendall&#39;s rank correlation tau
## 
## data:  att_data$attitude and att_data$duration
## z = 3.9095, p-value = 0.0000925
## alternative hypothesis: true tau is not equal to 0
## sample estimates:
##       tau 
## 0.8960287</code></pre>
<p>Report the results:</p>
<p>A Pearson product-moment correlation coefficient was computed to assess the relationship between the duration of being a customer of a store and the attitude toward this store. There was a positive correlation between the two variables, r = 0.936, n = 12, p &lt; 0.05. A scatterplot summarizes the results (Figure XY).</p>
<p><strong>A note on the interpretation of correlation coefficients:</strong></p>
<p>As we have already seen in Chapter 1, correlation coefficients give no indication of the direction of causality. In our example, we can conclude that the attitude toward the store is more positive as the months of being a customer increase. However, we cannot say that the duration causes the attitudes to be more positive. There are two main reasons for caution when interpreting correlations:</p>
<ul>
<li>Third-variable problem: there may be other unobserved factors that affect both the ‘attitude’ and the ‘duration’ variables</li>
<li>Direction of causality: Correlations say nothing about which variable causes the other to change (reverse causality: attitudes may just as well cause the duration variable).</li>
</ul>
</div>
</div>
<div id="regression-analysis" class="section level3 hasAnchor" number="6.1.2">
<h3><span class="header-section-number">6.1.2</span> Regression analysis<a href="supervised-learning.html#regression-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Correlations measure relationships between variables (i.e., how much two variables covary). Using regression analysis we can predict the outcome of a dependent variable (Y) from one or more independent variables (X). For example, we could be interested in how many products will we will sell if we increase the advertising expenditures by 1000 Euros? In regression analysis, we fit a model to our data and use it to predict the values of the dependent variable from one predictor variable (bivariate regression) or several predictor variables (multiple regression). The following table shows a comparison of correlation and regression analysis:</p>
<p><br></p>
<table>
<colgroup>
<col width="20%" />
<col width="40%" />
<col width="40%" />
</colgroup>
<thead>
<tr class="header">
<th> </th>
<th>Correlation</th>
<th>Regression</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Estimated coefficient</td>
<td>Coefficient of correlation (bounded between -1 and +1)</td>
<td>Regression coefficient (not bounded a priori)</td>
</tr>
<tr class="even">
<td>Interpretation</td>
<td>Linear association between two variables; Association is bidirectional</td>
<td>(Linear) relation between one or more independent variables and dependent variable; Relation is directional</td>
</tr>
<tr class="odd">
<td>Role of theory</td>
<td>Theory neither required nor testable</td>
<td>Theory required and testable</td>
</tr>
</tbody>
</table>
<p><br></p>
<div id="simple-linear-regression" class="section level4 hasAnchor" number="6.1.2.1">
<h4><span class="header-section-number">6.1.2.1</span> Simple linear regression<a href="supervised-learning.html#simple-linear-regression" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>In simple linear regression, we assess the relationship between one dependent (regressand) and one independent (regressor) variable. The goal is to fit a line through a scatterplot of observations in order to find the line that best describes the data (scatterplot).</p>
<p>Suppose you are a marketing research analyst at a big retail group and your task is to suggest, on the basis of historical data, a marketing plan for the next year that will maximize product sales. The product in question is beer brand Budweiser. The data set that is available to you includes information on the sales of Budweiser <em>move_ounce</em> (in ounces, FYI: 1 oz = 29,57 ml), prices <em>price_ounce</em> (in dollars per ounce), and several other variables: bonus buy - a price reduction if customers buy a certain quantity of a product (<em>sale_B</em>), price reduction in % (<em>sale_S</em>), and others. Let’s load and inspect the data first:</p>
<div class="sourceCode" id="cb227"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb227-1"><a href="supervised-learning.html#cb227-1" tabindex="-1"></a>regression <span class="ot">&lt;-</span> <span class="fu">read.table</span>(<span class="st">&quot;https://raw.githubusercontent.com/dariayudaeva/RMA2024/main/data/bud_store102.csv&quot;</span>,</span>
<span id="cb227-2"><a href="supervised-learning.html#cb227-2" tabindex="-1"></a>    <span class="at">sep =</span> <span class="st">&quot;,&quot;</span>, <span class="at">header =</span> <span class="cn">TRUE</span>)  <span class="co"># read in data</span></span>
<span id="cb227-3"><a href="supervised-learning.html#cb227-3" tabindex="-1"></a><span class="fu">str</span>(regression)</span></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    220 obs. of  22 variables:
##  $ store           : int  102 102 102 102 102 102 102 102 102 102 ...
##  $ brand_id        : int  26 26 26 26 26 26 26 26 26 26 ...
##  $ brand           : chr  &quot;Budweiser&quot; &quot;Budweiser&quot; &quot;Budweiser&quot; &quot;Budweiser&quot; ...
##  $ week            : int  91 92 93 94 95 96 97 98 99 100 ...
##  $ move_ounce      : num  41704 44212 28748 26169 35581 ...
##  $ price_ounce     : num  4.67 4.58 4.58 4.53 4.54 ...
##  $ sale_B          : num  0.133 0.133 0.125 0 0 ...
##  $ sale_C          : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ sale_S          : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ summove_ounce   : num  158871900 158871900 158871900 158871900 158871900 ...
##  $ nweeks          : int  220 220 220 220 220 220 220 220 220 220 ...
##  $ mean_marketshare: num  0.0893 0.0893 0.0893 0.0893 0.0893 ...
##  $ sharerank       : int  3 3 3 3 3 3 3 3 3 3 ...
##  $ priclow         : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ pricmed         : int  1 1 1 1 1 1 1 1 1 1 ...
##  $ prichigh        : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ logprice_ounce  : num  1.54 1.52 1.52 1.51 1.51 ...
##  $ logmove_ounce   : num  10.6 10.7 10.3 10.2 10.5 ...
##  $ saledummy_B     : int  1 1 1 0 0 0 0 0 0 0 ...
##  $ saledummy_C     : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ saledummy_S     : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ promoweek       : int  91 92 93 NA NA NA NA NA NA NA ...</code></pre>
<div class="sourceCode" id="cb229"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb229-1"><a href="supervised-learning.html#cb229-1" tabindex="-1"></a>regression<span class="sc">$</span>store <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(regression<span class="sc">$</span>store)  <span class="co">#convert grouping variable to factor</span></span>
<span id="cb229-2"><a href="supervised-learning.html#cb229-2" tabindex="-1"></a>regression<span class="sc">$</span>brand_id <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(regression<span class="sc">$</span>brand_id)  <span class="co">#convert grouping variable to factor</span></span>
<span id="cb229-3"><a href="supervised-learning.html#cb229-3" tabindex="-1"></a>regression<span class="sc">$</span>saledummy_B <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(regression<span class="sc">$</span>saledummy_B)  <span class="co">#convert grouping variable to factor</span></span>
<span id="cb229-4"><a href="supervised-learning.html#cb229-4" tabindex="-1"></a>regression<span class="sc">$</span>saledummy_C <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(regression<span class="sc">$</span>saledummy_C)  <span class="co">#convert grouping variable to factor</span></span>
<span id="cb229-5"><a href="supervised-learning.html#cb229-5" tabindex="-1"></a>regression<span class="sc">$</span>saledummy_S <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(regression<span class="sc">$</span>saledummy_S)  <span class="co">#convert grouping variable to factor</span></span>
<span id="cb229-6"><a href="supervised-learning.html#cb229-6" tabindex="-1"></a><span class="fu">head</span>(regression)</span></code></pre></div>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["store"],"name":[1],"type":["fct"],"align":["left"]},{"label":["brand_id"],"name":[2],"type":["fct"],"align":["left"]},{"label":["brand"],"name":[3],"type":["chr"],"align":["left"]},{"label":["week"],"name":[4],"type":["int"],"align":["right"]},{"label":["move_ounce"],"name":[5],"type":["dbl"],"align":["right"]},{"label":["price_ounce"],"name":[6],"type":["dbl"],"align":["right"]},{"label":["sale_B"],"name":[7],"type":["dbl"],"align":["right"]},{"label":["sale_C"],"name":[8],"type":["int"],"align":["right"]},{"label":["sale_S"],"name":[9],"type":["dbl"],"align":["right"]},{"label":["summove_ounce"],"name":[10],"type":["dbl"],"align":["right"]},{"label":["nweeks"],"name":[11],"type":["int"],"align":["right"]},{"label":["mean_marketshare"],"name":[12],"type":["dbl"],"align":["right"]},{"label":["sharerank"],"name":[13],"type":["int"],"align":["right"]},{"label":["priclow"],"name":[14],"type":["int"],"align":["right"]},{"label":["pricmed"],"name":[15],"type":["int"],"align":["right"]},{"label":["prichigh"],"name":[16],"type":["int"],"align":["right"]},{"label":["logprice_ounce"],"name":[17],"type":["dbl"],"align":["right"]},{"label":["logmove_ounce"],"name":[18],"type":["dbl"],"align":["right"]},{"label":["saledummy_B"],"name":[19],"type":["fct"],"align":["left"]},{"label":["saledummy_C"],"name":[20],"type":["fct"],"align":["left"]},{"label":["saledummy_S"],"name":[21],"type":["fct"],"align":["left"]},{"label":["promoweek"],"name":[22],"type":["int"],"align":["right"]}],"data":[{"1":"102","2":"26","3":"Budweiser","4":"91","5":"41704.00","6":"4.665278","7":"0.1333333","8":"0","9":"0","10":"158871900","11":"220","12":"0.08933073","13":"3","14":"0","15":"1","16":"0","17":"1.540147","18":"10.63835","19":"1","20":"0","21":"0","22":"91"},{"1":"102","2":"26","3":"Budweiser","4":"92","5":"44212.48","6":"4.575852","7":"0.1333333","8":"0","9":"0","10":"158871900","11":"220","12":"0.08933073","13":"3","14":"0","15":"1","16":"0","17":"1.520793","18":"10.69676","19":"1","20":"0","21":"0","22":"92"},{"1":"102","2":"26","3":"Budweiser","4":"93","5":"28748.48","6":"4.575668","7":"0.1250000","8":"0","9":"0","10":"158871900","11":"220","12":"0.08933073","13":"3","14":"0","15":"1","16":"0","17":"1.520753","18":"10.26634","19":"1","20":"0","21":"0","22":"93"},{"1":"102","2":"26","3":"Budweiser","4":"94","5":"26168.96","6":"4.530460","7":"0.0000000","8":"0","9":"0","10":"158871900","11":"220","12":"0.08933073","13":"3","14":"0","15":"1","16":"0","17":"1.510824","18":"10.17233","19":"0","20":"0","21":"0","22":"NA"},{"1":"102","2":"26","3":"Budweiser","4":"95","5":"35581.44","6":"4.539798","7":"0.0000000","8":"0","9":"0","10":"158871900","11":"220","12":"0.08933073","13":"3","14":"0","15":"1","16":"0","17":"1.512883","18":"10.47958","19":"0","20":"0","21":"0","22":"NA"},{"1":"102","2":"26","3":"Budweiser","4":"96","5":"34128.00","6":"4.650231","7":"0.0000000","8":"0","9":"0","10":"158871900","11":"220","12":"0.08933073","13":"3","14":"0","15":"1","16":"0","17":"1.536917","18":"10.43787","19":"0","20":"0","21":"0","22":"NA"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<div class="sourceCode" id="cb230"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb230-1"><a href="supervised-learning.html#cb230-1" tabindex="-1"></a>psych<span class="sc">::</span><span class="fu">describe</span>(regression)  <span class="co">#descriptive statistics using psych</span></span></code></pre></div>
<pre><code>##                  vars   n         mean      sd       median      trimmed
## store*              1 220         1.00    0.00         1.00         1.00
## brand_id*           2 220         1.00    0.00         1.00         1.00
## brand*              3 220         1.00    0.00         1.00         1.00
## week                4 220       202.12   65.75       200.50       201.65
## move_ounce          5 220     19713.01 7837.08     17868.00     18580.18
## price_ounce         6 220         4.88    0.21         4.90         4.88
## sale_B              7 220         0.20    0.15         0.19         0.19
## sale_C              8 220         0.00    0.00         0.00         0.00
## sale_S              9 220         0.00    0.02         0.00         0.00
## summove_ounce      10 220 158871900.16    0.00 158871900.16 158871900.16
## nweeks             11 220       220.00    0.00       220.00       220.00
## mean_marketshare   12 220         0.09    0.00         0.09         0.09
## sharerank          13 220         3.00    0.00         3.00         3.00
## priclow            14 220         0.00    0.00         0.00         0.00
## pricmed            15 220         1.00    0.00         1.00         1.00
## prichigh           16 220         0.00    0.00         0.00         0.00
## logprice_ounce     17 220         1.58    0.04         1.59         1.59
## logmove_ounce      18 220         9.83    0.34         9.79         9.81
## saledummy_B*       19 220         1.75    0.43         2.00         1.82
## saledummy_C*       20 220         1.00    0.00         1.00         1.00
## saledummy_S*       21 220         1.01    0.10         1.00         1.00
## promoweek          22 167       212.28   62.15       217.00       212.78
##                      mad          min          max    range  skew kurtosis
## store*              0.00         1.00         1.00     0.00   NaN      NaN
## brand_id*           0.00         1.00         1.00     0.00   NaN      NaN
## brand*              0.00         1.00         1.00     0.00   NaN      NaN
## week               82.28        91.00       317.00   226.00  0.06    -1.20
## move_ounce       5538.99      7992.00     71032.00 63040.00  2.32     9.32
## price_ounce         0.20         4.30         5.45     1.15 -0.15    -0.19
## sale_B              0.18         0.00         0.67     0.67  0.28    -0.59
## sale_C              0.00         0.00         0.00     0.00   NaN      NaN
## sale_S              0.00         0.00         0.21     0.21 10.68   115.22
## summove_ounce       0.00 158871900.16 158871900.16     0.00   Inf      NaN
## nweeks              0.00       220.00       220.00     0.00   NaN      NaN
## mean_marketshare    0.00         0.09         0.09     0.00  -Inf      NaN
## sharerank           0.00         3.00         3.00     0.00   NaN      NaN
## priclow             0.00         0.00         0.00     0.00   NaN      NaN
## pricmed             0.00         1.00         1.00     0.00   NaN      NaN
## prichigh            0.00         0.00         0.00     0.00   NaN      NaN
## logprice_ounce      0.04         1.46         1.70     0.24 -0.27    -0.15
## logmove_ounce       0.32         8.99        11.17     2.18  0.67     0.86
## saledummy_B*        0.00         1.00         2.00     1.00 -1.17    -0.62
## saledummy_C*        0.00         1.00         1.00     0.00   NaN      NaN
## saledummy_S*        0.00         1.00         2.00     1.00 10.27   104.03
## promoweek          78.58        91.00       316.00   225.00 -0.08    -1.14
##                      se
## store*             0.00
## brand_id*          0.00
## brand*             0.00
## week               4.43
## move_ounce       528.38
## price_ounce        0.01
## sale_B             0.01
## sale_C             0.00
## sale_S             0.00
## summove_ounce      0.00
## nweeks             0.00
## mean_marketshare   0.00
## sharerank          0.00
## priclow            0.00
## pricmed            0.00
## prichigh           0.00
## logprice_ounce     0.00
## logmove_ounce      0.02
## saledummy_B*       0.03
## saledummy_C*       0.00
## saledummy_S*       0.01
## promoweek          4.81</code></pre>
<p>As stated above, regression analysis may be used to relate a quantitative response (“dependent variable”) to one or more predictor variables (“independent variables”). In a simple linear regression, we have one dependent and one independent variable and we regress the dependent variable on the independent variable.</p>
<p>Here are a few important questions that we might seek to address based on the data:</p>
<ul>
<li>Is there a relationship between prices and sales?</li>
<li>How strong is the relationship between prices and sales?</li>
<li>Which other variables contribute to sales?</li>
<li>How accurately can we estimate the effect of each variable on sales?</li>
<li>How accurately can we predict future sales?</li>
<li>Is the relationship linear?</li>
<li>Is there synergy among the advertising activities?</li>
</ul>
<p>We may use linear regression to answer these questions. We will see later that the interpretation of the results strongly depends on the goal of the analysis - whether you would like to simply predict an outcome variable or you would like to explain the causal effect of the independent variable on the dependent variable (see Chapter 1). Let’s start with the first question and investigate the relationship between advertising and sales.</p>
<div id="estimating-the-coefficients" class="section level5 hasAnchor" number="6.1.2.1.1">
<h5><span class="header-section-number">6.1.2.1.1</span> Estimating the coefficients<a href="supervised-learning.html#estimating-the-coefficients" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>A simple linear regression model only has one predictor and can be written as:</p>
<p><span class="math display" id="eq:regequ">\[\begin{equation}
Y=\beta_0+\beta_1X+\epsilon
\tag{6.5}
\end{equation}\]</span></p>
<p>In our specific context, let’s consider only the influence of prices on sales for now:</p>
<p><span class="math display" id="eq:regequadv">\[\begin{equation}
Sales=\beta_0+\beta_1*price+\epsilon
\tag{6.6}
\end{equation}\]</span></p>
<p>The word “price” represents data on advertising expenditures that we have observed and β<sub>1</sub> (the “slope”“) represents the unknown relationship between prices and sales. It tells you by how much sales will increase or decrease for an additional dollar added to price. β<sub>0</sub> (the”intercept”) is the number of sales we would expect if the price is set to 0. Note that the last assumption is highly theoretical: in the majority of real world scenarios, we never have such variables as prices, advertising expenditures, kilometers to the nearest store set to 0. Hence, it is incorrect to interpret the intercept like this. Together, β<sub>0</sub> and β<sub>1</sub> represent the model coefficients or <em>parameters</em>. The error term (ε) captures everything that we miss by using our model, including, (1) misspecifications (the true relationship might not be linear), (2) omitted variables (other variables might drive sales), and (3) measurement error (our measurement of the variables might be imperfect).</p>
<p>Once we have used our training data to produce estimates for the model coefficients, we can predict future sales on the basis of a particular value of price by computing:</p>
<p><span class="math display" id="eq:predreg">\[\begin{equation}
\hat{Sales}=\hat{\beta_0}+\hat{\beta_1}*price
\tag{6.7}
\end{equation}\]</span></p>
<p>We use the hat symbol, <sup>^</sup>, to denote the estimated value for an unknown parameter or coefficient, or to denote the predicted value of the response (sales). In practice, β<sub>0</sub> and β<sub>1</sub> are unknown and must be estimated from the data to make predictions. In the case of our pricing example, the data set consists of the prices and product sales for 220 weeks (n = 220). Our goal is to obtain coefficient estimates such that the linear model fits the available data well. In other words, we fit a line through the scatterplot of observations and try to find the line that best describes the data. The following graph shows the scatterplot for our data, where the black line shows the regression line. The grey vertical lines shows the difference between the predicted values (the regression line) and the observed values. This difference is referred to as the residuals (“e”).</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-16"></span>
<img src="07-supervised_learning_files/figure-html/unnamed-chunk-16-1.png" alt="Ordinary least squares (OLS)" width="672" />
<p class="caption">
Figure 6.1: Ordinary least squares (OLS)
</p>
</div>
<p>The estimation of the regression function is based on the idea of the method of least squares (OLS = ordinary least squares). The first step is to calculate the residuals by subtracting the observed values from the predicted values.</p>
<p style="text-align:center;">
<span class="math inline">\(e_i = Y_i-(\hat{\beta_0}+\hat{\beta_1}X_i)\)</span>
</p>
<p>This difference is then minimized by minimizing the sum of the squared residuals:</p>
<p><span class="math display" id="eq:rss">\[\begin{equation}
\sum_{i=1}^{N} e_i^2= \sum_{i=1}^{N} [Y_i-(\hat{\beta_0}+\hat{\beta_1X_i)}]^2\rightarrow min!
\tag{6.8}
\end{equation}\]</span></p>
<p>e<sub>i</sub>: Residuals (i = 1,2,…,N)<br>
Y<sub>i</sub>: Values of the dependent variable (i = 1,2,…,N) <br>
&amp;: Intercept<br>
&amp;: Regression coefficient / slope parameters<br>
X<sub>ni</sub>: Values of the nth independent variables and the i<em>th</em> observation<br>
N: Number of observations<br></p>
<p>This is also referred to as the <b>residual sum of squares (RSS)</b>. Now we need to choose the values for β<sub>0</sub> and β<sub>1</sub> that minimize RSS. So how can we derive these values for the regression coefficient? The equation for β<sub>1</sub> is given by:</p>
<p><span class="math display" id="eq:slope">\[\begin{equation}
\hat{\beta_1}=\frac{COV_{XY}}{s_x^2}
\tag{6.9}
\end{equation}\]</span></p>
<p>The exact mathematical derivation of this formula is beyond the scope of this script, but the intuition is to calculate the first derivative of the squared residuals with respect to β<sub>1</sub> and set it to zero, thereby finding the β<sub>1</sub> that minimizes the term. Using the above formula, you can easily compute β<sub>1</sub> using the following code:</p>
<div class="sourceCode" id="cb232"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb232-1"><a href="supervised-learning.html#cb232-1" tabindex="-1"></a>cov_y_x <span class="ot">&lt;-</span> <span class="fu">cov</span>(regression<span class="sc">$</span>price_ounce, regression<span class="sc">$</span>move_ounce)</span>
<span id="cb232-2"><a href="supervised-learning.html#cb232-2" tabindex="-1"></a>cov_y_x</span></code></pre></div>
<pre><code>## [1] -405.2959</code></pre>
<div class="sourceCode" id="cb234"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb234-1"><a href="supervised-learning.html#cb234-1" tabindex="-1"></a>var_x <span class="ot">&lt;-</span> <span class="fu">var</span>(regression<span class="sc">$</span>price_ounce)</span>
<span id="cb234-2"><a href="supervised-learning.html#cb234-2" tabindex="-1"></a>var_x</span></code></pre></div>
<pre><code>## [1] 0.04473198</code></pre>
<div class="sourceCode" id="cb236"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb236-1"><a href="supervised-learning.html#cb236-1" tabindex="-1"></a>beta_1 <span class="ot">&lt;-</span> cov_y_x<span class="sc">/</span>var_x</span>
<span id="cb236-2"><a href="supervised-learning.html#cb236-2" tabindex="-1"></a>beta_1</span></code></pre></div>
<pre><code>## [1] -9060.539</code></pre>
<p>The interpretation of β<sub>1</sub> is as follows:</p>
<p>For every extra dollar increase of the price, sales can be expected to decrease by -9060.539 oz, which is around 270 liters.</p>
<p>Using the estimated coefficient for β<sub>1</sub>, it is easy to compute β<sub>0</sub> (the intercept) as follows:</p>
<p><span class="math display" id="eq:intercept">\[\begin{equation}
\hat{\beta_0}=\overline{Y}-\hat{\beta_1}\overline{X}
\tag{6.10}
\end{equation}\]</span></p>
<p>The R code for this is:</p>
<div class="sourceCode" id="cb238"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb238-1"><a href="supervised-learning.html#cb238-1" tabindex="-1"></a>beta_0 <span class="ot">&lt;-</span> <span class="fu">mean</span>(regression<span class="sc">$</span>move_ounce) <span class="sc">-</span> beta_1 <span class="sc">*</span> <span class="fu">mean</span>(regression<span class="sc">$</span>price_ounce)</span>
<span id="cb238-2"><a href="supervised-learning.html#cb238-2" tabindex="-1"></a>beta_0</span></code></pre></div>
<pre><code>## [1] 63950.63</code></pre>
<p>You may also verify this based on a scatterplot of the data. The following plot shows the scatterplot including the regression line, which is estimated using OLS.</p>
<div class="sourceCode" id="cb240"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb240-1"><a href="supervised-learning.html#cb240-1" tabindex="-1"></a><span class="fu">ggplot</span>(regression, <span class="at">mapping =</span> <span class="fu">aes</span>(price_ounce, move_ounce)) <span class="sc">+</span></span>
<span id="cb240-2"><a href="supervised-learning.html#cb240-2" tabindex="-1"></a>    <span class="fu">geom_point</span>(<span class="at">shape =</span> <span class="dv">1</span>) <span class="sc">+</span> <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;lm&quot;</span>,</span>
<span id="cb240-3"><a href="supervised-learning.html#cb240-3" tabindex="-1"></a>    <span class="at">fill =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">alpha =</span> <span class="fl">0.1</span>) <span class="sc">+</span> <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">&quot;Price ($ per oz)&quot;</span>,</span>
<span id="cb240-4"><a href="supervised-learning.html#cb240-4" tabindex="-1"></a>    <span class="at">y =</span> <span class="st">&quot;Sales (oz)&quot;</span>) <span class="sc">+</span> <span class="fu">theme_bw</span>()</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-19"></span>
<img src="07-supervised_learning_files/figure-html/unnamed-chunk-19-1.png" alt="Scatterplot" width="672" />
<p class="caption">
Figure 1.16: Scatterplot
</p>
</div>
<p>The slope coefficient (β<sub>1</sub>) tells you by how much sales (on the y-axis) would decrease if the price (on the x-axis) is increased by one unit ($).</p>
</div>
<div id="significance-testing-1" class="section level5 hasAnchor" number="6.1.2.1.2">
<h5><span class="header-section-number">6.1.2.1.2</span> Significance testing<a href="supervised-learning.html#significance-testing-1" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>In a next step, we assess if the effect of prices on sales is statistically significant. This means that we test the null hypothesis H<sub>0</sub>: “There is no relationship between prices and sales” versus the alternative hypothesis H<sub>1</sub>: “The is some relationship between prices and sales”. Or, to state this formally:</p>
<p><span class="math display">\[H_0:\beta_1=0\]</span>
<span class="math display">\[H_1:\beta_1\ne0\]</span></p>
<p>How can we test if the effect is statistically significant? Recall the generalized equation to derive a test statistic:</p>
<p><span class="math display" id="eq:teststatgeneral">\[\begin{equation}
test\ statistic = \frac{effect}{error}
\tag{6.11}
\end{equation}\]</span></p>
<p>The effect is given by the β<sub>1</sub> coefficient in this case. To compute the test statistic, we need to come up with a measure of uncertainty around this estimate (the error). This is because we use information from a sample to estimate the least squares line to make inferences regarding the regression line in the entire population. Since we only have access to one sample, the regression line will be slightly different every time we take a different sample from the population. This is sampling variation and it is perfectly normal! It just means that we need to take into account the uncertainty around the estimate, which is achieved by the standard error. Thus, the test statistic for our hypothesis is given by:</p>
<p><span class="math display" id="eq:teststatreg">\[\begin{equation}
t = \frac{\hat{\beta_1}}{SE(\hat{\beta_1})}
\tag{6.12}
\end{equation}\]</span></p>
<p>After calculating the test statistic, we compare its value to the values that we would expect to find if there was no effect based on the t-distribution. In a regression context, the degrees of freedom are given by <code>N - p - 1</code> where N is the sample size and p is the number of predictors. In our case, we have 220 observations and one predictor. Thus, the degrees of freedom is 220 - 1 - 1 = 218. In the regression output below, R provides the exact probability of observing a t value of this magnitude (or larger) if the null hypothesis was true. This probability is the p-value. A small p-value indicates that it is unlikely to observe such a substantial association between the predictor and the outcome variable due to chance in the absence of any real association between the predictor and the outcome.</p>
<p>To estimate the regression model in R, you can use the <code>lm()</code> function. Within the function, you first specify the dependent variable (“move_ounce”) and independent variable (“price_ounce”) separated by a <code>~</code> (tilde). As mentioned previously, this is known as <em>formula notation</em> in R. The <code>data = regression</code> argument specifies that the variables come from the data frame named “regression”. Strictly speaking, you use the <code>lm()</code> function to create an object called “sales_reg,” which holds the regression output. You can then view the results using the <code>summary()</code> function:</p>
<div class="sourceCode" id="cb241"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb241-1"><a href="supervised-learning.html#cb241-1" tabindex="-1"></a>sales_reg <span class="ot">&lt;-</span> <span class="fu">lm</span>(move_ounce <span class="sc">~</span> price_ounce, <span class="at">data =</span> regression)  <span class="co">#estimate linear model</span></span>
<span id="cb241-2"><a href="supervised-learning.html#cb241-2" tabindex="-1"></a><span class="fu">summary</span>(sales_reg)  <span class="co">#summary of results</span></span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = move_ounce ~ price_ounce, data = regression)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -11439  -4798  -1539   2744  50733 
## 
## Coefficients:
##             Estimate Std. Error t value    Pr(&gt;|t|)    
## (Intercept)    63951      11892   5.377 0.000000194 ***
## price_ounce    -9060       2434  -3.723     0.00025 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 7617 on 218 degrees of freedom
## Multiple R-squared:  0.05979,    Adjusted R-squared:  0.05548 
## F-statistic: 13.86 on 1 and 218 DF,  p-value: 0.0002504</code></pre>
<p>Note that the estimated coefficients for β<sub>0</sub> (63950.625) and β<sub>1</sub> (-9060.539) correspond to the results of our manual computation above. The associated t-values and p-values are given in the output. The t-values are larger than the critical t-values for the 95% confidence level, since the associated p-values are smaller than 0.05. In case of the coefficient for β<sub>1</sub>, this means that the probability of an association between the prices and sales of the observed magnitude (or larger) is smaller than 0.05, if the value of β<sub>1</sub> was, in fact, 0. This finding leads us to reject the null hypothesis of no association between prices and sales.</p>
<p>The coefficients associated with the respective variables represent <b>point estimates</b>. To obtain a better understanding of the range of values that the coefficients could take, it is helpful to compute <b>confidence intervals</b>. A 95% confidence interval is defined as a range of values such that with a 95% probability, the range will contain the true unknown value of the parameter. For example, for β<sub>1</sub>, the confidence interval can be computed as.</p>
<p><span class="math display" id="eq:regCI">\[\begin{equation}
CI = \hat{\beta_1}\pm(t_{1-\frac{\alpha}{2}}*SE(\beta_1))
\tag{6.13}
\end{equation}\]</span></p>
<p>It is easy to compute confidence intervals in R using the <code>confint()</code> function. You just have to provide the name of you estimated model as an argument:</p>
<div class="sourceCode" id="cb243"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb243-1"><a href="supervised-learning.html#cb243-1" tabindex="-1"></a><span class="fu">confint</span>(sales_reg)</span></code></pre></div>
<pre><code>##                 2.5 %   97.5 %
## (Intercept)  40511.67 87389.58
## price_ounce -13856.72 -4264.36</code></pre>
<p>For our model, the 95% confidence interval for β<sub>0</sub> is [40511.67,87389.58], and the 95% confidence interval for β<sub>1</sub> is [-13856.72,-4264.36]. Thus, we can conclude that when we do not spend any money on advertising, sales will be somewhere between 40512 and 87390 units on average. In addition, for each increase in advertising expenditures by one Euro, there will be an average increase in sales of between -13856.72 and -4264.36. If you revisit the graphic depiction of the regression model above, the uncertainty regarding the intercept and slope parameters can be seen in the confidence bounds (blue area) around the regression line.</p>
</div>
<div id="assessing-model-fit" class="section level5 hasAnchor" number="6.1.2.1.3">
<h5><span class="header-section-number">6.1.2.1.3</span> Assessing model fit<a href="supervised-learning.html#assessing-model-fit" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>Once we have rejected the null hypothesis in favor of the alternative hypothesis, the next step is to investigate how well the model represents (“fits”) the data. How can we assess the model fit?</p>
<ul>
<li>First, we calculate the fit of the most basic model (i.e., the mean)</li>
<li>Then, we calculate the fit of the best model (i.e., the regression model)</li>
<li>A good model should fit the data significantly better than the basic model</li>
<li>R<sup>2</sup>: Represents the percentage of the variation in the outcome that can be explained by the model</li>
<li>The F-ratio measures how much the model has improved the prediction of the outcome compared to the level of inaccuracy in the model</li>
</ul>
<p>Similar to ANOVA, the calculation of model fit statistics relies on estimating the different sum of squares values. SS<sub>T</sub> is the difference between the observed data and the mean value of Y (aka. total variation). In the absence of any other information, the mean value of Y (<span class="math inline">\(\overline{Y}\)</span>) represents the best guess on where a particular observation <span class="math inline">\(Y_{i}\)</span> at a given level of advertising will fall:</p>
<p><span class="math display" id="eq:regSST">\[\begin{equation}
SS_T= \sum_{i=1}^{N} (Y_i-\overline{Y})^2
\tag{6.14}
\end{equation}\]</span></p>
<p>The following graph shows the total sum of squares:</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-22"></span>
<img src="07-supervised_learning_files/figure-html/unnamed-chunk-22-1.png" alt="Total sum of squares" width="672" />
<p class="caption">
Figure 1.19: Total sum of squares
</p>
</div>
<p>Based on our linear model, the best guess about the sales level at a given level of prices is the predicted value <span class="math inline">\(\hat{Y}_i\)</span>. The model sum of squares (SS<sub>M</sub>) therefore has the mathematical representation:</p>
<p><span class="math display" id="eq:regSSM">\[\begin{equation}
SS_M= \sum_{i=1}^{N}  (\hat{Y}_i-\overline{Y})^2
\tag{6.15}
\end{equation}\]</span></p>
<p>The model sum of squares represents the improvement in prediction resulting from using the regression model rather than the mean of the data. The following graph shows the model sum of squares for our example:</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-23"></span>
<img src="07-supervised_learning_files/figure-html/unnamed-chunk-23-1.png" alt="Ordinary least squares (OLS)" width="672" />
<p class="caption">
Figure 1.20: Ordinary least squares (OLS)
</p>
</div>
<p>The residual sum of squares (SS<sub>R</sub>) is the difference between the observed data points (<span class="math inline">\(Y_{i}\)</span>) and the predicted values along the regression line (<span class="math inline">\(\hat{Y}_{i}\)</span>), i.e., the variation <em>not</em> explained by the model.</p>
<p><span class="math display" id="eq:regSSR">\[\begin{equation}
SS_R= \sum_{i=1}^{N} ({Y}_{i}-\hat{Y}_{i})^2
\tag{6.16}
\end{equation}\]</span></p>
<p>The following graph shows the residual sum of squares for our example:</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-24"></span>
<img src="07-supervised_learning_files/figure-html/unnamed-chunk-24-1.png" alt="Ordinary least squares (OLS)" width="672" />
<p class="caption">
Figure 1.21: Ordinary least squares (OLS)
</p>
</div>
<p>Based on these statistics, we can determine how well the model fits the data as we will see next.</p>
<div id="r-squared" class="section level6 unnumbered hasAnchor">
<h6>R-squared<a href="supervised-learning.html#r-squared" class="anchor-section" aria-label="Anchor link to header"></a></h6>
<p>The R<sup>2</sup> statistic represents the proportion of variance that is explained by the model and is computed as:</p>
<p><span class="math display" id="eq:regSSR">\[\begin{equation}
R^2= \frac{SS_M}{SS_T}
\tag{6.16}
\end{equation}\]</span></p>
<p>It takes values between 0 (very bad fit) and 1 (very good fit). Note that when the goal of your model is to <em>predict</em> future outcomes, a “too good” model fit can pose severe challenges. The reason is that the model might fit your specific sample so well, that it will only predict well within the sample but not generalize to other samples. This is called <strong>overfitting</strong> and it shows that there is a trade-off between model fit and out-of-sample predictive ability of the model, if the goal is to predict beyond the sample. We will come back to this point later in this chapter.</p>
<p>You can get a first impression of the fit of the model by inspecting the scatter plot as can be seen in the plot below. If the observations are highly dispersed around the regression line (left plot), the fit will be lower compared to a data set where the values are less dispersed (right plot).</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-25"></span>
<img src="07-supervised_learning_files/figure-html/unnamed-chunk-25-1.png" alt="Good vs. bad model fit" width="960" />
<p class="caption">
Figure 1.22: Good vs. bad model fit
</p>
</div>
<p>The R<sup>2</sup> statistic is reported in the regression output, so you don’t need to compute it manually.</p>
</div>
<div id="adjusted-r-squared" class="section level6 unnumbered hasAnchor">
<h6>Adjusted R-squared<a href="supervised-learning.html#adjusted-r-squared" class="anchor-section" aria-label="Anchor link to header"></a></h6>
<p>Due to the way the R<sup>2</sup> statistic is calculated, it will never decrease if a new explanatory variable is introduced into the model. This means that every new independent variable either doesn’t change the R<sup>2</sup> or increases it, even if there is no real relationship between the new variable and the dependent variable. Hence, one could be tempted to just add as many variables as possible to increase the R<sup>2</sup> and thus obtain a “better” model. However, this actually only leads to more noise and therefore a worse model.</p>
<p>To account for this, there exists a test statistic closely related to the R<sup>2</sup>, the <strong>adjusted R<sup>2</sup></strong>. It can be calculated as follows:</p>
<p><span class="math display" id="eq:adjustedR2">\[\begin{equation}
\overline{R^2} = 1 - (1 - R^2)\frac{n-1}{n - k - 1}
\tag{6.17}
\end{equation}\]</span></p>
<p>where <code>n</code> is the total number of observations and <code>k</code> is the total number of explanatory variables. The adjusted R<sup>2</sup> is equal to or less than the regular R<sup>2</sup> and can be negative. It will only increase if the added variable adds more explanatory power than one would expect by pure chance. Essentially, it contains a “penalty” for including unnecessary variables and therefore favors more parsimonious models. As such, it is a measure of suitability, good for comparing different models and is very useful in the model selection stage of a project. In R, the standard <code>lm()</code> function automatically also reports the adjusted R<sup>2</sup> as you can see above.</p>
</div>
<div id="f-test" class="section level6 unnumbered hasAnchor">
<h6>F-test<a href="supervised-learning.html#f-test" class="anchor-section" aria-label="Anchor link to header"></a></h6>
<p>Similar to the ANOVA, another significance test is the F-test, which tests the null hypothesis:</p>
<p><span class="math display">\[H_0:R^2=0\]</span></p>
<p><br></p>
<p>Or, to state it slightly differently:</p>
<p><span class="math display">\[H_0:\beta_1=\beta_2=\beta_3=\beta_k=0\]</span>
<br>
This means that we test whether any of the included independent variables has a significant effect on the dependent variable. So far, we have only included one independent variable, but we will extend the set of predictor variables below.</p>
<p>The F-test statistic is calculated as follows:</p>
<p><span class="math display" id="eq:regSSR">\[\begin{equation}
F=\frac{\frac{SS_M}{k}}{\frac{SS_R}{(n-k-1)}}=\frac{MS_M}{MS_R}
\tag{6.16}
\end{equation}\]</span></p>
<p>which has a F distribution with k number of predictors and n degrees of freedom. In other words, you divide the systematic (“explained”) variation due to the predictor variables by the unsystematic (“unexplained”) variation.</p>
<p>The result of the F-test is provided in the regression output as well. However, you might manually compute the F-test using the ANOVA results from the model:</p>
<div class="sourceCode" id="cb245"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb245-1"><a href="supervised-learning.html#cb245-1" tabindex="-1"></a>f_calc <span class="ot">&lt;-</span> <span class="fu">anova</span>(sales_reg)<span class="sc">$</span><span class="st">&quot;Mean Sq&quot;</span>[<span class="dv">1</span>]<span class="sc">/</span><span class="fu">anova</span>(sales_reg)<span class="sc">$</span><span class="st">&quot;Mean Sq&quot;</span>[<span class="dv">2</span>]  <span class="co">#compute F</span></span>
<span id="cb245-2"><a href="supervised-learning.html#cb245-2" tabindex="-1"></a>f_calc</span></code></pre></div>
<pre><code>## [1] 14</code></pre>
<div class="sourceCode" id="cb247"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb247-1"><a href="supervised-learning.html#cb247-1" tabindex="-1"></a>f_crit <span class="ot">&lt;-</span> <span class="fu">qf</span>(<span class="fl">0.95</span>, <span class="at">df1 =</span> <span class="dv">1</span>, <span class="at">df2 =</span> <span class="dv">100</span>)  <span class="co">#critical value</span></span>
<span id="cb247-2"><a href="supervised-learning.html#cb247-2" tabindex="-1"></a>f_crit</span></code></pre></div>
<pre><code>## [1] 3.9</code></pre>
<div class="sourceCode" id="cb249"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb249-1"><a href="supervised-learning.html#cb249-1" tabindex="-1"></a>f_calc <span class="sc">&gt;</span> f_crit  <span class="co">#test if calculated test statistic is larger than critical value</span></span></code></pre></div>
<pre><code>## [1] TRUE</code></pre>
</div>
</div>
<div id="using-the-model" class="section level5 hasAnchor" number="6.1.2.1.4">
<h5><span class="header-section-number">6.1.2.1.4</span> Using the model<a href="supervised-learning.html#using-the-model" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>After fitting the model, we can use the estimated coefficients to predict sales of Budweiser for different values of prices. Suppose the store plans to set the price per ounce to 2 dollars. How much will it sell? You can easily compute this either by hand:</p>
<p><span class="math display">\[\hat{sales}=63950.6 + (-9060.5)*2=45,829.6\]</span></p>
<p><br></p>
<p>… or by extracting the estimated coefficients from the model summary:</p>
<div class="sourceCode" id="cb251"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb251-1"><a href="supervised-learning.html#cb251-1" tabindex="-1"></a>prediction <span class="ot">&lt;-</span> <span class="fu">summary</span>(sales_reg)<span class="sc">$</span>coefficients[<span class="dv">1</span>, <span class="dv">1</span>] <span class="sc">+</span></span>
<span id="cb251-2"><a href="supervised-learning.html#cb251-2" tabindex="-1"></a>    <span class="fu">summary</span>(sales_reg)<span class="sc">$</span>coefficients[<span class="dv">2</span>, <span class="dv">1</span>] <span class="sc">*</span> <span class="dv">2</span>  <span class="co"># the slope * 2 EUR</span></span>
<span id="cb251-3"><a href="supervised-learning.html#cb251-3" tabindex="-1"></a>prediction</span></code></pre></div>
<pre><code>## [1] 45830</code></pre>
<p>The predicted value of the dependent variable is 45,829.6 oz, i.e., the store will sell around 45,829.6 oz (~1,355 liters) of Budweiser.</p>
</div>
</div>
<div id="log-log-transformation" class="section level4 hasAnchor" number="6.1.2.2">
<h4><span class="header-section-number">6.1.2.2</span> Log-Log transformation<a href="supervised-learning.html#log-log-transformation" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Have a look at the plots above again. You might notice some data specific pattern, making the data points look odd: they are pulled to lower edge of the scatterplot. In this particular case, we’re dealing with different measurement scales of our independent and dependent variables. Moreover, you could also notice how odd the interpretation of the regression coefficients sounds.</p>
<p>It is very rare that in the retailing context, the predictions are made as we did before. The concept that is used instead is familiar to you from the microeconomics course - elasticity is a measure that is used by retail managers and researchers much more often than mere unit changes.</p>
<p>Let’s have a look at the plot again:</p>
<div class="sourceCode" id="cb253"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb253-1"><a href="supervised-learning.html#cb253-1" tabindex="-1"></a><span class="fu">ggplot</span>(regression, <span class="at">mapping =</span> <span class="fu">aes</span>(price_ounce, move_ounce)) <span class="sc">+</span></span>
<span id="cb253-2"><a href="supervised-learning.html#cb253-2" tabindex="-1"></a>    <span class="fu">geom_point</span>(<span class="at">shape =</span> <span class="dv">1</span>) <span class="sc">+</span> <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;lm&quot;</span>,</span>
<span id="cb253-3"><a href="supervised-learning.html#cb253-3" tabindex="-1"></a>    <span class="at">color =</span> <span class="st">&quot;lavenderblush4&quot;</span>, <span class="at">fill =</span> <span class="st">&quot;red&quot;</span>, <span class="at">alpha =</span> <span class="fl">0.1</span>) <span class="sc">+</span></span>
<span id="cb253-4"><a href="supervised-learning.html#cb253-4" tabindex="-1"></a>    <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">&quot;Price (ounce)&quot;</span>, <span class="at">y =</span> <span class="st">&quot;Sales (ounce)&quot;</span>) <span class="sc">+</span></span>
<span id="cb253-5"><a href="supervised-learning.html#cb253-5" tabindex="-1"></a>    <span class="fu">theme_minimal</span>()</span></code></pre></div>
<p><img src="07-supervised_learning_files/figure-html/unnamed-chunk-28-1.png" width="672" /></p>
<p>The way of obtaining a more reasonable view and interpretation in this case is called “multiplicative modeling”, or log-log transformation (you can find additional details about log-log transformations below with a slightly different motivation and example).</p>
<p>The multiplicative model has the following formal representation:</p>
<p><span class="math display" id="eq:multiplicative">\[\begin{equation}
Y =\beta_0 *X_1^{\beta_1}*X_2^{\beta_2}*...*X_J^{\beta_J}*\epsilon
\tag{6.18}
\end{equation}\]</span></p>
<p>This functional form can be linearized by taking the logarithm of both sides of the equation:</p>
<p><span class="math display" id="eq:multiplicativetransformed">\[\begin{equation}
log(Y) =log(\beta_0) + \beta_1*log(X_1) + \beta_2*log(X_2) + ...+ \beta_J*log(X_J) + log(\epsilon)
\tag{6.19}
\end{equation}\]</span></p>
<p>This means that taking logarithms of both sides of the equation makes linear estimation possible. The above transformation follows from two logarithm rules that we apply here:</p>
<ol style="list-style-type: decimal">
<li>the product rule states that <span class="math inline">\(log(xy)=log(x)+log(y)\)</span>; thus, when taking the logarithm of the right hand side of the multiplicative model, we can write <span class="math inline">\(log(X_1) + log(X_2)... log(X_J)\)</span> instead of <span class="math inline">\(log(X_1*X_2*...X_J)\)</span>, and</li>
<li>the power rule states that <span class="math inline">\(log(x^y) = ylog(x)\)</span>; thus, we can write <span class="math inline">\(\beta*log(X)\)</span> instead of <span class="math inline">\(X^{\beta}\)</span></li>
</ol>
<p>Let’s test how the scatterplot would look like if we use the logarithm of our variables using the <code>log()</code> function instead of the original values.</p>
<div class="sourceCode" id="cb254"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb254-1"><a href="supervised-learning.html#cb254-1" tabindex="-1"></a><span class="fu">ggplot</span>(regression, <span class="at">mapping =</span> <span class="fu">aes</span>(<span class="fu">log</span>(price_ounce),</span>
<span id="cb254-2"><a href="supervised-learning.html#cb254-2" tabindex="-1"></a>    <span class="fu">log</span>(move_ounce))) <span class="sc">+</span> <span class="fu">geom_point</span>(<span class="at">shape =</span> <span class="dv">1</span>) <span class="sc">+</span> <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;lm&quot;</span>,</span>
<span id="cb254-3"><a href="supervised-learning.html#cb254-3" tabindex="-1"></a>    <span class="at">color =</span> <span class="st">&quot;lavenderblush4&quot;</span>, <span class="at">fill =</span> <span class="st">&quot;red&quot;</span>, <span class="at">alpha =</span> <span class="fl">0.1</span>) <span class="sc">+</span></span>
<span id="cb254-4"><a href="supervised-learning.html#cb254-4" tabindex="-1"></a>    <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">&quot;Price&quot;</span>, <span class="at">y =</span> <span class="st">&quot;Sales&quot;</span>) <span class="sc">+</span> <span class="fu">theme_minimal</span>()</span></code></pre></div>
<p><img src="07-supervised_learning_files/figure-html/unnamed-chunk-29-1.png" width="672" /></p>
<p>You can see how the scales changed, and how the observations got more normally distributed. Hence, we can log-transform our variables and estimate the following equation:</p>
<p><span class="math display" id="eq:multiplicativetransformed1">\[\begin{equation}
log(sales) = log(\beta_0) + \beta_1*log(price) + log(\epsilon)
\tag{6.20}
\end{equation}\]</span></p>
<p>Now, let’s estimate a new regression by applying <code>log()</code> function to both sales and prices:</p>
<div class="sourceCode" id="cb255"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb255-1"><a href="supervised-learning.html#cb255-1" tabindex="-1"></a>sales_reg2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="fu">log</span>(move_ounce) <span class="sc">~</span> <span class="fu">log</span>(price_ounce),</span>
<span id="cb255-2"><a href="supervised-learning.html#cb255-2" tabindex="-1"></a>    <span class="at">data =</span> regression)</span>
<span id="cb255-3"><a href="supervised-learning.html#cb255-3" tabindex="-1"></a><span class="fu">summary</span>(sales_reg2)  <span class="co">#remember that now the interpretation changed</span></span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = log(move_ounce) ~ log(price_ounce), data = regression)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -0.8252 -0.2239 -0.0285  0.1818  1.3156 
## 
## Coefficients:
##                  Estimate Std. Error t value             Pr(&gt;|t|)    
## (Intercept)        13.354      0.796   16.77 &lt; 0.0000000000000002 ***
## log(price_ounce)   -2.225      0.502   -4.43             0.000015 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.32 on 218 degrees of freedom
## Multiple R-squared:  0.0826, Adjusted R-squared:  0.0784 
## F-statistic: 19.6 on 1 and 218 DF,  p-value: 0.0000148</code></pre>
<p>In this example, you would interpret the coefficient as follows: <strong>A 1% increase in price leads to a 2.23% decrease in sales</strong>. Hence, the interpretation is in proportional terms and no longer in units. This means that the coefficients in a log-log model can be directly interpreted as elasticities, which also makes communication easier. We can generally also inspect the R<sup>2</sup> statistic to see that the model fit has increased compared to the linear specification (i.e., R<sup>2</sup> has increased to 0.08 from 0.06). However, please note that the variables are now measured on a different scale, which means that the model fit in theory is not directly comparable.</p>
</div>
<div id="multiple-linear-regression" class="section level4 hasAnchor" number="6.1.2.3">
<h4><span class="header-section-number">6.1.2.3</span> Multiple linear regression<a href="supervised-learning.html#multiple-linear-regression" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Multiple linear regression is a statistical technique that simultaneously tests the relationships between two or more independent variables and an interval-scaled dependent variable. The general form of the equation is given by:</p>
<p><span class="math display" id="eq:regequ">\[\begin{equation}
Y=(\beta_0+\beta_1*X_1+\beta_2*X_2+\beta_n*X_n)+\epsilon
\tag{6.5}
\end{equation}\]</span></p>
<p>Again, we aim to find the combination of predictors that correlate maximally with the outcome variable. Note that if you change the composition of predictors, the partial regression coefficient of an independent variable will be different from that of the bivariate regression coefficient. This is because the regressors are usually correlated, and any variation in Y that was shared by X1 and X2 was attributed to X1. The interpretation of the partial regression coefficients is the expected change in Y when X is changed by one unit and all other predictors are held constant.</p>
<p>Let’s extend the previous example. Say, in addition to the influence of price itself, you are interested in estimating the influence of two sales promotion techniques on the amount of Budweiser. The corresponding equation, including bonus buy and price reduction, would then be given by:</p>
<p><span class="math display">\[ Sales=\beta_0+\beta_1*price+\beta_2*bonus\_buy+\beta_3*price\_reduction+\epsilon\]</span></p>
<p>β<sub>1</sub>, β<sub>2</sub>, and β<sub>3</sub> represent the unknown relationship between sales and independent variables (price, bonus buy, and price reduction, respectively). The corresponding coefficients tell you by how much sales will change for an additional dollar increase of price (when the other IVs are held constant) and by how much sales will change for an additional unit of price reduction (when price itself and bonus buy are held constant), etc. Thus, we can make predictions about sales using all these variables.</p>
<p>With several predictors, the partitioning of sum of squares is the same as in the bivariate model, except that the model is no longer a 2-D straight line. With two predictors, the regression line becomes a 3-D regression plane. While multiple regression models that have more than two predictors are not as easy to visualize, you may apply the same principles when interpreting the model outcome:</p>
<ul>
<li>Total sum of squares (SS<sub>T</sub>) is still the difference between the observed data and the mean value of Y (total variation)</li>
<li>Residual sum of squares (SS<sub>R</sub>) is still the difference between the observed data and the values predicted by the model (unexplained variation)</li>
<li>Model sum of squares (SS<sub>M</sub>) is still the difference between the values predicted by the model and the mean value of Y (explained variation)</li>
<li>R measures the multiple correlation between the predictors and the outcome</li>
<li>R<sup>2</sup> is the amount of variation in the outcome variable explained by the model</li>
</ul>
<p>Estimating multiple regression models is straightforward using the <code>lm()</code> function. You just need to separate the individual predictors on the right hand side of the equation using the <code>+</code> symbol. In addition, as discussed before, we would need to use log-log transformation for our use case, which can be done in multiple regression context as well. Hence, we would specify the model as follows (note that bonus buy and price reduction are already percentages in our data set, i.e., 0.2 value of price reduction is translated as 20% price decrease - hence, we don’t need to take an additional logarithm of it):</p>
<p><span class="math display">\[ log(Sales) =log(\beta_0) + \beta_1*log(Price) + \beta_2*bonus\_buy+\beta_3*price\_reduction+ log(\epsilon) \]</span></p>
<p>This regression could be estimated as follows:</p>
<div class="sourceCode" id="cb257"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb257-1"><a href="supervised-learning.html#cb257-1" tabindex="-1"></a>multiple_sales_reg <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="fu">log</span>(move_ounce) <span class="sc">~</span> <span class="fu">log</span>(price_ounce) <span class="sc">+</span></span>
<span id="cb257-2"><a href="supervised-learning.html#cb257-2" tabindex="-1"></a>    sale_B <span class="sc">+</span> sale_S, <span class="at">data =</span> regression)  <span class="co"># estimate the model</span></span>
<span id="cb257-3"><a href="supervised-learning.html#cb257-3" tabindex="-1"></a><span class="fu">summary</span>(multiple_sales_reg)  <span class="co">#summary of results</span></span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = log(move_ounce) ~ log(price_ounce) + sale_B + sale_S, 
##     data = regression)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -0.7960 -0.2149 -0.0269  0.1871  1.2959 
## 
## Coefficients:
##                  Estimate Std. Error t value             Pr(&gt;|t|)    
## (Intercept)        13.318      0.811   16.42 &lt; 0.0000000000000002 ***
## log(price_ounce)   -2.221      0.508   -4.37             0.000019 ***
## sale_B              0.123      0.149    0.82                0.411    
## sale_S              3.033      1.223    2.48                0.014 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.32 on 216 degrees of freedom
## Multiple R-squared:  0.11,   Adjusted R-squared:  0.0979 
## F-statistic: 8.92 on 3 and 216 DF,  p-value: 0.0000134</code></pre>
<p>The interpretation of the coefficients is as follows:</p>
<ul>
<li>price (β<sub>1</sub>): when price increases by 1%, sales will change by -2.221%</li>
<li>bonus buy (β<sub>2</sub>): when bonus buy increases by 1%, sales will change by 0.123%</li>
<li>price reduction (β<sub>3</sub>): when the price reduction increases by 1%, sales will change by 3.033%</li>
</ul>
<p>The associated t-values and p-values are also given in the output. You can see that the p-values are smaller than 0.05 for price and price reduction coefficients, while bonus sale is insignificant. Moreover, the p-value for F-test is smaller than 0.05. This means that if the null hypothesis was true (i.e., there was no effect between the variables and sales), the probability of observing associations of the estimated magnitudes (or larger) is very small (e.g., smaller than 0.05).</p>
<p>Again, to get a better feeling for the range of values that the coefficients could take, it is helpful to compute <b>confidence intervals</b>.</p>
<div class="sourceCode" id="cb259"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb259-1"><a href="supervised-learning.html#cb259-1" tabindex="-1"></a><span class="fu">confint</span>(multiple_sales_reg)</span></code></pre></div>
<pre><code>##                  2.5 % 97.5 %
## (Intercept)      11.72  14.92
## log(price_ounce) -3.22  -1.22
## sale_B           -0.17   0.42
## sale_S            0.62   5.44</code></pre>
<p>What does this tell you? Recall that a 95% confidence interval is defined as a range of values such that with a 95% probability, the range will contain the true unknown value of the parameter. For example, for β<sub>3</sub>, the confidence interval is [0.6229993,5.4421432]. Thus, although we have computed a point estimate of 3.033 for the effect of price reduction on sales based on our sample, the effect might actually just as well take any other value within this range, considering the sample size and the variability in our data. You could also visualize the output from your regression model including the confidence intervals using the <code>ggstatsplot</code> package as follows:</p>
<div class="sourceCode" id="cb261"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb261-1"><a href="supervised-learning.html#cb261-1" tabindex="-1"></a><span class="fu">library</span>(ggstatsplot)</span>
<span id="cb261-2"><a href="supervised-learning.html#cb261-2" tabindex="-1"></a><span class="fu">ggcoefstats</span>(<span class="at">x =</span> multiple_sales_reg, <span class="at">title =</span> <span class="st">&quot;Sales predicted by price, bonus buy, and price reduction&quot;</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-33"></span>
<img src="07-supervised_learning_files/figure-html/unnamed-chunk-33-1.png" alt="Confidence intervals for regression model" width="672" />
<p class="caption">
Figure 4.2: Confidence intervals for regression model
</p>
</div>
<p>The output also tells us that 11.0258688% of the variation can be explained by our model. You may also visually inspect the fit of the model by plotting the predicted values against the observed values. We can extract the predicted values using the <code>predict()</code> function. So let’s create a new variable <code>yhat</code>, which contains those predicted values.</p>
<div class="sourceCode" id="cb262"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb262-1"><a href="supervised-learning.html#cb262-1" tabindex="-1"></a>regression<span class="sc">$</span>logmove_ounce_hat <span class="ot">&lt;-</span> <span class="fu">fitted</span>(multiple_sales_reg)</span></code></pre></div>
<p>We can now use this variable to plot the predicted values against the observed values. In the following plot, the model fit would be perfect if all points would fall on the diagonal line. The larger the distance between the points and the line, the worse the model fit. In other words, if all points would fall exactly on the diagonal line, the model would perfectly predict the observed values.</p>
<div class="sourceCode" id="cb263"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb263-1"><a href="supervised-learning.html#cb263-1" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data =</span> regression, <span class="fu">aes</span>(week, <span class="fu">log</span>(move_ounce))) <span class="sc">+</span></span>
<span id="cb263-2"><a href="supervised-learning.html#cb263-2" tabindex="-1"></a>    <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> regression<span class="sc">$</span>promoweek, <span class="at">colour =</span> <span class="st">&quot;lightgrey&quot;</span>) <span class="sc">+</span></span>
<span id="cb263-3"><a href="supervised-learning.html#cb263-3" tabindex="-1"></a>    <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">y =</span> <span class="fu">log</span>(move_ounce), <span class="at">colour =</span> <span class="st">&quot;logsales&quot;</span>),</span>
<span id="cb263-4"><a href="supervised-learning.html#cb263-4" tabindex="-1"></a>        <span class="at">size =</span> <span class="dv">1</span>) <span class="sc">+</span> <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">y =</span> (logmove_ounce_hat),</span>
<span id="cb263-5"><a href="supervised-learning.html#cb263-5" tabindex="-1"></a>    <span class="at">colour =</span> <span class="st">&quot;logsales (predicted)&quot;</span>), <span class="at">size =</span> <span class="dv">1</span>) <span class="sc">+</span> <span class="fu">scale_color_manual</span>(<span class="at">values =</span> <span class="fu">c</span>(<span class="st">&quot;black&quot;</span>,</span>
<span id="cb263-6"><a href="supervised-learning.html#cb263-6" tabindex="-1"></a>    <span class="st">&quot;gold&quot;</span>)) <span class="sc">+</span> <span class="fu">theme_minimal</span>()</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-35"></span>
<img src="07-supervised_learning_files/figure-html/unnamed-chunk-35-1.png" alt="Model fit" width="672" />
<p class="caption">
Figure 4.4: Model fit
</p>
</div>
<p><strong>Partial plots</strong></p>
<p>In the context of a simple linear regression (i.e., with a single independent variable), a scatter plot of the dependent variable against the independent variable provides a good indication of the nature of the relationship. If there is more than one independent variable, however, things become more complicated. The reason is that although the scatter plot still show the relationship between the two variables, it does not take into account the effect of the other independent variables in the model. Partial regression plot show the effect of adding another variable to a model that already controls for the remaining variables in the model. In other words, it is a scatterplot of the residuals of the outcome variable and each predictor when both variables are regressed separately on the remaining predictors. In our example, the partial plot would show the effect of adding price as an explanatory variables while controlling for the variation that is explained by sales promotions in both variables (sales and price). Think of it as the purified relationship between price and sales that remains after controlling for other factors. The partial plots can easily be created using the <code>avPlots()</code> function from the <code>car</code> package:</p>
<div class="sourceCode" id="cb264"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb264-1"><a href="supervised-learning.html#cb264-1" tabindex="-1"></a><span class="fu">library</span>(car)</span>
<span id="cb264-2"><a href="supervised-learning.html#cb264-2" tabindex="-1"></a><span class="fu">avPlots</span>(multiple_sales_reg)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-36"></span>
<img src="07-supervised_learning_files/figure-html/unnamed-chunk-36-1.png" alt="Partial plots" width="672" />
<p class="caption">
Figure 6.2: Partial plots
</p>
</div>
</div>
</div>
<div id="categorical-predictors" class="section level3 hasAnchor" number="6.1.3">
<h3><span class="header-section-number">6.1.3</span> Categorical predictors<a href="supervised-learning.html#categorical-predictors" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="two-categories" class="section level4 hasAnchor" number="6.1.3.1">
<h4><span class="header-section-number">6.1.3.1</span> Two categories<a href="supervised-learning.html#two-categories" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div class="sourceCode" id="cb265"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb265-1"><a href="supervised-learning.html#cb265-1" tabindex="-1"></a>categories <span class="ot">&lt;-</span> <span class="fu">read.table</span>(<span class="st">&quot;https://raw.githubusercontent.com/WU-RDS/RMA2024/main/data/beer_categorical&quot;</span>,</span>
<span id="cb265-2"><a href="supervised-learning.html#cb265-2" tabindex="-1"></a>    <span class="at">sep =</span> <span class="st">&quot;,&quot;</span>, <span class="at">header =</span> <span class="cn">TRUE</span>)</span>
<span id="cb265-3"><a href="supervised-learning.html#cb265-3" tabindex="-1"></a>categories<span class="sc">$</span>store <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(categories<span class="sc">$</span>store)</span>
<span id="cb265-4"><a href="supervised-learning.html#cb265-4" tabindex="-1"></a>categories<span class="sc">$</span>brand <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(categories<span class="sc">$</span>brand)</span>
<span id="cb265-5"><a href="supervised-learning.html#cb265-5" tabindex="-1"></a><span class="fu">str</span>(categories)</span></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    3194 obs. of  19 variables:
##  $ store           : Factor w/ 2 levels &quot;98&quot;,&quot;100&quot;: 1 1 1 1 1 1 1 1 1 1 ...
##  $ brand_id        : int  2 2 2 2 2 2 2 2 2 2 ...
##  $ brand           : Factor w/ 6 levels &quot;Amstel&quot;,&quot;Budweiser&quot;,..: 1 1 1 1 1 1 1 1 1 1 ...
##  $ week            : int  234 235 237 238 239 240 241 242 243 245 ...
##  $ move_ounce      : num  763.2 741.6 259.2 165.6 21.6 ...
##  $ price_ounce     : num  0.0773 0.0773 0.0887 0.0918 0.0921 ...
##  $ sale_B          : num  0.5 0.5 0 0 0 ...
##  $ sale_C          : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ sale_S          : num  0 0 0 0 0 0 0 0 0 0 ...
##  $ summove_ounce   : num  6071191 6071191 6071191 6071191 6071191 ...
##  $ mean_marketshare: num  0.00341 0.00341 0.00341 0.00341 0.00341 ...
##  $ sharerank       : int  24 24 24 24 24 24 24 24 24 24 ...
##  $ priclow         : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ pricmed         : int  1 1 1 1 1 1 1 1 1 1 ...
##  $ prichigh        : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ saledummy_B     : int  1 1 0 0 0 0 0 0 0 1 ...
##  $ saledummy_C     : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ saledummy_S     : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ promoweek       : int  234 235 NA NA NA NA NA NA NA 245 ...</code></pre>
<p>We will use a slightly different data set to explore additional opportunities for regression analysis. Suppose, you wish to investigate the effect of the variable “store” on sales, which is a categorical variable that can only take two levels (i.e., 98 = store with ID 98, and 100 = store with ID 100). Categorical variables with two levels are also called binary predictors; in our example, however, they are not decoded into typical binary view (i.e., they are not 0 and 1). It is straightforward to include these variables in your model as “dummy” variables. Dummy variables are factor variables that can only take two values. For our “store” variable, we can create a new predictor variable that takes the form:</p>
<p><span class="math display" id="eq:dummycoding">\[\begin{equation}
x_4 =
  \begin{cases}
    0       &amp; \quad \text{if } i \text{th observation comes from store 98}\\
    1  &amp; \quad \text{if } i \text{th observation comes from store 100}
  \end{cases}
\tag{6.21}
\end{equation}\]</span></p>
<p>This new variable is then added to our regression equation from before, so that the equation becomes</p>
<p><span class="math display">\[\begin{align}
Sales =\beta_0 &amp;+\beta_1*price\\
      &amp;+\beta_2*bonus\_buy\\
      &amp;+\beta_3*price\_reduction\\
      &amp;+\beta_4*store+\epsilon
\end{align}\]</span></p>
<p>where “store” represents the new dummy variable and is the coefficient associated with this variable. Estimating the model is straightforward - you just need to include the variable as an additional predictor variable. Note that the variable needs to be specified as a factor variable before including it in your model. If you haven’t converted it to a factor variable before, you could also use the wrapper function <code>as.factor()</code> within the equation.</p>
<p>First, let’s reestimate the regression we had before (note that the result slightly changes because we are using a different data set - you can recall that with different samples, the estimation of true value changes). For the sake of easier interpretation, we use regular regression specification (i.e., not log-log transformed).</p>
<div class="sourceCode" id="cb267"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb267-1"><a href="supervised-learning.html#cb267-1" tabindex="-1"></a>multiple_regression_new <span class="ot">&lt;-</span> <span class="fu">lm</span>(move_ounce <span class="sc">~</span> price_ounce <span class="sc">+</span></span>
<span id="cb267-2"><a href="supervised-learning.html#cb267-2" tabindex="-1"></a>    sale_B <span class="sc">+</span> sale_S, <span class="at">data =</span> categories)</span>
<span id="cb267-3"><a href="supervised-learning.html#cb267-3" tabindex="-1"></a><span class="fu">summary</span>(multiple_regression_new)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = move_ounce ~ price_ounce + sale_B + sale_S, data = categories)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -10570  -2915  -1414    407  56446 
## 
## Coefficients:
##             Estimate Std. Error t value            Pr(&gt;|t|)    
## (Intercept)    14706        458   32.08 &lt;0.0000000000000002 ***
## price_ounce  -146658       5991  -24.48 &lt;0.0000000000000002 ***
## sale_B           601        457    1.32                0.19    
## sale_S         -2340       3015   -0.78                0.44    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 7430 on 3190 degrees of freedom
## Multiple R-squared:  0.16,   Adjusted R-squared:  0.159 
## F-statistic:  202 on 3 and 3190 DF,  p-value: &lt;0.0000000000000002</code></pre>
<p>Now, let’s add the store variable:</p>
<div class="sourceCode" id="cb269"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb269-1"><a href="supervised-learning.html#cb269-1" tabindex="-1"></a>multiple_regression_store <span class="ot">&lt;-</span> <span class="fu">lm</span>(move_ounce <span class="sc">~</span> price_ounce <span class="sc">+</span></span>
<span id="cb269-2"><a href="supervised-learning.html#cb269-2" tabindex="-1"></a>    sale_B <span class="sc">+</span> sale_S <span class="sc">+</span> store, <span class="at">data =</span> categories)</span>
<span id="cb269-3"><a href="supervised-learning.html#cb269-3" tabindex="-1"></a><span class="fu">summary</span>(multiple_regression_store)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = move_ounce ~ price_ounce + sale_B + sale_S + store, 
##     data = categories)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -10710  -3135  -1317    678  55579 
## 
## Coefficients:
##             Estimate Std. Error t value             Pr(&gt;|t|)    
## (Intercept)    13844        474   29.22 &lt; 0.0000000000000002 ***
## price_ounce  -147160       5952  -24.73 &lt; 0.0000000000000002 ***
## sale_B           666        454    1.47                 0.14    
## sale_S         -2428       2995   -0.81                 0.42    
## store100        1725        261    6.60       0.000000000047 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 7380 on 3189 degrees of freedom
## Multiple R-squared:  0.171,  Adjusted R-squared:  0.17 
## F-statistic:  165 on 4 and 3189 DF,  p-value: &lt;0.0000000000000002</code></pre>
<p>You can see that we now have an additional coefficient in the regression output, which tells us the effect of the dummy predictor. The dummy variable can generally be interpreted as the average difference in the dependent variable between the two groups, conditional on the other variables you have included in your model. In this case, the coefficient tells you the difference in sales between store 98 and 100 artists, and whether this difference is significant. Specifically, it means that sales in store 100 are on average 1,724.92 oz higher than in store 98, and this difference is significant (i.e., p &lt; 0.05).</p>
</div>
<div id="more-than-two-categories" class="section level4 hasAnchor" number="6.1.3.2">
<h4><span class="header-section-number">6.1.3.2</span> More than two categories<a href="supervised-learning.html#more-than-two-categories" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Predictors with more than two categories, like our “brand”” variable, can also be included in your model. However, in this case one dummy variable cannot represent all possible values, since there are many brands (i.e., 1 = Amstel, 2 = Budweiser, 3 = Corona, 4 = Fosters, 5 = Heineken, 6 = Old Milwaukee). Thus, we need to create additional dummy variables. For example, for our “brand” variable, we create five dummy variables as follows:</p>
<p><span class="math display" id="eq:dummycoding1">\[\begin{equation}
x_5 =
  \begin{cases}
    1       &amp; \quad \text{if } i \text{th  product is Budweiser}\\
    0  &amp; \quad \text{if } i \text{th product is Amstel}
  \end{cases}
\tag{6.22}
\end{equation}\]</span></p>
<p><span class="math display" id="eq:dummycoding2">\[\begin{equation}
x_6 =
  \begin{cases}
    1       &amp; \quad \text{if } i \text{th  product is Corona}\\
    0  &amp; \quad \text{if } i \text{th product is Amstel}
  \end{cases}
\tag{6.23}
\end{equation}\]</span></p>
<p>and so on.</p>
<p>We would then add these variables as additional predictors in the regression equation and obtain the following model</p>
<p><span class="math display">\[\begin{align}
Sales =\beta_0 &amp;+\beta_1*price\\
      &amp;+\beta_2*bonus\_buy\\
      &amp;+\beta_3*price\_reduction\\
      &amp;+\beta_4*store\\
      &amp;+\beta_5*Budweiser\\
      &amp;+\beta_6*Corona\\
      &amp;+\beta_7*Fosters\\
      &amp;+\beta_8*Heineken\\
      &amp;+\beta_9*Old\_Milwaukee+\epsilon
\end{align}\]</span></p>
<p>where “Budweiser”, “Corona”, “Fosters”, “Heineken”, and “Old Milwaukee” represent our new dummy variables, and refer to the associated regression coefficients. You don’t have to create the dummy variables manually as R will do this automatically when you add the variable to your equation.</p>
<p>The interpretation of the coefficients is as follows: <span class="math inline">\(\beta_5\)</span> is the difference in average sales between the brands “Amstel” and “Budweiser”, <span class="math inline">\(\beta_6\)</span> is the difference in average sales between the brands “Amstel” and “Corona”, and so on. Note that the level for which no dummy variable is created is also referred to as the <em>baseline</em>. In our case, “Amstel” would be the baseline brand. This means that there will always be one fewer dummy variable than the number of levels.</p>
<div class="sourceCode" id="cb271"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb271-1"><a href="supervised-learning.html#cb271-1" tabindex="-1"></a>multiple_regression_ext <span class="ot">&lt;-</span> <span class="fu">lm</span>(move_ounce <span class="sc">~</span> price_ounce <span class="sc">+</span></span>
<span id="cb271-2"><a href="supervised-learning.html#cb271-2" tabindex="-1"></a>    sale_B <span class="sc">+</span> sale_S <span class="sc">+</span> store <span class="sc">+</span> brand, <span class="at">data =</span> categories)</span>
<span id="cb271-3"><a href="supervised-learning.html#cb271-3" tabindex="-1"></a><span class="fu">summary</span>(multiple_regression_ext)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = move_ounce ~ price_ounce + sale_B + sale_S + store + 
##     brand, data = categories)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -13305  -1179   -381   1091  40849 
## 
## Coefficients:
##                   Estimate Std. Error t value             Pr(&gt;|t|)    
## (Intercept)          -1614       1372   -1.18               0.2395    
## price_ounce           7220      15159    0.48               0.6339    
## sale_B                1235        307    4.03  0.00005819767165529 ***
## sale_S                1661       1496    1.11               0.2669    
## store100              1858        129   14.44 &lt; 0.0000000000000002 ***
## brandBudweiser       22299        619   36.05 &lt; 0.0000000000000002 ***
## brandCorona           2289        237    9.66 &lt; 0.0000000000000002 ***
## brandFosters          -110        240   -0.46               0.6472    
## brandHeinekenBeer     1815        220    8.25  0.00000000000000022 ***
## brandOldMilwaukee     2584        838    3.08               0.0021 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 3620 on 3184 degrees of freedom
## Multiple R-squared:  0.801,  Adjusted R-squared:   0.8 
## F-statistic: 1.42e+03 on 9 and 3184 DF,  p-value: &lt;0.0000000000000002</code></pre>
<p>How can we interpret the coefficients? It is estimated based on our model that products from the “Budweiser” brand will on average sell 22,298.63 oz more than products from the “Amstel” brand, and that products from the “Corona” brand will sell on average 2,288.88 oz more than the products from the “Amstel” brand, etc. The p-value of both these and some other brand-variables is smaller than 0.05, suggesting that there is statistical evidence for a real difference in sales between the brands</p>
<p>The level of the baseline category is arbitrary. As you have seen, R simply selects the first level as the baseline. If you would like to use a different baseline category, you can use the <code>relevel()</code> function and set the reference category using the <code>ref</code> argument. The following would estimate the same model using the second category as the baseline:</p>
<div class="sourceCode" id="cb273"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb273-1"><a href="supervised-learning.html#cb273-1" tabindex="-1"></a>multiple_regression_ext <span class="ot">&lt;-</span> <span class="fu">lm</span>(move_ounce <span class="sc">~</span> price_ounce <span class="sc">+</span></span>
<span id="cb273-2"><a href="supervised-learning.html#cb273-2" tabindex="-1"></a>    sale_B <span class="sc">+</span> sale_S <span class="sc">+</span> store <span class="sc">+</span> <span class="fu">relevel</span>(brand, <span class="at">ref =</span> <span class="dv">2</span>),</span>
<span id="cb273-3"><a href="supervised-learning.html#cb273-3" tabindex="-1"></a>    <span class="at">data =</span> categories)</span>
<span id="cb273-4"><a href="supervised-learning.html#cb273-4" tabindex="-1"></a><span class="fu">summary</span>(multiple_regression_ext)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = move_ounce ~ price_ounce + sale_B + sale_S + store + 
##     relevel(brand, ref = 2), data = categories)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -13305  -1179   -381   1091  40849 
## 
## Coefficients:
##                                     Estimate Std. Error t value
## (Intercept)                            20684        811   25.51
## price_ounce                             7220      15159    0.48
## sale_B                                  1235        307    4.03
## sale_S                                  1661       1496    1.11
## store100                                1858        129   14.44
## relevel(brand, ref = 2)Amstel         -22299        619  -36.05
## relevel(brand, ref = 2)Corona         -20010        534  -37.50
## relevel(brand, ref = 2)Fosters        -22409        616  -36.36
## relevel(brand, ref = 2)HeinekenBeer   -20484        625  -32.77
## relevel(brand, ref = 2)OldMilwaukee   -19714        329  -59.88
##                                                 Pr(&gt;|t|)    
## (Intercept)                         &lt; 0.0000000000000002 ***
## price_ounce                                         0.63    
## sale_B                                          0.000058 ***
## sale_S                                              0.27    
## store100                            &lt; 0.0000000000000002 ***
## relevel(brand, ref = 2)Amstel       &lt; 0.0000000000000002 ***
## relevel(brand, ref = 2)Corona       &lt; 0.0000000000000002 ***
## relevel(brand, ref = 2)Fosters      &lt; 0.0000000000000002 ***
## relevel(brand, ref = 2)HeinekenBeer &lt; 0.0000000000000002 ***
## relevel(brand, ref = 2)OldMilwaukee &lt; 0.0000000000000002 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 3620 on 3184 degrees of freedom
## Multiple R-squared:  0.801,  Adjusted R-squared:   0.8 
## F-statistic: 1.42e+03 on 9 and 3184 DF,  p-value: &lt;0.0000000000000002</code></pre>
<p>Note that while your choice of the baseline category impacts the coefficients and the significance level, the prediction for each group will be the same regardless of this choice.</p>
</div>
<div id="non-linear-relationships" class="section level4 hasAnchor" number="6.1.3.3">
<h4><span class="header-section-number">6.1.3.3</span> Non-linear relationships<a href="supervised-learning.html#non-linear-relationships" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div id="multiplicative-model" class="section level5 hasAnchor" number="6.1.3.3.1">
<h5><span class="header-section-number">6.1.3.3.1</span> Multiplicative model<a href="supervised-learning.html#multiplicative-model" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>In many practical applications, linear relationship might not be the case. Let’s review the implications of a linear specification again:</p>
<ul>
<li>Constant marginal returns (e.g., an increase in ad-spend from 10€ to 11€ yields the same increase in sales as an increase from 100,000€ to 100,001€)</li>
<li>Elasticities increase with X (e.g., advertising becomes relatively more effective; i.e., a relatively smaller change in advertising expenditure will yield the same return)</li>
</ul>
<p>In many marketing contexts, these might not be reasonable assumptions. Consider the case of advertising. It is unlikely that the return on advertising will not depend on the level of advertising expenditures. It is rather likely that saturation occurs at some level, meaning that the return from an additional Euro spend on advertising is decreasing with the level of advertising expenditures (i.e., decreasing marginal returns). In other words, at some point the advertising campaign has achieved a certain level of penetration and an additional Euro spend on advertising won’t yield the same return as in the beginning.</p>
<p>Let’s use an example data set, containing the advertising expenditures of a company and the sales (in thousand units).</p>
<div class="sourceCode" id="cb275"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb275-1"><a href="supervised-learning.html#cb275-1" tabindex="-1"></a>non_linear_reg <span class="ot">&lt;-</span> <span class="fu">read.table</span>(<span class="st">&quot;https://raw.githubusercontent.com/IMSMWU/Teaching/master/MRDA2017/non_linear.dat&quot;</span>,</span>
<span id="cb275-2"><a href="supervised-learning.html#cb275-2" tabindex="-1"></a>    <span class="at">sep =</span> <span class="st">&quot;</span><span class="sc">\t</span><span class="st">&quot;</span>, <span class="at">header =</span> <span class="cn">TRUE</span>)  <span class="co">#read in data</span></span>
<span id="cb275-3"><a href="supervised-learning.html#cb275-3" tabindex="-1"></a><span class="fu">head</span>(non_linear_reg)</span></code></pre></div>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["sales"],"name":[1],"type":["dbl"],"align":["right"]},{"label":["advertising"],"name":[2],"type":["dbl"],"align":["right"]}],"data":[{"1":"16","2":"13817"},{"1":"17","2":"5075"},{"1":"15","2":"3501"},{"1":"27","2":"23662"},{"1":"14","2":"9977"},{"1":"19","2":"11972"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<p>Now we inspect if a linear specification is appropriate by looking at the scatterplot:</p>
<div class="sourceCode" id="cb276"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb276-1"><a href="supervised-learning.html#cb276-1" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data =</span> non_linear_reg, <span class="fu">aes</span>(<span class="at">x =</span> advertising,</span>
<span id="cb276-2"><a href="supervised-learning.html#cb276-2" tabindex="-1"></a>    <span class="at">y =</span> sales)) <span class="sc">+</span> <span class="fu">geom_point</span>(<span class="at">shape =</span> <span class="dv">1</span>) <span class="sc">+</span> <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;lm&quot;</span>,</span>
<span id="cb276-3"><a href="supervised-learning.html#cb276-3" tabindex="-1"></a>    <span class="at">fill =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">alpha =</span> <span class="fl">0.1</span>) <span class="sc">+</span> <span class="fu">theme_bw</span>()</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-43"></span>
<img src="07-supervised_learning_files/figure-html/unnamed-chunk-43-1.png" alt="Non-linear relationship" width="672" />
<p class="caption">
Figure 4.9: Non-linear relationship
</p>
</div>
<p>It appears that a linear model might <strong>not</strong> represent the data well. It rather appears that the effect of an additional Euro spend on advertising is decreasing with increasing levels of advertising expenditures. Thus, we have decreasing marginal returns. We could put this to a test and estimate a linear model:</p>
<div class="sourceCode" id="cb277"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb277-1"><a href="supervised-learning.html#cb277-1" tabindex="-1"></a>linear_reg <span class="ot">&lt;-</span> <span class="fu">lm</span>(sales <span class="sc">~</span> advertising, <span class="at">data =</span> non_linear_reg)</span>
<span id="cb277-2"><a href="supervised-learning.html#cb277-2" tabindex="-1"></a><span class="fu">summary</span>(linear_reg)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = sales ~ advertising, data = non_linear_reg)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -10.477  -2.389  -0.356   2.188  16.745 
## 
## Coefficients:
##              Estimate Std. Error t value            Pr(&gt;|t|)    
## (Intercept) 9.9575216  0.2251151    44.2 &lt;0.0000000000000002 ***
## advertising 0.0005024  0.0000156    32.2 &lt;0.0000000000000002 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 3.6 on 998 degrees of freedom
## Multiple R-squared:  0.509,  Adjusted R-squared:  0.509 
## F-statistic: 1.04e+03 on 1 and 998 DF,  p-value: &lt;0.0000000000000002</code></pre>
<p>Advertising appears to be positively related to sales with an additional Euro that is spent on advertising resulting in 0.0005 additional sales. The R<sup>2</sup> statistic suggests that approximately 51% of the total variation can be explained by the model</p>
<p>To test if the linear specification is appropriate, let’s inspect some of the plots that are generated by R. We start by inspecting the residuals plot.</p>
<div class="sourceCode" id="cb279"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb279-1"><a href="supervised-learning.html#cb279-1" tabindex="-1"></a><span class="fu">plot</span>(linear_reg, <span class="dv">1</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-45"></span>
<img src="07-supervised_learning_files/figure-html/unnamed-chunk-45-1.png" alt="Residuals vs. Fitted" width="672" />
<p class="caption">
Figure 6.3: Residuals vs. Fitted
</p>
</div>
<p>The plot suggests that the assumption of homoscedasticity is violated (i.e., the spread of values on the y-axis is different for different levels of the fitted values). In addition, the red line deviates from the dashed grey line, suggesting that the relationship might not be linear. Finally, the Q-Q plot of the residuals suggests that the residuals are not normally distributed.</p>
<div class="sourceCode" id="cb280"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb280-1"><a href="supervised-learning.html#cb280-1" tabindex="-1"></a><span class="fu">plot</span>(linear_reg, <span class="dv">2</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-46"></span>
<img src="07-supervised_learning_files/figure-html/unnamed-chunk-46-1.png" alt="Q-Q plot" width="672" />
<p class="caption">
Figure 6.4: Q-Q plot
</p>
</div>
<p>To sum up, a linear specification might not be the best model for this data set.</p>
<p>In this case, a multiplicative model might be a better representation of the data. The multiplicative model has the following formal representation:</p>
<p><span class="math display" id="eq:multiplicative">\[\begin{equation}
Y =\beta_0 *X_1^{\beta_1}*X_2^{\beta_2}*...*X_J^{\beta_J}*\epsilon
\tag{6.18}
\end{equation}\]</span></p>
<p>This functional form can be linearized by taking the logarithm of both sides of the equation:</p>
<p><span class="math display" id="eq:multiplicativetransformed">\[\begin{equation}
log(Y) =log(\beta_0) + \beta_1*log(X_1) + \beta_2*log(X_2) + ...+ \beta_J*log(X_J) + log(\epsilon)
\tag{6.19}
\end{equation}\]</span></p>
<p>This means that taking logarithms of both sides of the equation makes linear estimation possible. The above transformation follows from two logarithm rules that we apply here:</p>
<ol style="list-style-type: decimal">
<li>the product rule states that <span class="math inline">\(log(xy)=log(x)+log(y)\)</span>; thus, when taking the logarithm of the right hand side of the multiplicative model, we can write <span class="math inline">\(log(X_1) + log(X_2)... log(X_J)\)</span> instead of <span class="math inline">\(log(X_1*X_2*...X_J)\)</span>, and</li>
<li>the power rule states that <span class="math inline">\(log(x^y) = ylog(x)\)</span>; thus, we can write <span class="math inline">\(\beta*log(X)\)</span> instead of <span class="math inline">\(X^{\beta}\)</span></li>
</ol>
<p>Let’s test how the scatterplot would look like if we use the logarithm of our variables using the <code>log()</code> function instead of the original values.</p>
<div class="sourceCode" id="cb281"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb281-1"><a href="supervised-learning.html#cb281-1" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data =</span> non_linear_reg, <span class="fu">aes</span>(<span class="at">x =</span> <span class="fu">log</span>(advertising),</span>
<span id="cb281-2"><a href="supervised-learning.html#cb281-2" tabindex="-1"></a>    <span class="at">y =</span> <span class="fu">log</span>(sales))) <span class="sc">+</span> <span class="fu">geom_point</span>(<span class="at">shape =</span> <span class="dv">1</span>) <span class="sc">+</span> <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;lm&quot;</span>,</span>
<span id="cb281-3"><a href="supervised-learning.html#cb281-3" tabindex="-1"></a>    <span class="at">fill =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">alpha =</span> <span class="fl">0.1</span>) <span class="sc">+</span> <span class="fu">theme_bw</span>()</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-47"></span>
<img src="07-supervised_learning_files/figure-html/unnamed-chunk-47-1.png" alt="Linearized effect" width="672" />
<p class="caption">
Figure 4.10: Linearized effect
</p>
</div>
<p>It appears that now, with the log-transformed variables, a linear specification is a much better representation of the data. Hence, we can log-transform our variables and estimate the following equation:</p>
<p><span class="math display" id="eq:multiplicativetransformed1">\[\begin{equation}
log(sales) = log(\beta_0) + \beta_1*log(advertising) + log(\epsilon)
\tag{6.20}
\end{equation}\]</span></p>
<p>This can be easily implemented in R by transforming the variables using the <code>log()</code> function:</p>
<div class="sourceCode" id="cb282"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb282-1"><a href="supervised-learning.html#cb282-1" tabindex="-1"></a>log_reg <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="fu">log</span>(sales) <span class="sc">~</span> <span class="fu">log</span>(advertising), <span class="at">data =</span> non_linear_reg)</span>
<span id="cb282-2"><a href="supervised-learning.html#cb282-2" tabindex="-1"></a><span class="fu">summary</span>(log_reg)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = log(sales) ~ log(advertising), data = non_linear_reg)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -0.666 -0.127  0.003  0.134  0.640 
## 
## Coefficients:
##                  Estimate Std. Error t value            Pr(&gt;|t|)    
## (Intercept)      -0.01493    0.05971   -0.25                 0.8    
## log(advertising)  0.30077    0.00651   46.20 &lt;0.0000000000000002 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.2 on 998 degrees of freedom
## Multiple R-squared:  0.681,  Adjusted R-squared:  0.681 
## F-statistic: 2.13e+03 on 1 and 998 DF,  p-value: &lt;0.0000000000000002</code></pre>
<p>Note that this specification implies decreasing marginal returns (i.e., the returns of advertising are decreasing with the level of advertising), which appear to be more consistent with the data. The specification is also consistent with proportional changes in advertising being associated with proportional changes in sales (i.e., advertising does not become more effective with increasing levels). This has important implications on the interpretation of the coefficients. In our example, you would interpret the coefficient as follows: <strong>A 1% increase in advertising leads to a 0.3% increase in sales</strong>. Hence, the interpretation is in proportional terms and no longer in units. This means that the coefficients in a log-log model can be directly interpreted as elasticities, which also makes communication easier. We can generally also inspect the R<sup>2</sup> statistic to see that the model fit has increased compared to the linear specification (i.e., R<sup>2</sup> has increased to 0.681 from 0.509). However, please note that the variables are now measured on a different scale, which means that the model fit in theory is not directly comparable. Also, we could use the residuals plot to confirm that the revised specification is more appropriate:</p>
<div class="sourceCode" id="cb284"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb284-1"><a href="supervised-learning.html#cb284-1" tabindex="-1"></a><span class="fu">plot</span>(log_reg, <span class="dv">1</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-49-1"></span>
<img src="07-supervised_learning_files/figure-html/unnamed-chunk-49-1.png" alt="Residuals plot" width="672" />
<p class="caption">
Figure 6.5: Residuals plot
</p>
</div>
<div class="sourceCode" id="cb285"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb285-1"><a href="supervised-learning.html#cb285-1" tabindex="-1"></a><span class="fu">plot</span>(log_reg, <span class="dv">2</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-49-2"></span>
<img src="07-supervised_learning_files/figure-html/unnamed-chunk-49-2.png" alt="Q-Q plot" width="672" />
<p class="caption">
Figure 6.6: Q-Q plot
</p>
</div>
<p>Finally, we can plot the predicted values against the observed values to see that the results from the log-log model (red) provide a better prediction than the results from the linear model (blue).</p>
<div class="sourceCode" id="cb286"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb286-1"><a href="supervised-learning.html#cb286-1" tabindex="-1"></a>non_linear_reg<span class="sc">$</span>pred_lin_reg <span class="ot">&lt;-</span> <span class="fu">predict</span>(linear_reg)</span>
<span id="cb286-2"><a href="supervised-learning.html#cb286-2" tabindex="-1"></a>non_linear_reg<span class="sc">$</span>pred_log_reg <span class="ot">&lt;-</span> <span class="fu">predict</span>(log_reg)</span>
<span id="cb286-3"><a href="supervised-learning.html#cb286-3" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data =</span> non_linear_reg) <span class="sc">+</span> <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">x =</span> advertising,</span>
<span id="cb286-4"><a href="supervised-learning.html#cb286-4" tabindex="-1"></a>    <span class="at">y =</span> sales), <span class="at">shape =</span> <span class="dv">1</span>) <span class="sc">+</span> <span class="fu">geom_line</span>(<span class="at">data =</span> non_linear_reg,</span>
<span id="cb286-5"><a href="supervised-learning.html#cb286-5" tabindex="-1"></a>    <span class="fu">aes</span>(<span class="at">x =</span> advertising, <span class="at">y =</span> pred_lin_reg), <span class="at">color =</span> <span class="st">&quot;blue&quot;</span>,</span>
<span id="cb286-6"><a href="supervised-learning.html#cb286-6" tabindex="-1"></a>    <span class="at">size =</span> <span class="fl">1.05</span>) <span class="sc">+</span> <span class="fu">geom_line</span>(<span class="at">data =</span> non_linear_reg,</span>
<span id="cb286-7"><a href="supervised-learning.html#cb286-7" tabindex="-1"></a>    <span class="fu">aes</span>(<span class="at">x =</span> advertising, <span class="at">y =</span> <span class="fu">exp</span>(pred_log_reg)), <span class="at">color =</span> <span class="st">&quot;red&quot;</span>,</span>
<span id="cb286-8"><a href="supervised-learning.html#cb286-8" tabindex="-1"></a>    <span class="at">size =</span> <span class="fl">1.05</span>) <span class="sc">+</span> <span class="fu">theme_bw</span>()</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-50"></span>
<img src="07-supervised_learning_files/figure-html/unnamed-chunk-50-1.png" alt="Comparison if model fit" width="672" />
<p class="caption">
Figure 4.12: Comparison if model fit
</p>
</div>
</div>
</div>
</div>
</div>
<div id="logistic-regression" class="section level2 hasAnchor" number="6.2">
<h2><span class="header-section-number">6.2</span> Logistic regression<a href="supervised-learning.html#logistic-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="motivation-and-intuition" class="section level3 hasAnchor" number="6.2.1">
<h3><span class="header-section-number">6.2.1</span> Motivation and intuition<a href="supervised-learning.html#motivation-and-intuition" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In the last section we saw how to predict continuous outcomes (e.g., sales) via linear regression models. Another interesting case is that of binary outcomes, i.e. when the variable we want to model can only take two values (yes or no, group 1 or group 2, dead or alive, etc.). To this end we would like to estimate how our predictor variables change the <em>probability</em> of a value being 0 or 1. In this case we can technically still use a linear model (e.g. OLS). However, its predictions will most likely not be particularly useful. A more useful method is the logistic regression. In particular we are going to have a look at the logit model. In the following dataset we are trying to predict whether a customer will churn (i.e., stop being our customer) any time soon. In the first step we are going to use only the “cash-back amount” index as a predictor. Later we are going to add more independent variables.</p>
<div class="sourceCode" id="cb287"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb287-1"><a href="supervised-learning.html#cb287-1" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb287-2"><a href="supervised-learning.html#cb287-2" tabindex="-1"></a><span class="fu">library</span>(gridExtra)</span>
<span id="cb287-3"><a href="supervised-learning.html#cb287-3" tabindex="-1"></a></span>
<span id="cb287-4"><a href="supervised-learning.html#cb287-4" tabindex="-1"></a>churn_data <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;https://raw.githubusercontent.com/WU-RDS/RMA2024/main/data/e_com_data.csv&quot;</span>,</span>
<span id="cb287-5"><a href="supervised-learning.html#cb287-5" tabindex="-1"></a>    <span class="at">sep =</span> <span class="st">&quot;,&quot;</span>, <span class="at">header =</span> T)</span>
<span id="cb287-6"><a href="supervised-learning.html#cb287-6" tabindex="-1"></a><span class="fu">head</span>(churn_data)</span></code></pre></div>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["CustomerID"],"name":[1],"type":["int"],"align":["right"]},{"label":["Churn"],"name":[2],"type":["int"],"align":["right"]},{"label":["Tenure"],"name":[3],"type":["int"],"align":["right"]},{"label":["PreferredLoginDevice"],"name":[4],"type":["chr"],"align":["left"]},{"label":["CityTier"],"name":[5],"type":["int"],"align":["right"]},{"label":["WarehouseToHome"],"name":[6],"type":["int"],"align":["right"]},{"label":["PreferredPaymentMode"],"name":[7],"type":["chr"],"align":["left"]},{"label":["Gender"],"name":[8],"type":["chr"],"align":["left"]},{"label":["HourSpendOnApp"],"name":[9],"type":["int"],"align":["right"]},{"label":["NumberOfDeviceRegistered"],"name":[10],"type":["int"],"align":["right"]},{"label":["PreferedOrderCat"],"name":[11],"type":["chr"],"align":["left"]},{"label":["SatisfactionScore"],"name":[12],"type":["int"],"align":["right"]},{"label":["MaritalStatus"],"name":[13],"type":["chr"],"align":["left"]},{"label":["NumberOfAddress"],"name":[14],"type":["int"],"align":["right"]},{"label":["Complain"],"name":[15],"type":["int"],"align":["right"]},{"label":["OrderAmountHikeFromlastYear"],"name":[16],"type":["int"],"align":["right"]},{"label":["CouponUsed"],"name":[17],"type":["int"],"align":["right"]},{"label":["OrderCount"],"name":[18],"type":["int"],"align":["right"]},{"label":["DaySinceLastOrder"],"name":[19],"type":["int"],"align":["right"]},{"label":["CashbackAmount"],"name":[20],"type":["int"],"align":["right"]}],"data":[{"1":"50001","2":"1","3":"4","4":"Mobile Phone","5":"3","6":"6","7":"Debit Card","8":"Female","9":"3","10":"3","11":"Laptop & Accessory","12":"2","13":"Single","14":"9","15":"1","16":"11","17":"1","18":"1","19":"5","20":"160"},{"1":"50002","2":"1","3":"NA","4":"Phone","5":"1","6":"8","7":"UPI","8":"Male","9":"3","10":"4","11":"Mobile","12":"3","13":"Single","14":"7","15":"1","16":"15","17":"0","18":"1","19":"0","20":"121"},{"1":"50003","2":"1","3":"NA","4":"Phone","5":"1","6":"30","7":"Debit Card","8":"Male","9":"2","10":"4","11":"Mobile","12":"3","13":"Single","14":"6","15":"1","16":"14","17":"0","18":"1","19":"3","20":"120"},{"1":"50004","2":"1","3":"0","4":"Phone","5":"3","6":"15","7":"Debit Card","8":"Male","9":"2","10":"4","11":"Laptop & Accessory","12":"5","13":"Single","14":"8","15":"0","16":"23","17":"0","18":"1","19":"3","20":"134"},{"1":"50005","2":"1","3":"0","4":"Phone","5":"1","6":"12","7":"CC","8":"Male","9":"NA","10":"3","11":"Mobile","12":"5","13":"Single","14":"3","15":"0","16":"11","17":"1","18":"1","19":"3","20":"130"},{"1":"50006","2":"1","3":"0","4":"Computer","5":"1","6":"22","7":"Debit Card","8":"Female","9":"3","10":"5","11":"Mobile Phone","12":"5","13":"Single","14":"2","15":"1","16":"22","17":"4","18":"6","19":"7","20":"139"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<div class="sourceCode" id="cb288"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb288-1"><a href="supervised-learning.html#cb288-1" tabindex="-1"></a><span class="fu">str</span>(churn_data)</span></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    5630 obs. of  20 variables:
##  $ CustomerID                 : int  50001 50002 50003 50004 50005 50006 50007 50008 50009 50010 ...
##  $ Churn                      : int  1 1 1 1 1 1 1 1 1 1 ...
##  $ Tenure                     : int  4 NA NA 0 0 0 NA NA 13 NA ...
##  $ PreferredLoginDevice       : chr  &quot;Mobile Phone&quot; &quot;Phone&quot; &quot;Phone&quot; &quot;Phone&quot; ...
##  $ CityTier                   : int  3 1 1 3 1 1 3 1 3 1 ...
##  $ WarehouseToHome            : int  6 8 30 15 12 22 11 6 9 31 ...
##  $ PreferredPaymentMode       : chr  &quot;Debit Card&quot; &quot;UPI&quot; &quot;Debit Card&quot; &quot;Debit Card&quot; ...
##  $ Gender                     : chr  &quot;Female&quot; &quot;Male&quot; &quot;Male&quot; &quot;Male&quot; ...
##  $ HourSpendOnApp             : int  3 3 2 2 NA 3 2 3 NA 2 ...
##  $ NumberOfDeviceRegistered   : int  3 4 4 4 3 5 3 3 4 5 ...
##  $ PreferedOrderCat           : chr  &quot;Laptop &amp; Accessory&quot; &quot;Mobile&quot; &quot;Mobile&quot; &quot;Laptop &amp; Accessory&quot; ...
##  $ SatisfactionScore          : int  2 3 3 5 5 5 2 2 3 3 ...
##  $ MaritalStatus              : chr  &quot;Single&quot; &quot;Single&quot; &quot;Single&quot; &quot;Single&quot; ...
##  $ NumberOfAddress            : int  9 7 6 8 3 2 4 3 2 2 ...
##  $ Complain                   : int  1 1 1 0 0 1 0 1 1 0 ...
##  $ OrderAmountHikeFromlastYear: int  11 15 14 23 11 22 14 16 14 12 ...
##  $ CouponUsed                 : int  1 0 0 0 1 4 0 2 0 1 ...
##  $ OrderCount                 : int  1 1 1 1 1 6 1 2 1 1 ...
##  $ DaySinceLastOrder          : int  5 0 3 3 3 7 0 0 2 1 ...
##  $ CashbackAmount             : int  160 121 120 134 130 139 121 123 127 123 ...</code></pre>
<div class="sourceCode" id="cb290"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb290-1"><a href="supervised-learning.html#cb290-1" tabindex="-1"></a><span class="co"># Variable &#39;Churn&#39; is 1 if a customer left and 0</span></span>
<span id="cb290-2"><a href="supervised-learning.html#cb290-2" tabindex="-1"></a><span class="co"># else</span></span>
<span id="cb290-3"><a href="supervised-learning.html#cb290-3" tabindex="-1"></a></span>
<span id="cb290-4"><a href="supervised-learning.html#cb290-4" tabindex="-1"></a><span class="co"># correct the variables types</span></span>
<span id="cb290-5"><a href="supervised-learning.html#cb290-5" tabindex="-1"></a>churn_data<span class="sc">$</span>CustomerID <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(churn_data<span class="sc">$</span>CustomerID)</span>
<span id="cb290-6"><a href="supervised-learning.html#cb290-6" tabindex="-1"></a>churn_data<span class="sc">$</span>Gender <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(churn_data<span class="sc">$</span>Gender)</span>
<span id="cb290-7"><a href="supervised-learning.html#cb290-7" tabindex="-1"></a>churn_data<span class="sc">$</span>Tenure <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(churn_data<span class="sc">$</span>Tenure)</span>
<span id="cb290-8"><a href="supervised-learning.html#cb290-8" tabindex="-1"></a>churn_data<span class="sc">$</span>PreferredLoginDevice <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(churn_data<span class="sc">$</span>PreferredLoginDevice)</span>
<span id="cb290-9"><a href="supervised-learning.html#cb290-9" tabindex="-1"></a>churn_data<span class="sc">$</span>CityTier <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(churn_data<span class="sc">$</span>CityTier)</span></code></pre></div>
<p>Below are two attempts to model the data. The left assumes a linear probability model (calculated with the same methods that we used in the last chapter), while the right model is a <strong>logistic regression model</strong>. As you can see, the linear probability model is able of producing probabilities that are above 1 and below 0 (see the lower right corner), which are not valid probabilities, while the logistic model stays between 0 and 1. Notice that songs with a higher cash-back value (on the right of the x-axis) seem to cluster more at <span class="math inline">\(0\)</span> and those with a lower more at <span class="math inline">\(1\)</span> so we expect a negative influence of cash-back value on the probability of churn.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-52"></span>
<img src="07-supervised_learning_files/figure-html/unnamed-chunk-52-1.png" alt="The same binary data explained by two models; A linear probability model (on the left) and a logistic regression model (on the right)" width="672" />
<p class="caption">
Figure 4.13: The same binary data explained by two models; A linear probability model (on the left) and a logistic regression model (on the right)
</p>
</div>
<p>A key insight at this point is that the connection between <span class="math inline">\(\mathbf{X}\)</span> and <span class="math inline">\(Y\)</span> is <strong>non-linear</strong> in the logistic regression model.</p>
</div>
<div id="technical-details-of-the-model" class="section level3 hasAnchor" number="6.2.2">
<h3><span class="header-section-number">6.2.2</span> Technical details of the model<a href="supervised-learning.html#technical-details-of-the-model" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>As the name suggests, the logistic function is an important component of the logistic regression model. It has the following form:</p>
<p><span class="math display">\[
f(\mathbf{X}) = \frac{1}{1 + e^{-\mathbf{X}}}
\]</span>
This function transforms all real numbers into the range between 0 and 1. We need this to model probabilities, as probabilities can only be between 0 and 1.</p>
<p><img src="07-supervised_learning_files/figure-html/unnamed-chunk-53-1.png" width="672" /></p>
<p>The logistic function on its own is not very useful yet, as we want to be able to determine how predictors influence the probability of a value to be equal to 1. To this end we replace the <span class="math inline">\(\mathbf{X}\)</span> in the function above with our familiar linear specification, i.e.</p>
<p><span class="math display">\[
\mathbf{X} = \beta_0 + \beta_1 * x_{1,i} + \beta_2 * x_{2,i} + ... +\beta_m * x_{m,i}\\
f(\mathbf{X}) = P(y_i = 1) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 * x_{1,i} + \beta_2 * x_{2,i} + ... +\beta_m * x_{m,i})}}
\]</span></p>
<p>In our case we only have <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>, the coefficient associated with cash-back.</p>
<p>In general we now have a mathematical relationship between our predictor variables <span class="math inline">\((x_1, ..., x_m)\)</span> and the probability of <span class="math inline">\(y_i\)</span> being equal to one. The last step is to estimate the parameters of this model <span class="math inline">\((\beta_0, \beta_1, ..., \beta_m)\)</span> to determine the magnitude of the effects.</p>
</div>
<div id="estimation-in-r" class="section level3 hasAnchor" number="6.2.3">
<h3><span class="header-section-number">6.2.3</span> Estimation in R<a href="supervised-learning.html#estimation-in-r" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We are now going to show how to perform logistic regression in R. Instead of <code>lm()</code> we now use <code>glm(Y~X, family=binomial(link = 'logit'))</code> to use the logit model. We can still use the <code>summary()</code> command to inspect the output of the model.</p>
<div class="sourceCode" id="cb291"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb291-1"><a href="supervised-learning.html#cb291-1" tabindex="-1"></a><span class="co"># Run the glm</span></span>
<span id="cb291-2"><a href="supervised-learning.html#cb291-2" tabindex="-1"></a>logit_model <span class="ot">&lt;-</span> <span class="fu">glm</span>(Churn <span class="sc">~</span> CashbackAmount, <span class="at">family =</span> <span class="fu">binomial</span>(<span class="at">link =</span> <span class="st">&quot;logit&quot;</span>),</span>
<span id="cb291-3"><a href="supervised-learning.html#cb291-3" tabindex="-1"></a>    <span class="at">data =</span> churn_data)</span>
<span id="cb291-4"><a href="supervised-learning.html#cb291-4" tabindex="-1"></a><span class="co"># Inspect model summary</span></span>
<span id="cb291-5"><a href="supervised-learning.html#cb291-5" tabindex="-1"></a><span class="fu">summary</span>(logit_model)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = Churn ~ CashbackAmount, family = binomial(link = &quot;logit&quot;), 
##     data = churn_data)
## 
## Coefficients:
##                 Estimate Std. Error z value            Pr(&gt;|z|)    
## (Intercept)     0.189865   0.156571    1.21                0.23    
## CashbackAmount -0.010542   0.000936  -11.26 &lt;0.0000000000000002 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 5104.3  on 5629  degrees of freedom
## Residual deviance: 4950.3  on 5628  degrees of freedom
## AIC: 4954
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<p>Noticeably this output does not include an <span class="math inline">\(R^2\)</span> value to asses model fit. Multiple “Pseudo <span class="math inline">\(R^2\)</span>s”, similar to the one used in OLS, have been developed. There are packages that return the <span class="math inline">\(R^2\)</span> given a logit model:</p>
<div class="sourceCode" id="cb293"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb293-1"><a href="supervised-learning.html#cb293-1" tabindex="-1"></a><span class="fu">library</span>(DescTools)</span>
<span id="cb293-2"><a href="supervised-learning.html#cb293-2" tabindex="-1"></a><span class="fu">PseudoR2</span>(logit_model, <span class="at">which =</span> <span class="st">&quot;CoxSnell&quot;</span>)  <span class="co"># you can also use &#39;McFadden&#39;, &#39;McFaddenAdj&#39;, &#39;Nagelkerke&#39;, &#39;AldrichNelson&#39;, &#39;VeallZimmermann&#39;, &#39;Efron&#39;, &#39;McKelveyZavoina&#39;, &#39;Tjur&#39;, &#39;all&#39;</span></span></code></pre></div>
<pre><code>## CoxSnell 
##    0.027</code></pre>
<p>The coefficients of the model give the change in the <a href="https://en.wikipedia.org/wiki/Odds#Statistical_usage">log odds</a> of the dependent variable due to a unit change in the regressor. This makes the exact interpretation of the coefficients difficult, but we can still interpret the signs and the p-values which will tell us if a variable has a significant positive or negative impact on the probability of the dependent variable being <span class="math inline">\(1\)</span>. In order to get the odds ratios we can simply take the exponent of the coefficients.</p>
<div class="sourceCode" id="cb295"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb295-1"><a href="supervised-learning.html#cb295-1" tabindex="-1"></a><span class="fu">exp</span>(<span class="fu">coef</span>(logit_model))</span></code></pre></div>
<pre><code>##    (Intercept) CashbackAmount 
##           1.21           0.99</code></pre>
<p>This now gives the effect on the dependent variable: an additional dollar paid back as cash-back, on average, makes it <span class="math inline">\(0.99\)</span> time more likely (= by a constant factor of 0.99 = 1% less) for a customer to churn.</p>
<p>Note: In some cases, depending on the scales, the coefficient can be extremely large (e.g., due to the fact that the variable is constrained to values between <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span> and the coefficients are for a unit change). We can make the “unit-change” interpretation more meaningful by multiplying the variable by <span class="math inline">\(100\)</span>, thus changing its scale. This linear transformation does not affect the model fit or the p-values.</p>
<p>We observe that cash-back negatively affects the likelihood of churning. To get the confidence intervals for the coefficients we can use the same function as with OLS.</p>
<div class="sourceCode" id="cb297"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb297-1"><a href="supervised-learning.html#cb297-1" tabindex="-1"></a><span class="fu">confint</span>(logit_model)</span></code></pre></div>
<pre><code>##                 2.5 %  97.5 %
## (Intercept)    -0.114  0.5000
## CashbackAmount -0.012 -0.0087</code></pre>
<p>To get the effect of an additional point at a specific value, we can calculate the odds ratio by predicting the probability at a value and at the value <span class="math inline">\(+1\)</span>. For example, if we are interested in how much more (or, in our case, less) likely a customer with cash-back value of 201 compared to 200 is to churn, we can simply calculate the following:</p>
<div class="sourceCode" id="cb299"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb299-1"><a href="supervised-learning.html#cb299-1" tabindex="-1"></a><span class="co"># Probability of churn with a cashback amount of</span></span>
<span id="cb299-2"><a href="supervised-learning.html#cb299-2" tabindex="-1"></a><span class="co"># 200</span></span>
<span id="cb299-3"><a href="supervised-learning.html#cb299-3" tabindex="-1"></a>prob_200 <span class="ot">&lt;-</span> <span class="fu">exp</span>(<span class="sc">-</span>(<span class="sc">-</span><span class="fu">summary</span>(logit_model)<span class="sc">$</span>coefficients[<span class="dv">1</span>,</span>
<span id="cb299-4"><a href="supervised-learning.html#cb299-4" tabindex="-1"></a>    <span class="dv">1</span>] <span class="sc">-</span> <span class="fu">summary</span>(logit_model)<span class="sc">$</span>coefficients[<span class="dv">2</span>, <span class="dv">1</span>] <span class="sc">*</span></span>
<span id="cb299-5"><a href="supervised-learning.html#cb299-5" tabindex="-1"></a>    <span class="dv">200</span>))</span>
<span id="cb299-6"><a href="supervised-learning.html#cb299-6" tabindex="-1"></a>prob_200</span></code></pre></div>
<pre><code>## [1] 0.15</code></pre>
<div class="sourceCode" id="cb301"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb301-1"><a href="supervised-learning.html#cb301-1" tabindex="-1"></a><span class="co"># Probability of churn with a cashback amount of</span></span>
<span id="cb301-2"><a href="supervised-learning.html#cb301-2" tabindex="-1"></a><span class="co"># 201</span></span>
<span id="cb301-3"><a href="supervised-learning.html#cb301-3" tabindex="-1"></a>prob_201 <span class="ot">&lt;-</span> <span class="fu">exp</span>(<span class="sc">-</span>(<span class="sc">-</span><span class="fu">summary</span>(logit_model)<span class="sc">$</span>coefficients[<span class="dv">1</span>,</span>
<span id="cb301-4"><a href="supervised-learning.html#cb301-4" tabindex="-1"></a>    <span class="dv">1</span>] <span class="sc">-</span> <span class="fu">summary</span>(logit_model)<span class="sc">$</span>coefficients[<span class="dv">2</span>, <span class="dv">1</span>] <span class="sc">*</span></span>
<span id="cb301-5"><a href="supervised-learning.html#cb301-5" tabindex="-1"></a>    <span class="dv">201</span>))</span>
<span id="cb301-6"><a href="supervised-learning.html#cb301-6" tabindex="-1"></a>prob_201</span></code></pre></div>
<pre><code>## [1] 0.15</code></pre>
<div class="sourceCode" id="cb303"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb303-1"><a href="supervised-learning.html#cb303-1" tabindex="-1"></a><span class="co"># Odds ratio</span></span>
<span id="cb303-2"><a href="supervised-learning.html#cb303-2" tabindex="-1"></a>prob_201<span class="sc">/</span>prob_200</span></code></pre></div>
<pre><code>## [1] 0.99</code></pre>
<p>This is essentially what we got in the regression output earlier. So the odds are 1% lower at 201 than at 200 cash-back.</p>
<div id="logistic-model-with-multiple-predictors" class="section level4 hasAnchor" number="6.2.3.1">
<h4><span class="header-section-number">6.2.3.1</span> Logistic model with multiple predictors<a href="supervised-learning.html#logistic-model-with-multiple-predictors" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Of course we can also use multiple predictors in logistic regression as shown in the formula above. We might want to add order amount from last year, days since last order, warehouse-to-home distance, and order count.</p>
<p>Again, the familiar formula interface can be used with the <code>glm()</code> function. All the model summaries shown above still work with multiple predictors.</p>
<div class="sourceCode" id="cb305"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb305-1"><a href="supervised-learning.html#cb305-1" tabindex="-1"></a>multiple_logit_model <span class="ot">&lt;-</span> <span class="fu">glm</span>(Churn <span class="sc">~</span> OrderAmountHikeFromlastYear <span class="sc">+</span></span>
<span id="cb305-2"><a href="supervised-learning.html#cb305-2" tabindex="-1"></a>    DaySinceLastOrder <span class="sc">+</span> WarehouseToHome <span class="sc">+</span> OrderCount <span class="sc">+</span></span>
<span id="cb305-3"><a href="supervised-learning.html#cb305-3" tabindex="-1"></a>    CashbackAmount, <span class="at">family =</span> <span class="fu">binomial</span>(<span class="at">link =</span> <span class="st">&quot;logit&quot;</span>),</span>
<span id="cb305-4"><a href="supervised-learning.html#cb305-4" tabindex="-1"></a>    <span class="at">data =</span> churn_data)</span>
<span id="cb305-5"><a href="supervised-learning.html#cb305-5" tabindex="-1"></a><span class="fu">summary</span>(multiple_logit_model)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = Churn ~ OrderAmountHikeFromlastYear + DaySinceLastOrder + 
##     WarehouseToHome + OrderCount + CashbackAmount, family = binomial(link = &quot;logit&quot;), 
##     data = churn_data)
## 
## Coefficients:
##                             Estimate Std. Error z value             Pr(&gt;|z|)
## (Intercept)                 -0.16916    0.28264   -0.60                 0.55
## OrderAmountHikeFromlastYear  0.00679    0.01104    0.62                 0.54
## DaySinceLastOrder           -0.16095    0.01667   -9.66 &lt; 0.0000000000000002
## WarehouseToHome              0.02481    0.00459    5.41      0.0000000630563
## OrderCount                   0.10421    0.01959    5.32      0.0000001046673
## CashbackAmount              -0.00978    0.00140   -6.96      0.0000000000033
##                                
## (Intercept)                    
## OrderAmountHikeFromlastYear    
## DaySinceLastOrder           ***
## WarehouseToHome             ***
## OrderCount                  ***
## CashbackAmount              ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 4162.4  on 4548  degrees of freedom
## Residual deviance: 3942.0  on 4543  degrees of freedom
##   (1081 observations deleted due to missingness)
## AIC: 3954
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<div class="sourceCode" id="cb307"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb307-1"><a href="supervised-learning.html#cb307-1" tabindex="-1"></a><span class="fu">PseudoR2</span>(multiple_logit_model, <span class="at">which =</span> <span class="st">&quot;CoxSnell&quot;</span>)</span></code></pre></div>
<pre><code>## CoxSnell 
##    0.047</code></pre>
<p>Again, to properly interpret the coefficient, we extract odds ratio:</p>
<div class="sourceCode" id="cb309"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb309-1"><a href="supervised-learning.html#cb309-1" tabindex="-1"></a><span class="fu">exp</span>(<span class="fu">coef</span>(multiple_logit_model))</span></code></pre></div>
<pre><code>##                 (Intercept) OrderAmountHikeFromlastYear 
##                        0.84                        1.01 
##           DaySinceLastOrder             WarehouseToHome 
##                        0.85                        1.03 
##                  OrderCount              CashbackAmount 
##                        1.11                        0.99</code></pre>
<div class="sourceCode" id="cb311"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb311-1"><a href="supervised-learning.html#cb311-1" tabindex="-1"></a><span class="fu">confint</span>(multiple_logit_model)</span></code></pre></div>
<pre><code>## Waiting for profiling to be done...</code></pre>
<pre><code>##                              2.5 %  97.5 %
## (Intercept)                 -0.720  0.3878
## OrderAmountHikeFromlastYear -0.015  0.0283
## DaySinceLastOrder           -0.194 -0.1287
## WarehouseToHome              0.016  0.0338
## OrderCount                   0.065  0.1423
## CashbackAmount              -0.013 -0.0071</code></pre>
<p>We can see that such variables as warehouse-to-home distance and order count increase the probability of churn, while days since last order and cash-back value decrease it.</p>
</div>
<div id="model-selection" class="section level4 hasAnchor" number="6.2.3.2">
<h4><span class="header-section-number">6.2.3.2</span> Model selection<a href="supervised-learning.html#model-selection" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The question remains, whether a variable <em>should</em> be added to the model. We will present two methods for model selection for logistic regression. The first is based on the <em>Akaike Information Criterium</em> (AIC). It is reported with the summary output for logit models. The value of the AIC is <strong>relative</strong>, meaning that it has no interpretation by itself. However, it can be used to compare and select models. The model with the lowest AIC value is the one that should be chosen. Note that the AIC does not indicate how well the model fits the data, but is merely used to compare models.</p>
<p>For example, consider the following model, where we exclude the <code>order count</code> predictor. Seeing as it was able to contribute significantly to the explanatory power of the model, the AIC increases, indicating that the model including <code>order count</code> is better suited to explain the data. We always want the <em>lowest</em> possible AIC.</p>
<div class="sourceCode" id="cb314"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb314-1"><a href="supervised-learning.html#cb314-1" tabindex="-1"></a>multiple_logit_model2 <span class="ot">&lt;-</span> <span class="fu">glm</span>(Churn <span class="sc">~</span> OrderAmountHikeFromlastYear <span class="sc">+</span></span>
<span id="cb314-2"><a href="supervised-learning.html#cb314-2" tabindex="-1"></a>    DaySinceLastOrder <span class="sc">+</span> WarehouseToHome <span class="sc">+</span> CashbackAmount,</span>
<span id="cb314-3"><a href="supervised-learning.html#cb314-3" tabindex="-1"></a>    <span class="at">family =</span> <span class="fu">binomial</span>(<span class="at">link =</span> <span class="st">&quot;logit&quot;</span>), <span class="at">data =</span> churn_data)</span>
<span id="cb314-4"><a href="supervised-learning.html#cb314-4" tabindex="-1"></a><span class="fu">summary</span>(multiple_logit_model2)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = Churn ~ OrderAmountHikeFromlastYear + DaySinceLastOrder + 
##     WarehouseToHome + CashbackAmount, family = binomial(link = &quot;logit&quot;), 
##     data = churn_data)
## 
## Coefficients:
##                             Estimate Std. Error z value           Pr(&gt;|z|)    
## (Intercept)                 -0.12798    0.26542   -0.48               0.63    
## OrderAmountHikeFromlastYear  0.00281    0.01092    0.26               0.80    
## DaySinceLastOrder           -0.11361    0.01390   -8.17 0.0000000000000003 ***
## WarehouseToHome              0.02556    0.00448    5.70 0.0000000119246808 ***
## CashbackAmount              -0.00912    0.00126   -7.25 0.0000000000004033 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 4315.0  on 4806  degrees of freedom
## Residual deviance: 4100.4  on 4802  degrees of freedom
##   (823 observations deleted due to missingness)
## AIC: 4110
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<p>As a second measure for variable selection, you can use the pseudo <span class="math inline">\(R^2\)</span>s as shown above. The fit is worse according to all three values presented here, when excluding the order count.</p>
<div class="sourceCode" id="cb316"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb316-1"><a href="supervised-learning.html#cb316-1" tabindex="-1"></a><span class="fu">PseudoR2</span>(multiple_logit_model2, <span class="at">which =</span> <span class="st">&quot;CoxSnell&quot;</span>)</span></code></pre></div>
<pre><code>## CoxSnell 
##    0.044</code></pre>
</div>
<div id="predictions" class="section level4 hasAnchor" number="6.2.3.3">
<h4><span class="header-section-number">6.2.3.3</span> Predictions<a href="supervised-learning.html#predictions" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>We can predict the probability given an observation using the <code>predict(my_logit, newdata = ..., type = "response")</code> function. Replace <code>...</code> with the observed values for which you would like to predict the outcome variable.</p>
<div class="sourceCode" id="cb318"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb318-1"><a href="supervised-learning.html#cb318-1" tabindex="-1"></a><span class="co"># Prediction for one observation</span></span>
<span id="cb318-2"><a href="supervised-learning.html#cb318-2" tabindex="-1"></a><span class="fu">predict</span>(multiple_logit_model, <span class="at">newdata =</span> <span class="fu">data.frame</span>(<span class="at">OrderAmountHikeFromlastYear =</span> <span class="dv">5</span>,</span>
<span id="cb318-3"><a href="supervised-learning.html#cb318-3" tabindex="-1"></a>    <span class="at">DaySinceLastOrder =</span> <span class="dv">30</span>, <span class="at">WarehouseToHome =</span> <span class="dv">10</span>, <span class="at">OrderCount =</span> <span class="dv">10</span>,</span>
<span id="cb318-4"><a href="supervised-learning.html#cb318-4" tabindex="-1"></a>    <span class="at">CashbackAmount =</span> <span class="dv">300</span>), <span class="at">type =</span> <span class="st">&quot;response&quot;</span>)</span></code></pre></div>
<pre><code>##      1 
## 0.0013</code></pre>
<p>The prediction indicates that a customer who made <span class="math inline">\(5\)</span> orders last year, <span class="math inline">\(10\)</span> orders in total, last order happened <span class="math inline">\(30\)</span> days ago, warehouse-to-home distance is 10 km, and cash-back amount is 300 dollars, has a tiny (<span class="math inline">\(0.2\%\)</span> chance of churning.</p>
</div>
</div>
</div>
<div id="learning-check-4" class="section level2 unnumbered hasAnchor">
<h2>Learning check<a href="supervised-learning.html#learning-check-4" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>(LC4.1) What is a correlation coefficient?</strong></p>
<ul class="task-list">
<li><label><input type="checkbox" />It describes the difference in means of two variables</label></li>
<li><label><input type="checkbox" />It describes the causal relation between two variables</label></li>
<li><label><input type="checkbox" />It is the standardized covariance</label></li>
<li><label><input type="checkbox" />It describes the degree to which the variation in one variable is related to the variation in another variable</label></li>
<li><label><input type="checkbox" />None of the above</label></li>
</ul>
<p><strong>(LC4.2) Which line through a scatterplot produces the best fit in a linear regression model?</strong></p>
<ul class="task-list">
<li><label><input type="checkbox" />The line associated with the steepest slope parameter</label></li>
<li><label><input type="checkbox" />The line that minimizes the sum of the squared deviations of the predicted values (regression line) from the observed values</label></li>
<li><label><input type="checkbox" />The line that minimizes the sum of the squared residuals</label></li>
<li><label><input type="checkbox" />The line that maximizes the sum of the squared residuals</label></li>
<li><label><input type="checkbox" />None of the above</label></li>
</ul>
<p><strong>(LC4.3) Which of the following statements about the adjusted R-squared is TRUE?</strong></p>
<ul class="task-list">
<li><label><input type="checkbox" />It is always larger than the regular <span class="math inline">\(R^{2}\)</span></label></li>
<li><label><input type="checkbox" />It increases with every additional variable</label></li>
<li><label><input type="checkbox" />It increases only with additional variables that add more explanatory power than pure chance</label></li>
<li><label><input type="checkbox" />It contains a “penalty” for including unnecessary variables</label></li>
<li><label><input type="checkbox" />None of the above</label></li>
</ul>
<p><strong>(LC4.4) When do you use a logistic regression model?</strong></p>
<ul class="task-list">
<li><label><input type="checkbox" />When the dependent variable is continuous</label></li>
<li><label><input type="checkbox" />When the independent and dependent variables are binary</label></li>
<li><label><input type="checkbox" />When the dependent variable is binary</label></li>
<li><label><input type="checkbox" />None of the above</label></li>
</ul>
<p><strong>(LC4.5) What is the correct way to implement a linear regression model in R? (x = independent variable, y = dependent variable)?</strong></p>
<ul class="task-list">
<li><label><input type="checkbox" /><code>lm(y~x, data=data)</code></label></li>
<li><label><input type="checkbox" /><code>lm(x~y + error, data=data)</code></label></li>
<li><label><input type="checkbox" /><code>lm(x~y, data=data)</code></label></li>
<li><label><input type="checkbox" /><code>lm(y~x + error, data=data)</code></label></li>
<li><label><input type="checkbox" />None of the above</label></li>
</ul>
<p><strong>(LC4.6) Consider the output from a bivariate correlation below</strong></p>
<ul class="task-list">
<li><label><input type="checkbox" />…lower prices cause higher sales.</label></li>
<li><label><input type="checkbox" />…lower prices are associated with higher sales and vice versa.</label></li>
<li><label><input type="checkbox" />None of the above</label></li>
</ul>
<p><img src="images/cor_table11.png" width="75%" style="display: block; margin: auto;" />
<strong>(LC4.7) When interpreting the statistical significance of a regression coefficient, the p-value…</strong></p>
<ul class="task-list">
<li><label><input type="checkbox" />…of 0.05 means that, if the null hypothesis is true (i.e., if the independent variable would NOT affect the outcome), the odds are 19 in 20 of getting a regression coefficient as large or larger than the estimated coefficient</label></li>
<li><label><input type="checkbox" />…of 0.05 means that, if the null hypothesis is true (i.e., if the independent variable would NOT affect the outcome), the odds are 1 in 20 of getting a regression coefficient as large or larger than the estimated coefficient</label></li>
<li><label><input type="checkbox" />…of 0.05 means that the effect is statistically significant at the 5% level.</label></li>
<li><label><input type="checkbox" />…does not tell you anything about the importance of the effect</label></li>
<li><label><input type="checkbox" />…will get smaller, the larger the calculated value of the test statistic (t-value).</label></li>
</ul>
<p><strong>(LC4.8) In which setting(s) would a regression coefficient be interpreted as “statistically significant”?</strong></p>
<ul class="task-list">
<li><label><input type="checkbox" />When the absolute value of the calculated test-statistic (e.g., t-value) exceeds the critical value of the test statistic at your specified significance level (e.g., 0.05)</label></li>
<li><label><input type="checkbox" />When the test-statistic (e.g., t-value) is lower than the critical value of the test statistic at your specified significance level (e.g., 0.05)</label></li>
<li><label><input type="checkbox" />When the confidence interval associated with the test does not contain zero</label></li>
<li><label><input type="checkbox" />When the p-value is smaller than your specified significance level (e.g., 0.05)</label></li>
</ul>
<p><strong>(LC4.9) When interpreting the significance of the coefficients in a regression model, what is the relationship between the test statistic (e.g., t-value) and the p-value?</strong></p>
<ul class="task-list">
<li><label><input type="checkbox" />The lower the absolute value of the test statistic, the lower the p-value</label></li>
<li><label><input type="checkbox" />The higher the absolute value of the test statistic, the higher the p-value</label></li>
<li><label><input type="checkbox" />There is no connection between the test statistic and the p-value</label></li>
<li><label><input type="checkbox" />The higher the absolute value of the test statistic, the lower the p-value</label></li>
</ul>
<p><strong>(LC4.10) What does the term overfitting refer to?</strong></p>
<ul class="task-list">
<li><label><input type="checkbox" />A regression model that fits to a specific data set so poorly, that it will not generalize to other samples</label></li>
<li><label><input type="checkbox" />A regression model that fits to a specific data set so well, that it will generalize to other samples particularly well</label></li>
<li><label><input type="checkbox" />A regression model that has too many predictor variables</label></li>
<li><label><input type="checkbox" />A regression model that fits to a specific data set so well, that it will only predict well within the sample but not generalize to other samples</label></li>
</ul>
</div>
<div id="references-3" class="section level2 unnumbered hasAnchor">
<h2>References<a href="supervised-learning.html#references-3" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>Field, A., Miles J., &amp; Field, Z. (2012): Discovering Statistics Using R. Sage Publications (<strong>chapters 6, 7, 8</strong>).</li>
<li>James, G., Witten, D., Hastie, T., &amp; Tibshirani, R. (2013): An Introduction to Statistical Learning with Applications in R, Springer (<strong>chapter 3</strong>)</li>
</ul>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="statistical-inference.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="unsupervised-learning.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
