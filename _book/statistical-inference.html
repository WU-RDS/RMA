<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>5 Statistical inference | Retail Marketing Analytics 2024</title>
  <meta name="description" content="An Introduction to Data Analytics Using R" />
  <meta name="generator" content="bookdown 0.39 and GitBook 2.6.7" />

  <meta property="og:title" content="5 Statistical inference | Retail Marketing Analytics 2024" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="An Introduction to Data Analytics Using R" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="5 Statistical inference | Retail Marketing Analytics 2024" />
  
  <meta name="twitter:description" content="An Introduction to Data Analytics Using R" />
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  <link rel="shortcut icon" href="favicon.ico" type="image/x-icon" />
<link rel="prev" href="summarizing-data.html"/>
<link rel="next" href="supervised-learning.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/pagedtable-1.1/css/pagedtable.css" rel="stylesheet" />
<script src="libs/pagedtable-1.1/js/pagedtable.js"></script>
<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><strong><a href="./">RMA 2024</a></strong></li>

<li class="divider"></li>
<li class="part"><span><b>I Course outline</b></span></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome!</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a>
<ul>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#course-structure"><i class="fa fa-check"></i>Course structure</a>
<ul>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#contents"><i class="fa fa-check"></i>Contents</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#grading"><i class="fa fa-check"></i>Grading</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#course-materials"><i class="fa fa-check"></i>Course materials</a>
<ul>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#main-reference"><i class="fa fa-check"></i>Main reference</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#further-readings"><i class="fa fa-check"></i>Further readings</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#discussion-forum"><i class="fa fa-check"></i>Discussion forum</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#other-web-resources"><i class="fa fa-check"></i>Other web-resources</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#contact"><i class="fa fa-check"></i>Contact</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#acknowledgements"><i class="fa fa-check"></i>Acknowledgements</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>II Lecture notes</b></span></li>
<li class="chapter" data-level="1" data-path="preliminaries.html"><a href="preliminaries.html"><i class="fa fa-check"></i><b>1</b> Preliminaries</a>
<ul>
<li class="chapter" data-level="1.1" data-path="preliminaries.html"><a href="preliminaries.html#marketing-foundations-recap"><i class="fa fa-check"></i><b>1.1</b> Marketing foundations (recap)</a></li>
<li class="chapter" data-level="1.2" data-path="preliminaries.html"><a href="preliminaries.html#the-research-process"><i class="fa fa-check"></i><b>1.2</b> The research process</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="preliminaries.html"><a href="preliminaries.html#research-question-and-hypothesis"><i class="fa fa-check"></i><b>1.2.1</b> Research question and hypothesis</a></li>
<li class="chapter" data-level="1.2.2" data-path="preliminaries.html"><a href="preliminaries.html#choosing-a-research-design"><i class="fa fa-check"></i><b>1.2.2</b> Choosing a research design</a></li>
<li class="chapter" data-level="1.2.3" data-path="preliminaries.html"><a href="preliminaries.html#collecting-data"><i class="fa fa-check"></i><b>1.2.3</b> Collecting data</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="preliminaries.html"><a href="preliminaries.html#learning-check"><i class="fa fa-check"></i>Learning check</a></li>
<li class="chapter" data-level="" data-path="preliminaries.html"><a href="preliminaries.html#references"><i class="fa fa-check"></i>References</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="getting-started-with-r.html"><a href="getting-started-with-r.html"><i class="fa fa-check"></i><b>2</b> Getting started with R</a>
<ul>
<li class="chapter" data-level="2.1" data-path="getting-started-with-r.html"><a href="getting-started-with-r.html#how-to-download-and-install-r-and-rstudio"><i class="fa fa-check"></i><b>2.1</b> How to download and install R and RStudio</a></li>
<li class="chapter" data-level="2.2" data-path="getting-started-with-r.html"><a href="getting-started-with-r.html#the-r-studio-interface"><i class="fa fa-check"></i><b>2.2</b> The R Studio interface</a></li>
<li class="chapter" data-level="2.3" data-path="getting-started-with-r.html"><a href="getting-started-with-r.html#functions"><i class="fa fa-check"></i><b>2.3</b> Functions</a></li>
<li class="chapter" data-level="2.4" data-path="getting-started-with-r.html"><a href="getting-started-with-r.html#packages"><i class="fa fa-check"></i><b>2.4</b> Packages</a></li>
<li class="chapter" data-level="2.5" data-path="getting-started-with-r.html"><a href="getting-started-with-r.html#a-typical-r-session"><i class="fa fa-check"></i><b>2.5</b> A typical R session</a></li>
<li class="chapter" data-level="2.6" data-path="getting-started-with-r.html"><a href="getting-started-with-r.html#getting-help"><i class="fa fa-check"></i><b>2.6</b> Getting help</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="data-handling.html"><a href="data-handling.html"><i class="fa fa-check"></i><b>3</b> Data handling</a>
<ul>
<li class="chapter" data-level="3.1" data-path="data-handling.html"><a href="data-handling.html#basic-data-handling"><i class="fa fa-check"></i><b>3.1</b> Basic data handling</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="data-handling.html"><a href="data-handling.html#creating-objects"><i class="fa fa-check"></i><b>3.1.1</b> Creating objects</a></li>
<li class="chapter" data-level="3.1.2" data-path="data-handling.html"><a href="data-handling.html#data-types"><i class="fa fa-check"></i><b>3.1.2</b> Data types</a></li>
<li class="chapter" data-level="3.1.3" data-path="data-handling.html"><a href="data-handling.html#data-structures"><i class="fa fa-check"></i><b>3.1.3</b> Data structures</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="data-handling.html"><a href="data-handling.html#data-import-and-export"><i class="fa fa-check"></i><b>3.2</b> Data import and export</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="data-handling.html"><a href="data-handling.html#getting-data-for-this-course"><i class="fa fa-check"></i><b>3.2.1</b> Getting data for this course</a></li>
<li class="chapter" data-level="3.2.2" data-path="data-handling.html"><a href="data-handling.html#export-data"><i class="fa fa-check"></i><b>3.2.2</b> Export data</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="data-handling.html"><a href="data-handling.html#learning-check-1"><i class="fa fa-check"></i>Learning check</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="summarizing-data.html"><a href="summarizing-data.html"><i class="fa fa-check"></i><b>4</b> Summarizing data</a>
<ul>
<li class="chapter" data-level="4.1" data-path="summarizing-data.html"><a href="summarizing-data.html#summary-statistics"><i class="fa fa-check"></i><b>4.1</b> Summary statistics</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="summarizing-data.html"><a href="summarizing-data.html#categorical-variables"><i class="fa fa-check"></i><b>4.1.1</b> Categorical variables</a></li>
<li class="chapter" data-level="4.1.2" data-path="summarizing-data.html"><a href="summarizing-data.html#continuous-variables"><i class="fa fa-check"></i><b>4.1.2</b> Continuous variables</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="summarizing-data.html"><a href="summarizing-data.html#data-visualization"><i class="fa fa-check"></i><b>4.2</b> Data visualization</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="summarizing-data.html"><a href="summarizing-data.html#categorical-variables-1"><i class="fa fa-check"></i><b>4.2.1</b> Categorical variables</a></li>
<li class="chapter" data-level="4.2.2" data-path="summarizing-data.html"><a href="summarizing-data.html#continuous-variables-1"><i class="fa fa-check"></i><b>4.2.2</b> Continuous variables</a></li>
<li class="chapter" data-level="4.2.3" data-path="summarizing-data.html"><a href="summarizing-data.html#saving-plots"><i class="fa fa-check"></i><b>4.2.3</b> Saving plots</a></li>
<li class="chapter" data-level="4.2.4" data-path="summarizing-data.html"><a href="summarizing-data.html#ggplot-extensions"><i class="fa fa-check"></i><b>4.2.4</b> ggplot extensions</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="summarizing-data.html"><a href="summarizing-data.html#learning-check-2"><i class="fa fa-check"></i>Learning check</a></li>
<li class="chapter" data-level="" data-path="summarizing-data.html"><a href="summarizing-data.html#references-1"><i class="fa fa-check"></i>References</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="statistical-inference.html"><a href="statistical-inference.html"><i class="fa fa-check"></i><b>5</b> Statistical inference</a>
<ul>
<li class="chapter" data-level="5.1" data-path="statistical-inference.html"><a href="statistical-inference.html#if-we-knew-it-all"><i class="fa fa-check"></i><b>5.1</b> If we knew it all</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="statistical-inference.html"><a href="statistical-inference.html#sampling-from-a-known-population"><i class="fa fa-check"></i><b>5.1.1</b> Sampling from a known population</a></li>
<li class="chapter" data-level="5.1.2" data-path="statistical-inference.html"><a href="statistical-inference.html#standard-error-of-the-mean"><i class="fa fa-check"></i><b>5.1.2</b> Standard error of the mean</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="statistical-inference.html"><a href="statistical-inference.html#the-central-limit-theorem"><i class="fa fa-check"></i><b>5.2</b> The Central Limit Theorem</a></li>
<li class="chapter" data-level="5.3" data-path="statistical-inference.html"><a href="statistical-inference.html#using-what-we-actually-know"><i class="fa fa-check"></i><b>5.3</b> Using what we actually know</a></li>
<li class="chapter" data-level="5.4" data-path="statistical-inference.html"><a href="statistical-inference.html#confidence-intervals-for-the-sample-mean"><i class="fa fa-check"></i><b>5.4</b> Confidence Intervals for the Sample Mean</a></li>
<li class="chapter" data-level="5.5" data-path="statistical-inference.html"><a href="statistical-inference.html#null-hypothesis-statistical-testing-nhst"><i class="fa fa-check"></i><b>5.5</b> Null Hypothesis Statistical Testing (NHST)</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="statistical-inference.html"><a href="statistical-inference.html#the-null-hypothesis"><i class="fa fa-check"></i><b>5.5.1</b> The null hypothesis</a></li>
<li class="chapter" data-level="5.5.2" data-path="statistical-inference.html"><a href="statistical-inference.html#statistical-inference-on-a-sample"><i class="fa fa-check"></i><b>5.5.2</b> Statistical inference on a sample</a></li>
<li class="chapter" data-level="5.5.3" data-path="statistical-inference.html"><a href="statistical-inference.html#nhst-considerations"><i class="fa fa-check"></i><b>5.5.3</b> NHST considerations</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="statistical-inference.html"><a href="statistical-inference.html#learning-check-3"><i class="fa fa-check"></i>Learning check</a></li>
<li class="chapter" data-level="" data-path="statistical-inference.html"><a href="statistical-inference.html#references-2"><i class="fa fa-check"></i>References</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="supervised-learning.html"><a href="supervised-learning.html"><i class="fa fa-check"></i><b>6</b> Supervised learning</a>
<ul>
<li class="chapter" data-level="6.1" data-path="supervised-learning.html"><a href="supervised-learning.html#linear-regression"><i class="fa fa-check"></i><b>6.1</b> Linear regression</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="supervised-learning.html"><a href="supervised-learning.html#correlation"><i class="fa fa-check"></i><b>6.1.1</b> Correlation</a></li>
<li class="chapter" data-level="6.1.2" data-path="supervised-learning.html"><a href="supervised-learning.html#regression-analysis"><i class="fa fa-check"></i><b>6.1.2</b> Regression analysis</a></li>
<li class="chapter" data-level="6.1.3" data-path="supervised-learning.html"><a href="supervised-learning.html#categorical-predictors"><i class="fa fa-check"></i><b>6.1.3</b> Categorical predictors</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="supervised-learning.html"><a href="supervised-learning.html#logistic-regression"><i class="fa fa-check"></i><b>6.2</b> Logistic regression</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="supervised-learning.html"><a href="supervised-learning.html#motivation-and-intuition"><i class="fa fa-check"></i><b>6.2.1</b> Motivation and intuition</a></li>
<li class="chapter" data-level="6.2.2" data-path="supervised-learning.html"><a href="supervised-learning.html#technical-details-of-the-model"><i class="fa fa-check"></i><b>6.2.2</b> Technical details of the model</a></li>
<li class="chapter" data-level="6.2.3" data-path="supervised-learning.html"><a href="supervised-learning.html#estimation-in-r"><i class="fa fa-check"></i><b>6.2.3</b> Estimation in R</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="supervised-learning.html"><a href="supervised-learning.html#learning-check-4"><i class="fa fa-check"></i>Learning check</a></li>
<li class="chapter" data-level="" data-path="supervised-learning.html"><a href="supervised-learning.html#references-3"><i class="fa fa-check"></i>References</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html"><i class="fa fa-check"></i><b>7</b> Unsupervised learning</a>
<ul>
<li class="chapter" data-level="7.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#principal-component-analysis"><i class="fa fa-check"></i><b>7.1</b> Principal component analysis</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#introduction-1"><i class="fa fa-check"></i><b>7.1.1</b> Introduction</a></li>
<li class="chapter" data-level="7.1.2" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#steps-in-factor-analysis"><i class="fa fa-check"></i><b>7.1.2</b> Steps in factor analysis</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#cluster-analysis"><i class="fa fa-check"></i><b>7.2</b> Cluster analysis</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#k-means"><i class="fa fa-check"></i><b>7.2.1</b> K-Means</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#learning-check-5"><i class="fa fa-check"></i>Learning check</a></li>
<li class="chapter" data-level="" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#references-4"><i class="fa fa-check"></i>References</a></li>
</ul></li>
<li class="part"><span><b>III Assignments</b></span></li>
<li class="chapter" data-level="8" data-path="r-markdown.html"><a href="r-markdown.html"><i class="fa fa-check"></i><b>8</b> R Markdown</a>
<ul>
<li class="chapter" data-level="8.1" data-path="r-markdown.html"><a href="r-markdown.html#introduction-to-r-markdown"><i class="fa fa-check"></i><b>8.1</b> Introduction to R Markdown</a>
<ul>
<li class="chapter" data-level="" data-path="r-markdown.html"><a href="r-markdown.html#creating-a-new-r-markdown-document"><i class="fa fa-check"></i>Creating a new R Markdown document</a></li>
<li class="chapter" data-level="" data-path="r-markdown.html"><a href="r-markdown.html#text-and-equations"><i class="fa fa-check"></i>Text and Equations</a></li>
<li class="chapter" data-level="" data-path="r-markdown.html"><a href="r-markdown.html#r-code"><i class="fa fa-check"></i>R-Code</a></li>
<li class="chapter" data-level="" data-path="r-markdown.html"><a href="r-markdown.html#latex-math"><i class="fa fa-check"></i>LaTeX Math</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>IV FAQ</b></span></li>
<li class="chapter" data-level="9" data-path="faq.html"><a href="faq.html"><i class="fa fa-check"></i><b>9</b> FAQ</a>
<ul>
<li class="chapter" data-level="9.1" data-path="faq.html"><a href="faq.html#common-error-messages"><i class="fa fa-check"></i><b>9.1</b> Common error messages</a>
<ul>
<li class="chapter" data-level="" data-path="faq.html"><a href="faq.html#a-general-note-on-error-messages"><i class="fa fa-check"></i>A general note on error messages</a></li>
<li class="chapter" data-level="" data-path="faq.html"><a href="faq.html#error-in-filefile-rt-cannot-open-the-connection"><i class="fa fa-check"></i>Error in file(file, “rt”): cannot open the connection</a></li>
<li class="chapter" data-level="" data-path="faq.html"><a href="faq.html#error-unexpected-symbol-in-call"><i class="fa fa-check"></i>Error: unexpected ‘SYMBOL’ in “CALL”</a></li>
<li class="chapter" data-level="" data-path="faq.html"><a href="faq.html#error-in-call-object-of-type-closure-is-not-subsettable"><i class="fa fa-check"></i>Error in CALL : object of type ‘closure’ is not subsettable</a></li>
<li class="chapter" data-level="" data-path="faq.html"><a href="faq.html#error-in-call_with_-operator-is-invalid-for-atomic-vectors"><i class="fa fa-check"></i>Error in CALL_WITH_$: $ operator is invalid for atomic vectors</a></li>
<li class="chapter" data-level="" data-path="faq.html"><a href="faq.html#error-in-call-object-name-not-found"><i class="fa fa-check"></i>Error in CALL: object ‘NAME’ not found</a></li>
<li class="chapter" data-level="" data-path="faq.html"><a href="faq.html#error-in-call-could-not-find-function-function"><i class="fa fa-check"></i>Error in CALL: could not find function “FUNCTION”</a></li>
<li class="chapter" data-level="" data-path="faq.html"><a href="faq.html#error-in-call-incorrect-number-of-dimension"><i class="fa fa-check"></i>Error in CALL: incorrect number of dimension</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="faq.html"><a href="faq.html#installation-of-r-packages"><i class="fa fa-check"></i><b>9.2</b> Installation of R packages</a>
<ul>
<li class="chapter" data-level="" data-path="faq.html"><a href="faq.html#what-are-the-different-ways-to-install-r-packages"><i class="fa fa-check"></i>What are the different ways to install R packages?</a></li>
<li class="chapter" data-level="" data-path="faq.html"><a href="faq.html#i-cannot-install-packages-due-to-error-in-contrib.urlrepossource-or-warning-message-package-packagename-is-not-available-for-this-version-of-r"><i class="fa fa-check"></i>I cannot install packages due to “Error in contrib.url(repos,”source”)” or “Warning message: package ‘PACKAGENAME’ is not available for this version of R”</a></li>
<li class="chapter" data-level="" data-path="faq.html"><a href="faq.html#some-libraries-with-graphical-output-e.g.-summarytools-magick-fail-to-installload-properly-on-macos"><i class="fa fa-check"></i>Some libraries with graphical output (e.g., summarytools, magick) fail to install/load properly on MacOS</a></li>
<li class="chapter" data-level="" data-path="faq.html"><a href="faq.html#i-cannot-install-some-packages-on-macos"><i class="fa fa-check"></i>I cannot install some packages on MacOS</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="faq.html"><a href="faq.html#issues-with-statistics-and-data"><i class="fa fa-check"></i><b>9.3</b> Issues with statistics and data</a>
<ul>
<li class="chapter" data-level="" data-path="faq.html"><a href="faq.html#why-does-a-multi-item-scale-lead-to-increased-reliability"><i class="fa fa-check"></i>Why does a multi-item scale lead to increased reliability?</a></li>
<li class="chapter" data-level="" data-path="faq.html"><a href="faq.html#why-can-demeaningstandardization-lead-to-missing-values"><i class="fa fa-check"></i>Why can demeaning/standardization lead to missing values?</a></li>
<li class="chapter" data-level="" data-path="faq.html"><a href="faq.html#the-confidence-interval-ci-of-the-mean-seems-very-small-compared-to-the-dispersion-of-my-sample.-can-this-be-correct"><i class="fa fa-check"></i>The confidence interval (CI) of the mean seems very small compared to the dispersion of my sample. Can this be correct?</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="faq.html"><a href="faq.html#errors-related-to-specific-methods"><i class="fa fa-check"></i><b>9.4</b> Errors related to specific methods</a>
<ul>
<li class="chapter" data-level="" data-path="faq.html"><a href="faq.html#logistic-regression-1"><i class="fa fa-check"></i>Logistic regression</a></li>
<li class="chapter" data-level="" data-path="faq.html"><a href="faq.html#when-using-logistic-regression-error-in-weights-y-non-numeric-argument-to-binary-operator"><i class="fa fa-check"></i>When using logistic regression: Error in weights * y : non-numeric argument to binary operator</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="faq.html"><a href="faq.html#general-settings-and-options"><i class="fa fa-check"></i><b>9.5</b> General settings and options</a>
<ul>
<li class="chapter" data-level="" data-path="faq.html"><a href="faq.html#numbers-are-formatted-weirdly"><i class="fa fa-check"></i>Numbers are formatted weirdly</a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="faq.html"><a href="faq.html#data-visualizationoutput-issues"><i class="fa fa-check"></i><b>9.6</b> Data visualization/output issues</a>
<ul>
<li class="chapter" data-level="" data-path="faq.html"><a href="faq.html#how-can-the-geom-colors-in-a-ggplot-be-changed"><i class="fa fa-check"></i>How can the <code>geom</code> colors in a <code>ggplot</code> be changed?</a></li>
<li class="chapter" data-level="" data-path="faq.html"><a href="faq.html#why-are-some-histograms-displayed-differently"><i class="fa fa-check"></i>Why are some histograms displayed differently?</a></li>
<li class="chapter" data-level="" data-path="faq.html"><a href="faq.html#some-labels-in-plots-are-cut-off.-how-can-i-extend-the-plot-margins"><i class="fa fa-check"></i>Some labels in plots are cut off. How can I extend the plot margins?</a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="faq.html"><a href="faq.html#issues-with-functions-and-function-arguments"><i class="fa fa-check"></i><b>9.7</b> Issues with functions and function arguments</a>
<ul>
<li class="chapter" data-level="" data-path="faq.html"><a href="faq.html#problems-with-factor-and-as.factor"><i class="fa fa-check"></i>Problems with <code>factor</code> and <code>as.factor</code></a></li>
<li class="chapter" data-level="" data-path="faq.html"><a href="faq.html#how-can-i-find-an-explanation-of-the-output-of-a-function"><i class="fa fa-check"></i>How can I find an explanation of the output of a function?</a></li>
<li class="chapter" data-level="" data-path="faq.html"><a href="faq.html#what-does-the-margin-argument-do"><i class="fa fa-check"></i>What does the <code>MARGIN</code> argument do?</a></li>
</ul></li>
<li class="chapter" data-level="9.8" data-path="faq.html"><a href="faq.html#issues-with-r-markdown"><i class="fa fa-check"></i><b>9.8</b> Issues with R Markdown</a>
<ul>
<li class="chapter" data-level="" data-path="faq.html"><a href="faq.html#i-get-an-error-when-knitting-to-pdf-but-it-works-for-html"><i class="fa fa-check"></i>I get an error when <code>knit</code>ting to PDF but it works for HTML</a></li>
<li class="chapter" data-level="" data-path="faq.html"><a href="faq.html#i-am-not-sure-where-r-code-latex-math-and-text-go"><i class="fa fa-check"></i>I am not sure where R-code, LaTeX math, and text go</a></li>
</ul></li>
<li class="chapter" data-level="9.9" data-path="faq.html"><a href="faq.html#new-questions"><i class="fa fa-check"></i><b>9.9</b> New questions</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Retail Marketing Analytics 2024</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="statistical-inference" class="section level1 hasAnchor" number="5">
<h1><span class="header-section-number">5</span> Statistical inference<a href="statistical-inference.html#statistical-inference" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>This chapter will provide you with a basic intuition on statistical inference. As marketing researchers we are usually faced with “imperfect” data in the sense that we cannot collect <strong>all</strong> the data we would like. Imagine you are interested in the average amount of time WU students spend listening to music every month. Ideally, we could force all WU students to fill out our survey. Realistically we will only be able to observe a small fraction of students (maybe 500 out of the <span class="math inline">\(25.000+\)</span>). With the data from this small fraction at hand, we want to make an inference about the true average listening time of all WU students. We are going to start with the assumption that we know everything. That is, we first assume that we know all WU students’ listening times and analyze the distribution of the listening time in the entire population. Subsequently, we are going to look at the uncertainty that is introduced by only knowing some of the students’ listening times (i.e., a sample from the population) and how that influences our analysis.</p>
<div id="if-we-knew-it-all" class="section level2 hasAnchor" number="5.1">
<h2><span class="header-section-number">5.1</span> If we knew it all<a href="statistical-inference.html#if-we-knew-it-all" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Assume there are <span class="math inline">\(25,000\)</span> students at WU and every single one has kindly provided us with the hours they listened to music in the past month. Using the code below, the <code>rnorm()</code> function will be used to generate 25,000 observations from a normal distribution with a mean of 50 and a standard deviation of 10. Although you might not be used to working with this type of simulated (i.e., synthetic) data, it is useful when explaining statistical concepts because the properties of the data are known. In this case, for example, we know the true mean (<span class="math inline">\(49.93\)</span> hours) and the true standard deviation (SD = <span class="math inline">\(10.02\)</span>) and thus we can easily summarize the entire distribution. Since the data follows a normal distribution, roughly 95% of the values lie within 2 standard deviations from the mean, as the following plot shows:</p>
<p><img src="06-statistical_inference_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>In this case, we refer to all WU students as <strong>the population</strong>. In general, the population is the entire group we are interested in. This group does not have to necessarily consist of people, but could also be companies, stores, animals, etc.. The parameters of the distribution of population values (in hour case: “hours”) are called population parameters. As already mentioned, we do not usually know population parameters but use inferential statistics to infer them based on our sample from the population, i.e., we measure statistics from a sample (e.g., the sample mean <span class="math inline">\(\bar x\)</span>) to estimate population parameters (the population mean <span class="math inline">\(\mu\)</span>). Here, we will use the following notation to refer to either the population parameters or the sample statistic:</p>
<table>
<colgroup>
<col width="30%" />
<col width="34%" />
<col width="34%" />
</colgroup>
<thead>
<tr class="header">
<th>Variable</th>
<th>Sample statistic</th>
<th>Population parameter</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Size</td>
<td>n</td>
<td>N</td>
</tr>
<tr class="even">
<td>Mean</td>
<td><span class="math inline">\(\bar{x} = {1 \over n}\sum_{i=1}^n x_i\)</span></td>
<td><span class="math inline">\(\mu = {1 \over N}\sum_{i=1}^N x_i\)</span></td>
</tr>
<tr class="odd">
<td>Variance</td>
<td><span class="math inline">\(s^2 = {1 \over n-1}\sum_{i=1}^n (x_i-\bar{x})^2\)</span></td>
<td><span class="math inline">\(\sigma^2 = {1 \over N}\sum_{i=1}^N (x_i-\mu)^2\)</span></td>
</tr>
<tr class="even">
<td>Standard deviation</td>
<td><span class="math inline">\(s = \sqrt{s^2}\)</span></td>
<td><span class="math inline">\(\sigma = \sqrt{\sigma^2}\)</span></td>
</tr>
<tr class="odd">
<td>Standard error</td>
<td><span class="math inline">\(SE_{\bar x} = {s \over \sqrt{n}}\)</span></td>
<td><span class="math inline">\(\sigma_{\bar x} = {\sigma \over \sqrt{n}}\)</span></td>
</tr>
</tbody>
</table>
<p>Using this notation, <span class="math inline">\(N\)</span> refers to the number of observations in the entire population (i.e., 25,000 in our example) and <span class="math inline">\(n\)</span> refers to a subset of the population (i.e., a sample). As you can see, we will use different Greek letters to denote the sample statistics and the population parameters. Another difference, you might have noticed is that in the computation of the sample variance, we divide by <span class="math inline">\(n-1\)</span>, not <span class="math inline">\(n\)</span>. This is also know as the <em>‘Bessel’s correction’</em> and it corrects the bias in the estimation of the population variance based on a sample. More specifically, due to the correction, the corrected variance will be larger, since the denominator gets smaller by subtracting 1. This is done because the variance will most of the time be smaller when calculated using the sum of squared deviations from the sample mean, compared to using the sum of deviations from the population mean. The intuition is that, the larger your sample is, the more likely it is to get more population-representative points. Or, to put it another way, it is less likely to get a sample mean which results in differences which are too small. Thus, the larger your sample size <span class="math inline">\(n\)</span>, the less of a correction you need and, hence, the smaller the impact the correction component will be.</p>
<div id="sampling-from-a-known-population" class="section level3 hasAnchor" number="5.1.1">
<h3><span class="header-section-number">5.1.1</span> Sampling from a known population<a href="statistical-inference.html#sampling-from-a-known-population" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In the first step towards a realistic research setting, let us take one sample from this population and calculate the mean listening time. We can simply sample the row numbers of students and then subset the <code>hours</code> vector with the sampled row numbers. The <code>sample()</code> function will be used to draw a sample of size 100 from the population of 25,000 students, and one student can only be drawn once (i.e., <code>replace = FALSE</code>). The following plot shows the distribution of listening times for our sample.</p>
<p><img src="06-statistical_inference_files/figure-html/unnamed-chunk-3-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>Observe that in this first draw the mean (<span class="math inline">\(\bar x =\)</span> 49.67) is quite close to the actual mean (<span class="math inline">\(\mu =\)</span> 49.93). It seems like the sample mean is a decent estimate of the population mean. However, we could just be lucky this time and the next sample could turn out to have a different mean. Let us continue by looking at four additional random samples, consisting of 100 students each. The following plot shows the distribution of listening times for the four different samples from the population.</p>
<p><img src="06-statistical_inference_files/figure-html/unnamed-chunk-4-1.png" width="768" style="display: block; margin: auto;" /></p>
<p>It becomes clear that the mean is slightly different for each sample. This is referred to as <strong>sampling variation</strong> and it is completely fine to get a slightly different mean every time we take a sample. We just need to find a way of expressing the uncertainty associated with the fact that we only have data from one sample, because in a realistic setting you are most likely only going to have access to a single sample.</p>
<p>So in order to make sure that the first draw is not just pure luck and the sample mean is in fact a good estimate for the population mean, let us take <strong>many</strong> (e.g., <span class="math inline">\(20,000\)</span>) different samples from the population. That is, we repeatedly draw 100 students randomly from the population without replacement (that is, once a student has been drawn she or he is removed from the pool and cannot be drawn again) and calculate the mean of each sample. This will show us a range within which the sample mean of any sample we take is likely going to be. We are going to store the means of all the samples in a matrix and then plot a histogram of the means to observe the likely values.</p>
<p><img src="06-statistical_inference_files/figure-html/unnamed-chunk-5-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>As you can see, on average the sample mean (“mean of sample means”) is extremely close to the population mean, despite only sampling <span class="math inline">\(100\)</span> people at a time. This distribution of sample means is also referred to as <strong>sampling distribution</strong> of the sample mean. However, there is some uncertainty, and the means are slightly different for the different samples and range from 45.95 to 54.31.</p>
</div>
<div id="standard-error-of-the-mean" class="section level3 hasAnchor" number="5.1.2">
<h3><span class="header-section-number">5.1.2</span> Standard error of the mean<a href="statistical-inference.html#standard-error-of-the-mean" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Due to the variation in the sample means shown in our simulation, it is never possible to say exactly what the population mean is based on a single sample. However, even with a single sample we can infer a range of values within which the population mean is likely contained. In order to do so, notice that the sample means are approximately normally distributed. Another interesting fact is that the mean of sample means (i.e., 49.94) is roughly equal to the population mean (i.e., 49.93). This tells us already that generally the sample mean is a good approximation of the population mean. However, in order to make statements about the expected range of possible values, we would need to know the standard deviation of the sampling distribution. The formal representation of the standard deviation of the sample means is</p>
<p><span class="math display">\[
\sigma_{\bar x} = {\sigma \over \sqrt{n}}
\]</span></p>
<p>where <span class="math inline">\(\sigma\)</span> is the population SD and <span class="math inline">\(n\)</span> is the sample size. <span class="math inline">\(\sigma_{\bar{x}}\)</span> is referred to as the <strong>Standard Error</strong> of the mean and it expresses the variation in sample means we should expect given the number of observations in our sample and the population SD. That is, it provides a measure of how precisely we can estimate the population mean from the sample mean.</p>
<div id="sample-size" class="section level4 hasAnchor" number="5.1.2.1">
<h4><span class="header-section-number">5.1.2.1</span> Sample size<a href="statistical-inference.html#sample-size" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The first thing to notice here is that an increase in the <strong>number of observations per sample</strong> <span class="math inline">\(n\)</span> decreases the range of possible sample means (i.e., the standard error). This makes intuitive sense. Think of the two extremes: sample size <span class="math inline">\(1\)</span> and sample size <span class="math inline">\(25,000\)</span>. With a single person in the sample we do not gain a lot of information and our estimate is very uncertain, which is expressed through a larger standard deviation. Looking at the histogram at the beginning of this chapter showing the number of students for each of the listening times, clearly we would get values below <span class="math inline">\(25\)</span> or above <span class="math inline">\(75\)</span> for some samples. This is way farther away from the population mean than the minimum and the maximum of our <span class="math inline">\(100\)</span> person samples. On the other hand, if we sample every student we get the population mean every time and thus we do not have any uncertainty (assuming the population does not change). Even if we only sample, say <span class="math inline">\(24,000\)</span> people every time, we gain a lot of information about the population and the sample means would not be very different from each other since only up to <span class="math inline">\(1,000\)</span> people are potentially different in any given sample. Thus, with larger (smaller) samples, there is less (more) uncertainty that the sample is a good approximation of the entire population. The following plot shows the relationship between the sample size and the standard error. Samples of increasing size are randomly drawn from the population of WU students. You can see that the standard error is decreasing with the number of observations.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-6"></span>
<img src="06-statistical_inference_files/figure-html/unnamed-chunk-6-1.png" alt="Relationship between the sample size and the standard error" width="672" />
<p class="caption">
Figure 5.1: Relationship between the sample size and the standard error
</p>
</div>
<p>The following plots show the relationship between the sample size and the standard error in a slightly different way. The plots show the range of sample means resulting from the repeated sampling process for different sample sizes. Notice that the more students are contained in the individual samples, the less uncertainty there is when estimating the population mean from a sample (i.e., the possible values are more closely centered around the mean). So when the sample size is small, the sample mean can expected to be very different the next time we take a sample. When the sample size is large, we can expect the sample means to be more similar every time we take a sample.</p>
<p><img src="06-statistical_inference_files/figure-html/unnamed-chunk-7-1.png" width="768" style="display: block; margin: auto;" /></p>
<p>As you can see, the standard deviation of the sample means (<span class="math inline">\(\sigma_{\bar x}\)</span>) decreases as the sample size increases as a consequence of the reduced uncertainty about the true sample mean when we take larger samples.</p>
</div>
<div id="population-standard-deviation" class="section level4 hasAnchor" number="5.1.2.2">
<h4><span class="header-section-number">5.1.2.2</span> Population standard deviation<a href="statistical-inference.html#population-standard-deviation" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>A second factor determining the standard deviation of the distribution of sample means (<span class="math inline">\(\sigma_{\bar x}\)</span>) is the standard deviation associated with the population parameter (<span class="math inline">\(\sigma\)</span>). Again, looking at the extremes illustrates this well. If all WU students listened to music for approximately the same amount of time, the samples would not differ much from each other. In other words, if the standard deviation in the population is lower, we expect the standard deviation of the sample means to be lower as well. This is illustrated by the following plots.</p>
<p><img src="06-statistical_inference_files/figure-html/unnamed-chunk-8-1.png" width="768" style="display: block; margin: auto;" /></p>
<p>In the first plot (panel A), we assume a much smaller population standard deviation (e.g., <span class="math inline">\(\sigma\)</span> = 1 instead of <span class="math inline">\(\sigma\)</span> = 10). Notice how the smaller (larger) the population standard deviation, the less (more) uncertainty there is when estimating the population mean from a sample (i.e., the possible values are more closely centered around the mean). So when the population SD is large, the sample mean can expected to be very different the next time we take a sample. When the population SD is small, we can expect the sample means to be more similar.</p>
</div>
</div>
</div>
<div id="the-central-limit-theorem" class="section level2 hasAnchor" number="5.2">
<h2><span class="header-section-number">5.2</span> The Central Limit Theorem<a href="statistical-inference.html#the-central-limit-theorem" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The attentive reader might have noticed that the population above was generated using a normal distribution function. It would be very restrictive if we could only analyze populations whose values are normally distributed. Furthermore, we are unable in reality to check whether the population values are normally distributed since we do not know the entire population. However, it turns out that the results generalize to many other distributions. This is described by the <strong>Central Limit Theorem</strong>.</p>
<p>The central limit theorem states that if <strong>(1)</strong> the population distribution has a mean (there are examples of distributions that don’t have a mean , but we will ignore these here), and <strong>(2)</strong> we take a large enough sample, then the sampling distribution of the sample mean is approximately normally distributed. What exactly “large enough” means depends on the setting, but the interactive element at the end of this chapter illustrates how the sample size influences how accurately we can estimate the population parameters from the sample statistics.</p>
<p>To illustrate this, let’s repeat the analysis above with a population from a gamma distribution. In the previous example, we assumed a normal distribution so it was more likely for a given student to spend around 50 hours per week listening to music. The following example depicts the case in which most students spend a similar amount of time listening to music, but there are a few students who very rarely listen to music, and some music enthusiasts with a very high level of listening time. In the following code, we will use the <code>rgamma()</code> function to generate 25,000 random observations from the gamma distribution. The gamma distribution is specified by shape and scale parameters instead of the mean and standard deviation of the normal distribution. Here is a histogram of the listening times in the population:</p>
<p><img src="06-statistical_inference_files/figure-html/unnamed-chunk-9-1.png" width="576" style="display: block; margin: auto;" /></p>
<p>The vertical black line represents the population mean (<span class="math inline">\(\mu\)</span>), which is 19.98 hours. The following plot depicts the histogram of listening times of four random samples from the population, each consisting of 100 students:</p>
<p><img src="06-statistical_inference_files/figure-html/unnamed-chunk-10-1.png" width="768" style="display: block; margin: auto;" /></p>
<p>As in the previous example, the mean is slightly different every time we take a sample due to sampling variation. Also note that the distribution of listening times no longer follows a normal distribution as a result of the fact that we now assume a gamma distribution for the population with a positive skew (i.e., lower values more likely, higher values less likely).</p>
<p>Let’s see what happens to the distribution of sample means if we take an increasing number of samples, each drawn from the same gamma population:</p>
<p><img src="06-statistical_inference_files/figure-html/unnamed-chunk-11-1.png" width="768" style="display: block; margin: auto;" /></p>
<p>Two things are worth noting: <strong>(1)</strong> The more (hypothetical) samples we take, the more the sampling distribution approaches a normal distribution. <strong>(2)</strong> The mean of the sampling distribution of the sample mean (<span class="math inline">\(\mu_{\bar x}\)</span>) is very similar to the population mean (<span class="math inline">\(\mu\)</span>). From this we can see that the mean of a sample is a good estimate of the population mean.</p>
<p>In summary, it is important to distinguish two types of variation: <strong>(1)</strong> For each individual sample that we may take in real life, the standard deviation (<span class="math inline">\(s\)</span>) is used to describe the <strong>natural variation</strong> in the data and the data may follow a non-normal distribution. <strong>(2)</strong> If we would (hypothetically!) repeat the study many times, the sampling distribution of the sample mean follows a normal distribution for large samples sizes (even if data from each individual study are non-normal), and the standard error (<span class="math inline">\(\sigma_{\bar x}\)</span>) is used to describe the <strong>variation between study results</strong>. This is an important feature, since many statistical tests assume that the sampling distribution is normally distributed. As we have seen, this does <strong>not</strong> mean that the data from one particular sample needs to follow a normal distribution.</p>
</div>
<div id="using-what-we-actually-know" class="section level2 hasAnchor" number="5.3">
<h2><span class="header-section-number">5.3</span> Using what we actually know<a href="statistical-inference.html#using-what-we-actually-know" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>So far we have assumed to know the population standard deviation (<span class="math inline">\(\sigma\)</span>). This an unrealistic assumption since we do not know the entire population. The best guess for the population standard deviation we have is the sample standard deviation, denoted <span class="math inline">\(s\)</span>. Thus, the standard error of the mean is usually estimated from the sample standard deviation:</p>
<p><span class="math display">\[
\sigma_{\bar x} \approx SE_{\bar x}={s \over \sqrt{n}}
\]</span></p>
<p>Note that <span class="math inline">\(s\)</span> itself is a sample estimate of the population parameter <span class="math inline">\(\sigma\)</span>. This additional estimation introduces further uncertainty. You can see in the interactive element below that the sample SD, on average, provides a good estimate of the population SD. That is, the distribution of sample SDs that we get by drawing many samples is centered around the population value. Again, the larger the sample, the closer any given sample SD is going to be to the population parameter and we introduce less uncertainty. One conclusion is that your sample needs to be large enough to provide a reliable estimate of the population parameters. What exactly “large enough” means depends on the setting, but the interactive element illustrates how the remaining values change as a function of the sample size.</p>
<p>We will not go into detail about the importance of random samples but basically the correctness of your estimate depends crucially on having a sample at hand that actually represents the population. Unfortunately, we will usually not notice if the sample is non-random. Our statistics are still a good approximation of “a” population parameter, namely the one for the population that we actually sampled but not the one we are interested in. To illustrate this uncheck the “Random Sample” box below. The new sample will be only from the top <span class="math inline">\(50\%\)</span> music listeners (but this generalizes to different types of non-random samples).</p>
<iframe src="https://learn.wu.ac.at/shiny/imsm/clt/" style="border: none; width: 800px; height: 1265px">
</iframe>
</div>
<div id="confidence-intervals-for-the-sample-mean" class="section level2 hasAnchor" number="5.4">
<h2><span class="header-section-number">5.4</span> Confidence Intervals for the Sample Mean<a href="statistical-inference.html#confidence-intervals-for-the-sample-mean" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>When we try to estimate parameters of populations (e.g., the population mean <span class="math inline">\(\mu\)</span>) from a sample, the average value from a sample (e.g., the sample mean <span class="math inline">\(\bar x\)</span>) only provides an estimate of what the real population parameter is. The next time you collect a sample of the same size, you could get a different average. This is sampling variation and it is completely fine to get a slightly different sample mean every time we take a sample as we have seen above. However, this inherent uncertainty about the true population parameter means that coming up with an exact estimate (i.e., a <strong>point estimate</strong>) for a particular population parameter is really difficult. That is why it is often informative to construct a range around that statistic (i.e., an <strong>interval estimate</strong>) that likely contains the population parameter with a certain level of confidence. That is, we construct an interval such that for a large share (say 95%) of the sample means we could potentially get, the population mean is within that interval.</p>
<p>Let us consider one random sample of 100 students from our population above.</p>
<p><img src="06-statistical_inference_files/figure-html/unnamed-chunk-12-1.png" width="576" /></p>
<p>From the central limit theorem we know that the sampling distribution of the sample mean is approximately normal and we know that for the normal distribution, 95% of the values lie within about 2 standard deviations from the mean. Actually, it is not exactly 2 standard deviations from the mean. To get the exact number, we can use the quantile function for the normal distribution <code>qnorm()</code>:</p>
<div class="sourceCode" id="cb181"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb181-1"><a href="statistical-inference.html#cb181-1" tabindex="-1"></a><span class="fu">qnorm</span>(<span class="fl">0.975</span>)</span></code></pre></div>
<pre><code>## [1] 1.959964</code></pre>
<p>We use <code>0.975</code> (and not <code>0.95</code>) to account for the probability at each end of the distribution (i.e., 2.5% at the lower end and 2.5% at the upper end). We can see that 95% of the values are roughly within 1.96 standard deviations from the mean. Since we know the sample mean (<span class="math inline">\(\bar x\)</span>) and we can estimate the standard deviation of the sampling distribution (<span class="math inline">\(\sigma_{\bar x} \approx {s \over \sqrt{n}}\)</span>), we can now easily calculate the lower and upper boundaries of our confidence interval as:</p>
<p><span class="math display">\[
CI_{lower} = {\bar x} - z_{1-{\alpha \over 2}} * \sigma_{\bar x} \\
CI_{upper} = {\bar x} + z_{1-{\alpha \over 2}} * \sigma_{\bar x}
\]</span></p>
<p>Here, <span class="math inline">\(\alpha\)</span> refers to the significance level. You can find a detailed discussion of this point at the end of the next chapter. For now, we will adopt the widely accepted significance level of 5% and set <span class="math inline">\(\alpha\)</span> to 0.05. Thus, <span class="math inline">\(\pm z_{1-{\alpha \over 2}}\)</span> gives us the z-scores (i.e., number of standard deviations from the mean) within which range 95% of the probability density lies.</p>
<p>Plugging in the values from our sample, we get:</p>
<div class="sourceCode" id="cb183"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb183-1"><a href="statistical-inference.html#cb183-1" tabindex="-1"></a>sample_mean <span class="ot">&lt;-</span> <span class="fu">mean</span>(hours_s)</span>
<span id="cb183-2"><a href="statistical-inference.html#cb183-2" tabindex="-1"></a>se <span class="ot">&lt;-</span> <span class="fu">sd</span>(hours_s)<span class="sc">/</span><span class="fu">sqrt</span>(sample_size)</span>
<span id="cb183-3"><a href="statistical-inference.html#cb183-3" tabindex="-1"></a>ci_lower <span class="ot">&lt;-</span> sample_mean <span class="sc">-</span> <span class="fu">qnorm</span>(<span class="fl">0.975</span>)<span class="sc">*</span>se</span>
<span id="cb183-4"><a href="statistical-inference.html#cb183-4" tabindex="-1"></a>ci_upper <span class="ot">&lt;-</span> sample_mean <span class="sc">+</span> <span class="fu">qnorm</span>(<span class="fl">0.975</span>)<span class="sc">*</span>se</span>
<span id="cb183-5"><a href="statistical-inference.html#cb183-5" tabindex="-1"></a>ci_lower</span></code></pre></div>
<pre><code>## [1] 17.67089</code></pre>
<div class="sourceCode" id="cb185"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb185-1"><a href="statistical-inference.html#cb185-1" tabindex="-1"></a>ci_upper</span></code></pre></div>
<pre><code>## [1] 23.1592</code></pre>
<p>such that if we collected 100 samples and computed the mean and confidence interval for each of them, in <span class="math inline">\(95\%\)</span> of the cases, the true population mean is going to be within this interval between 17.67 and 23.16.</p>
<div class="infobox_orange hint">
<p>Note the correct interpretation of the confidence interval: If we collected 100 samples, calculated the mean and then calculated a confidence interval for that mean, then, for 95 of these samples, the confidence intervals we constructed would contain the true value of the mean in the population.</p>
</div>
<p>This is illustrated in the plot below that shows the mean of the first 100 samples and their confidence intervals:</p>
<p><img src="06-statistical_inference_files/figure-html/unnamed-chunk-15-1.png" width="960" /></p>
<div class="infobox_red caution">
<p>Note that this does <strong>not</strong> mean that for a specific sample there is a <span class="math inline">\(95\%\)</span> chance that the population mean lies within its confidence interval. The statement depends on the large number of samples we do not actually draw in a real setting. You can view the set of all possible confidence intervals similarly to the sides of a coin or a die. If we throw a coin many times, we are going to observe head roughly half of the times. This does not, however, exclude the possibility of observing tails for the first 10 throws. Similarly, any specific confidence interval might or might not include the population mean but if we take many samples on average <span class="math inline">\(95\%\)</span> of the confidence intervals are going to include the population mean.</p>
</div>
</div>
<div id="null-hypothesis-statistical-testing-nhst" class="section level2 hasAnchor" number="5.5">
<h2><span class="header-section-number">5.5</span> Null Hypothesis Statistical Testing (NHST)<a href="statistical-inference.html#null-hypothesis-statistical-testing-nhst" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We test hypotheses because we are confined to taking samples – we rarely work with the entire population. In the previous chapter, we introduced the standard error (i.e., the standard deviation of a large number of hypothetical samples) as an estimate of how well a particular sample represents the population. We also saw how we can construct confidence intervals around the sample mean <span class="math inline">\(\bar x\)</span> by computing <span class="math inline">\(SE_{\bar x}\)</span> as an estimate of <span class="math inline">\(\sigma_{\bar x}\)</span> using <span class="math inline">\(s\)</span> as an estimate of <span class="math inline">\(\sigma\)</span> and calculating the 95% CI as <span class="math inline">\(\bar x \pm 1.96 * SE_{\bar x}\)</span>. Although we do not know the true population mean (<span class="math inline">\(\mu\)</span>), we might have an hypothesis about it and this would tell us how the corresponding sampling distribution looks like. Based on the sampling distribution of the hypothesized population mean, we could then determine the probability of a given sample <strong>assuming that the hypothesis is true</strong>.</p>
<p>Let us again begin by assuming we know the entire population using the example of music listening times among students from the previous example. As a reminder, the following plot shows the distribution of music listening times in the population of WU students.</p>
<p><img src="06-statistical_inference_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
<p>In this example, the population mean (<span class="math inline">\(\mu\)</span>) is equal to 19.98, and the population standard deviation <span class="math inline">\(\sigma\)</span> is equal to 14.15.</p>
<div id="the-null-hypothesis" class="section level3 hasAnchor" number="5.5.1">
<h3><span class="header-section-number">5.5.1</span> The null hypothesis<a href="statistical-inference.html#the-null-hypothesis" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let us assume that we were planning to take a random sample of 50 students from this population and our hypothesis was that the mean listening time is equal to some specific value <span class="math inline">\(\mu_0\)</span>, say <span class="math inline">\(10\)</span>. This would be our <strong>null hypothesis</strong>. The null hypothesis refers to the statement that is being tested and is usually a statement of the status quo, one of no difference or no effect. In our example, the null hypothesis would state that there is no difference between the true population mean <span class="math inline">\(\mu\)</span> and the hypothesized value <span class="math inline">\(\mu_0\)</span> (in our example <span class="math inline">\(10\)</span>), which can be expressed as follows:</p>
<p><span class="math display">\[
H_0: \mu = \mu_0
\]</span>
When conducting research, we are usually interested in providing evidence against the null hypothesis. If we then observe sufficient evidence against it and our estimate is said to be significant. If the null hypothesis is rejected, this is taken as support for the <strong>alternative hypothesis</strong>. The alternative hypothesis assumes that some difference exists, which can be expressed as follows:</p>
<p><span class="math display">\[
H_1: \mu \neq \mu_0
\]</span>
Accepting the alternative hypothesis in turn will often lead to changes in opinions or actions. Note that while the null hypothesis may be rejected, it can never be accepted based on a single test. If we fail to reject the null hypothesis, it means that we simply haven’t collected enough evidence against the null hypothesis to disprove it. In classical hypothesis testing, there is no way to determine whether the null hypothesis is true. <strong>Hypothesis testing</strong> provides a means to quantify to what extent the data from our sample is in line with the null hypothesis.</p>
<p>In order to quantify the concept of “sufficient evidence” we look at the theoretical distribution of the sample means given our null hypothesis and the sample standard error. Using the available information we can infer the sampling distribution for our null hypothesis. Recall that the standard deviation of the sampling distribution (i.e., the standard error of the mean) is given by <span class="math inline">\(\sigma_{\bar x}={\sigma \over \sqrt{n}}\)</span>, and thus can be computed as follows:</p>
<div class="sourceCode" id="cb187"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb187-1"><a href="statistical-inference.html#cb187-1" tabindex="-1"></a>mean_pop <span class="ot">&lt;-</span> <span class="fu">mean</span>(hours)</span>
<span id="cb187-2"><a href="statistical-inference.html#cb187-2" tabindex="-1"></a>sigma <span class="ot">&lt;-</span> <span class="fu">sd</span>(hours) <span class="co">#population standard deviation</span></span>
<span id="cb187-3"><a href="statistical-inference.html#cb187-3" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">50</span> <span class="co">#sample size</span></span>
<span id="cb187-4"><a href="statistical-inference.html#cb187-4" tabindex="-1"></a>standard_error <span class="ot">&lt;-</span> sigma<span class="sc">/</span><span class="fu">sqrt</span>(n) <span class="co">#standard error</span></span>
<span id="cb187-5"><a href="statistical-inference.html#cb187-5" tabindex="-1"></a>standard_error</span></code></pre></div>
<pre><code>## [1] 2.001639</code></pre>
<p>Since we know from the central limit theorem that the sampling distribution is normal for large enough samples, we can now visualize the expected sampling distribution <strong>if our null hypothesis was in fact true</strong> (i.e., if the was no difference between the true population mean and the hypothesized mean of 10).</p>
<p><img src="06-statistical_inference_files/figure-html/unnamed-chunk-18-1.png" width="768" style="display: block; margin: auto;" /></p>
<p>We also know that 95% of the probability is within 1.96 standard deviations from the mean. Values higher than that are rather unlikely, if our hypothesis about the population mean was indeed true. This is shown by the shaded area, also known as the “rejection region”. To test our hypothesis that the population mean is equal to <span class="math inline">\(10\)</span>, let us take a random sample from the population.</p>
<p><img src="06-statistical_inference_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
<p>The mean listening time in the sample (black line) <span class="math inline">\(\bar x\)</span> is 18.59. We can already see from the graphic above that such a value is rather unlikely under the hypothesis that the population mean is <span class="math inline">\(10\)</span>. Intuitively, such a result would therefore provide evidence against our null hypothesis. But how could we quantify specifically how unlikely it is to obtain such a value and decide whether or not to reject the null hypothesis? Significance tests can be used to provide answers to these questions.</p>
</div>
<div id="statistical-inference-on-a-sample" class="section level3 hasAnchor" number="5.5.2">
<h3><span class="header-section-number">5.5.2</span> Statistical inference on a sample<a href="statistical-inference.html#statistical-inference-on-a-sample" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="test-statistic" class="section level4 hasAnchor" number="5.5.2.1">
<h4><span class="header-section-number">5.5.2.1</span> Test statistic<a href="statistical-inference.html#test-statistic" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div id="z-scores" class="section level5 hasAnchor" number="5.5.2.1.1">
<h5><span class="header-section-number">5.5.2.1.1</span> z-scores<a href="statistical-inference.html#z-scores" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>Let’s go back to the sampling distribution above. We know that 95% of all values will fall within 1.96 standard deviations from the mean. So if we could express the distance between our sample mean and the null hypothesis in terms of standard deviations, we could make statements about the probability of getting a sample mean of the observed magnitude (or more extreme values). Essentially, we would like to know how many standard deviations (<span class="math inline">\(\sigma_{\bar x}\)</span>) our sample mean (<span class="math inline">\(\bar x\)</span>) is away from the population mean if the null hypothesis was true (<span class="math inline">\(\mu_0\)</span>). This can be formally expressed as follows:</p>
<p><span class="math display">\[
\bar x-  \mu_0 = z \sigma_{\bar x}
\]</span></p>
<p>In this equation, <code>z</code> will tell us how many standard deviations the sample mean <span class="math inline">\(\bar x\)</span> is away from the null hypothesis <span class="math inline">\(\mu_0\)</span>. Solving for <code>z</code> gives us:</p>
<p><span class="math display">\[
z = {\bar x-  \mu_0 \over \sigma_{\bar x}}={\bar x-  \mu_0 \over \sigma / \sqrt{n}}
\]</span></p>
<p>This standardized value (or “z-score”) is also referred to as a <strong>test statistic</strong>. Let’s compute the test statistic for our example above:</p>
<div class="sourceCode" id="cb189"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb189-1"><a href="statistical-inference.html#cb189-1" tabindex="-1"></a>z_score <span class="ot">&lt;-</span> (mean_sample <span class="sc">-</span> H_0)<span class="sc">/</span>(sigma<span class="sc">/</span><span class="fu">sqrt</span>(n))</span>
<span id="cb189-2"><a href="statistical-inference.html#cb189-2" tabindex="-1"></a>z_score</span></code></pre></div>
<pre><code>## [1] 4.292454</code></pre>
<p>To make a decision on whether the difference can be deemed statistically significant, we now need to compare this calculated test statistic to a meaningful threshold. In order to do so, we need to decide on a significance level <span class="math inline">\(\alpha\)</span>, which expresses the probability of finding an effect that does not actually exist (i.e., Type I Error). You can find a detailed discussion of this point at the end of this chapter. For now, we will adopt the widely accepted significance level of 5% and set <span class="math inline">\(\alpha\)</span> to 0.05. The critical value for the normal distribution and <span class="math inline">\(\alpha\)</span> = 0.05 can be computed using the <code>qnorm()</code> function as follows:</p>
<div class="sourceCode" id="cb191"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb191-1"><a href="statistical-inference.html#cb191-1" tabindex="-1"></a>z_crit <span class="ot">&lt;-</span> <span class="fu">qnorm</span>(<span class="fl">0.975</span>)</span>
<span id="cb191-2"><a href="statistical-inference.html#cb191-2" tabindex="-1"></a>z_crit</span></code></pre></div>
<pre><code>## [1] 1.959964</code></pre>
<p>We use <code>0.975</code> and not <code>0.95</code> since we are running a two-sided test and need to account for the rejection region at the other end of the distribution. Recall that for the normal distribution, 95% of the total probability falls within 1.96 standard deviations of the mean, so that higher (absolute) values provide evidence against the null hypothesis. Generally, we speak of a statistically significant effect if the (absolute) calculated test statistic is larger than the (absolute) critical value. We can easily check if this is the case in our example:</p>
<div class="sourceCode" id="cb193"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb193-1"><a href="statistical-inference.html#cb193-1" tabindex="-1"></a><span class="fu">abs</span>(z_score) <span class="sc">&gt;</span> <span class="fu">abs</span>(z_crit)</span></code></pre></div>
<pre><code>## [1] TRUE</code></pre>
<p>Since the absolute value of the calculated test statistic is larger than the critical value, we would reject <span class="math inline">\(H_0\)</span> and conclude that the true population mean <span class="math inline">\(\mu\)</span> is significantly different from the hypothesized value <span class="math inline">\(\mu_0 = 10\)</span>.</p>
</div>
<div id="t-statistic" class="section level5 hasAnchor" number="5.5.2.1.2">
<h5><span class="header-section-number">5.5.2.1.2</span> t-statistic<a href="statistical-inference.html#t-statistic" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>You may have noticed that the formula for the z-score above assumes that we know the true population standard deviation (<span class="math inline">\(\sigma\)</span>) when computing the standard deviation of the sampling distribution (<span class="math inline">\(\sigma_{\bar x}\)</span>) in the denominator. However, the population standard deviation is usually not known in the real world and therefore represents another unknown population parameter which we have to estimate from the sample. We saw in the previous chapter that we usually use <span class="math inline">\(s\)</span> as an estimate of <span class="math inline">\(\sigma\)</span> and <span class="math inline">\(SE_{\bar x}\)</span> as and estimate of <span class="math inline">\(\sigma_{\bar x}\)</span>. Intuitively, we should be more conservative regarding the critical value that we used above to assess whether we have a significant effect to reflect this uncertainty about the true population standard deviation. That is, the threshold for a “significant” effect should be higher to safeguard against falsely claiming a significant effect when there is none. If we replace <span class="math inline">\(\sigma_{\bar x}\)</span> by it’s estimate <span class="math inline">\(SE_{\bar x}\)</span> in the formula for the z-score, we get a new test statistic (i.e, the <strong>t-statistic</strong>) with its own distribution (the <strong>t-distribution</strong>):</p>
<p><span class="math display">\[
t = {\bar x-  \mu_0 \over SE_{\bar x}}={\bar x-  \mu_0 \over s / \sqrt{n}}
\]</span></p>
<p>Here, <span class="math inline">\(\bar X\)</span> denotes the sample mean and <span class="math inline">\(s\)</span> the sample standard deviation. The t-distribution has more probability in its “tails”, i.e. farther away from the mean. This reflects the higher uncertainty introduced by replacing the population standard deviation by its sample estimate. Intuitively, this is particularly relevant for small samples, since the uncertainty about the true population parameters decreases with increasing sample size. This is reflected by the fact that the exact shape of the t-distribution depends on the <strong>degrees of freedom</strong>, which is the sample size minus one (i.e., <span class="math inline">\(n-1\)</span>). To see this, the following graph shows the t-distribution with different degrees of freedom for a two-tailed test and <span class="math inline">\(\alpha = 0.05\)</span>. The grey curve shows the normal distribution.</p>
<p><img src="06-statistical_inference_files/figure-html/unnamed-chunk-23-1.png" width="768" style="display: block; margin: auto;" /></p>
<p>Notice that as <span class="math inline">\(n\)</span> gets larger, the t-distribution gets closer and closer to the normal distribution, reflecting the fact that the uncertainty introduced by <span class="math inline">\(s\)</span> is reduced. To summarize, we now have an estimate for the standard deviation of the distribution of the sample mean (i.e., <span class="math inline">\(SE_{\bar x}\)</span>) and an appropriate distribution that takes into account the necessary uncertainty (i.e., the t-distribution). Let us now compute the t-statistic according to the formula above:</p>
<div class="sourceCode" id="cb195"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb195-1"><a href="statistical-inference.html#cb195-1" tabindex="-1"></a>SE <span class="ot">&lt;-</span> (<span class="fu">sd</span>(music_listening_sample<span class="sc">$</span>hours)<span class="sc">/</span><span class="fu">sqrt</span>(n))</span>
<span id="cb195-2"><a href="statistical-inference.html#cb195-2" tabindex="-1"></a>t_score <span class="ot">&lt;-</span> (mean_sample <span class="sc">-</span> H_0)<span class="sc">/</span>SE</span>
<span id="cb195-3"><a href="statistical-inference.html#cb195-3" tabindex="-1"></a>t_score</span></code></pre></div>
<pre><code>## [1] 4.84204</code></pre>
<p>Notice that the value of the t-statistic is higher compared to the z-score (4.29). This can be attributed to the fact that by using the <span class="math inline">\(s\)</span> as and estimate of <span class="math inline">\(\sigma\)</span>, we underestimate the true population standard deviation. Hence, the critical value would need to be larger to adjust for this. This is what the t-distribution does. Let us compute the critical value from the t-distribution with <code>n - 1</code>degrees of freedom.</p>
<div class="sourceCode" id="cb197"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb197-1"><a href="statistical-inference.html#cb197-1" tabindex="-1"></a>df <span class="ot">=</span> n <span class="sc">-</span> <span class="dv">1</span></span>
<span id="cb197-2"><a href="statistical-inference.html#cb197-2" tabindex="-1"></a>t_crit <span class="ot">&lt;-</span> <span class="fu">qt</span>(<span class="fl">0.975</span>, <span class="at">df =</span> df)</span>
<span id="cb197-3"><a href="statistical-inference.html#cb197-3" tabindex="-1"></a>t_crit</span></code></pre></div>
<pre><code>## [1] 2.009575</code></pre>
<p>Again, we use <code>0.975</code> and not <code>0.95</code> since we are running a two-sided test and need to account for the rejection region at the other end of the distribution. Notice that the new critical value based on the t-distributionis larger, to reflect the uncertainty when estimating <span class="math inline">\(\sigma\)</span> from <span class="math inline">\(s\)</span>. Now we can see that the calculated test statistic is still larger than the critical value.</p>
<div class="sourceCode" id="cb199"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb199-1"><a href="statistical-inference.html#cb199-1" tabindex="-1"></a><span class="fu">abs</span>(t_score) <span class="sc">&gt;</span> <span class="fu">abs</span>(t_crit)</span></code></pre></div>
<pre><code>## [1] TRUE</code></pre>
<p>The following graphics shows that the calculated test statistic (red line) falls into the rejection region so that in our example, we would reject the null hypothesis that the true population mean is equal to <span class="math inline">\(10\)</span>.</p>
<p><img src="06-statistical_inference_files/figure-html/unnamed-chunk-27-1.png" width="768" style="display: block; margin: auto;" /></p>
<p><strong>Decision:</strong> Reject <span class="math inline">\(H_0\)</span>, given that the calculated test statistic is larger than critical value.</p>
<p>Something to keep in mind here is the fact the test statistic is a function of the sample size. This, as <span class="math inline">\(n\)</span> gets large, the test statistic gets larger as well and we are more likely to find a significant effect. This reflects the decrease in uncertainty about the true population mean as our sample size increases.</p>
</div>
</div>
<div id="p-values" class="section level4 hasAnchor" number="5.5.2.2">
<h4><span class="header-section-number">5.5.2.2</span> P-values<a href="statistical-inference.html#p-values" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>In the previous section, we computed the test statistic, which tells us how close our sample is to the null hypothesis. The p-value corresponds to the probability that the test statistic would take a value as extreme or more extreme than the one that we actually observed, <strong>assuming that the null hypothesis is true</strong>. It is important to note that this is a <strong>conditional probability</strong>: we compute the probability of observing a sample mean (or a more extreme value) conditional on the assumption that the null hypothesis is true. The <code>pnorm()</code>function can be used to compute this probability. It is the cumulative probability distribution function of the `normal distribution. Cumulative probability means that the function returns the probability that the test statistic will take a value <strong>less than or equal to</strong> the calculated test statistic given the degrees of freedom. However, we are interested in obtaining the probability of observing a test statistic <strong>larger than or equal to</strong> the calculated test statistic under the null hypothesis (i.e., the p-value). Thus, we need to subtract the cumulative probability from 1. In addition, since we are running a two-sided test, we need to multiply the probability by 2 to account for the rejection region at the other side of the distribution.</p>
<div class="sourceCode" id="cb201"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb201-1"><a href="statistical-inference.html#cb201-1" tabindex="-1"></a>p_value <span class="ot">&lt;-</span> <span class="dv">2</span><span class="sc">*</span>(<span class="dv">1</span><span class="sc">-</span><span class="fu">pt</span>(<span class="fu">abs</span>(t_score), <span class="at">df =</span> df))</span>
<span id="cb201-2"><a href="statistical-inference.html#cb201-2" tabindex="-1"></a>p_value</span></code></pre></div>
<pre><code>## [1] 1.326885e-05</code></pre>
<p>This value corresponds to the probability of observing a mean equal to or larger than the one we obtained from our sample, if the null hypothesis was true. As you can see, this probability is very low. A small p-value signals that it is unlikely to observe the calculated test statistic under the null hypothesis. To decide whether or not to reject the null hypothesis, we would now compare this value to the level of significance (<span class="math inline">\(\alpha\)</span>) that we chose for our test. For this example, we adopt the widely accepted significance level of 5%, so any test results with a p-value &lt; 0.05 would be deemed statistically significant. Note that the p-value is directly related to the value of the test statistic. The relationship is such that the higher (lower) the value of the test statistic, the lower (higher) the p-value.</p>
<p><strong>Decision:</strong> Reject <span class="math inline">\(H_0\)</span>, given that the p-value is smaller than 0.05.</p>
</div>
<div id="confidence-interval" class="section level4 hasAnchor" number="5.5.2.3">
<h4><span class="header-section-number">5.5.2.3</span> Confidence interval<a href="statistical-inference.html#confidence-interval" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>For a given statistic calculated for a sample of observations (e.g., listening times), a 95% confidence interval can be constructed such that in 95% of samples, the true value of the true population mean will fall within its limits. If the parameter value specified in the null hypothesis (here <span class="math inline">\(10\)</span>) does not lie within the bounds, we reject <span class="math inline">\(H_0\)</span>. Building on what we learned about confidence intervals in the previous chapter, the 95% confidence interval based on the t-distribution can be computed as follows:</p>
<p><span class="math display">\[
CI_{lower} = {\bar x} - t_{1-{\alpha \over 2}} * SE_{\bar x} \\
CI_{upper} = {\bar x} + t_{1-{\alpha \over 2}} * SE_{\bar x}
\]</span></p>
<p>It is easy to compute this interval manually:</p>
<div class="sourceCode" id="cb203"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb203-1"><a href="statistical-inference.html#cb203-1" tabindex="-1"></a>ci_lower <span class="ot">&lt;-</span> (mean_sample)<span class="sc">-</span><span class="fu">qt</span>(<span class="fl">0.975</span>, <span class="at">df =</span> df)<span class="sc">*</span>SE</span>
<span id="cb203-2"><a href="statistical-inference.html#cb203-2" tabindex="-1"></a>ci_upper <span class="ot">&lt;-</span> (mean_sample)<span class="sc">+</span><span class="fu">qt</span>(<span class="fl">0.975</span>, <span class="at">df =</span> df)<span class="sc">*</span>SE</span>
<span id="cb203-3"><a href="statistical-inference.html#cb203-3" tabindex="-1"></a>ci_lower</span></code></pre></div>
<pre><code>## [1] 15.02606</code></pre>
<div class="sourceCode" id="cb205"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb205-1"><a href="statistical-inference.html#cb205-1" tabindex="-1"></a>ci_upper</span></code></pre></div>
<pre><code>## [1] 22.15783</code></pre>
<p>The interpretation of this interval is as follows: if we would (hypothetically) take 100 samples and calculated the mean and confidence interval for each of them, then the true population mean would be included in 95% of these intervals. The CI is informative when reporting the result of your test, since it provides an estimate of the uncertainty associated with the test result. From the test statistic or the p-value alone, it is not easy to judge in which range the true population parameter is located. The CI provides an estimate of this range.</p>
<p><strong>Decision:</strong> Reject <span class="math inline">\(H_0\)</span>, given that the parameter value from the null hypothesis (<span class="math inline">\(10\)</span>) is not included in the interval.</p>
<p>To summarize, you can see that we arrive at the same conclusion (i.e., reject <span class="math inline">\(H_0\)</span>), irrespective if we use the test statistic, the p-value, or the confidence interval. However, keep in mind that rejecting the null hypothesis does not prove the alternative hypothesis (we can merely provide support for it). Rather, think of the p-value as the chance of obtaining the data we’ve collected assuming that the null hypothesis is true. You should report the confidence interval to provide an estimate of the uncertainty associated with your test results.</p>
</div>
</div>
<div id="nhst-considerations" class="section level3 hasAnchor" number="5.5.3">
<h3><span class="header-section-number">5.5.3</span> NHST considerations<a href="statistical-inference.html#nhst-considerations" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="type-i-and-type-ii-errors" class="section level4 hasAnchor" number="5.5.3.1">
<h4><span class="header-section-number">5.5.3.1</span> Type I and Type II Errors<a href="statistical-inference.html#type-i-and-type-ii-errors" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>When choosing the level of significance (<span class="math inline">\(\alpha\)</span>). It is important to note that the choice of the significance level affects the type 1 and type 2 error:</p>
<ul>
<li>Type I error: When we believe there is a genuine effect in our population, when in fact there isn’t. Probability of type I error (<span class="math inline">\(\alpha\)</span>) = level of significance = the probability of finding an effect that does not genuinely exist.</li>
<li>Type II error: When we believe that there is no effect in the population, when in fact there is.</li>
</ul>
<p>This following table shows the possible outcomes of a test (retain vs. reject <span class="math inline">\(H_0\)</span>), depending on whether <span class="math inline">\(H_0\)</span> is true or false in reality.</p>
<table>
<colgroup>
<col width="16%" />
<col width="41%" />
<col width="41%" />
</colgroup>
<thead>
<tr class="header">
<th> </th>
<th>Retain <b>H<sub>0</sub></b></th>
<th>Reject <b>H<sub>0</sub></b></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><b>H<sub>0</sub> is true</b></td>
<td>Correct decision:<br>1-α (probability of correct retention);</td>
<td>Type 1 error:<br> α (level of significance)</td>
</tr>
<tr class="even">
<td><b>H<sub>0</sub> is false</b></td>
<td>Type 2 error:<br>β (type 2 error rate)</td>
<td>Correct decision:<br>1-β (power of the test)</td>
</tr>
</tbody>
</table>
</div>
<div id="p-values-1" class="section level4 hasAnchor" number="5.5.3.2">
<h4><span class="header-section-number">5.5.3.2</span> P-values<a href="statistical-inference.html#p-values-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>From my experience, students tend to place a lot of weight on p-values when interpreting their research findings. It is therefore important to note some points that hopefully help to put the meaning of a “significant” vs. “insignificant” test result into perspective. So what does a significant test result actually tell us?</p>
<ul>
<li>The importance of an effect? → No, significance depends on sample size.</li>
<li>That the null hypothesis is false? → No, it is always false.</li>
<li>That the null hypothesis is true? → No, it is never true.</li>
</ul>
<p>It is important to understand what the p-value actually tells you.</p>
<div class="infobox_orange hint">
<p>A p-value of &lt; 0.05 means that the probability of finding a difference of at least the observed magnitude is less than 5% if the null hypothesis was true. In other words, if there really wouldn’t be a difference between the groups, it tells you the probability of observing the difference that you found in your data (or more extreme differences).</p>
</div>
<p>The following points provide some guidance on how to interpret significant and insignificant test results.</p>
<p><strong>Significant result</strong></p>
<ul>
<li>Even if the probability of the effect being a chance result is small (e.g., less than .05) it doesn’t necessarily mean that the effect is important.</li>
<li>Very small and unimportant effects can turn out to be statistically significant if the sample size is large enough.</li>
</ul>
<p><strong>Insignificant result</strong></p>
<ul>
<li>If the probability of the effect occurring by chance is large (greater than .05), the alternative hypothesis is rejected. However, this does not mean that the null hypothesis is true.</li>
<li>Although an effect might not be large enough to be anything other than a chance finding, it doesn’t mean that the effect is zero.</li>
<li>In fact, two random samples will always have slightly different means that would deemed to be statistically significant if the samples were large enough.</li>
</ul>
<p>Thus, you should not base your research conclusion on p-values alone!</p>
</div>
</div>
</div>
<div id="learning-check-3" class="section level2 unnumbered hasAnchor">
<h2>Learning check<a href="statistical-inference.html#learning-check-3" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>(LC3.1) What is the correct interpretation of a confidence interval for a significance level of <span class="math inline">\(\alpha\)</span>=0.05?</strong></p>
<ul class="task-list">
<li><label><input type="checkbox" checked="" />If we take 100 samples and calculate mean and confidence interval for each one of them, then the true population mean would be included in 95% of these intervals.</label></li>
<li><label><input type="checkbox" />If we take 100 samples and calculate mean and confidence interval for each one of them, then the true population mean would be included in 5% of these intervals.</label></li>
<li><label><input type="checkbox" />If we take 100 samples and calculate mean and confidence interval for each one of them, then the true population mean would be included in 100% of these intervals.</label></li>
<li><label><input type="checkbox" />For a given sample, there is a 95% chance that the true population mean lies within the confidence interval.</label></li>
</ul>
<p><strong>(LC3.2) Which statements regarding standard error are TRUE?</strong></p>
<ul class="task-list">
<li><label><input type="checkbox" />There is no connection between the standard deviation and the standard error.</label></li>
<li><label><input type="checkbox" checked="" />The standard error is a function of the sample size and the standard deviation.</label></li>
<li><label><input type="checkbox" checked="" />The standard error of the mean decreases as the sample size increases.</label></li>
<li><label><input type="checkbox" checked="" />The standard error of the mean increases as the standard deviation increases.</label></li>
<li><label><input type="checkbox" />None of the above</label></li>
</ul>
<p><strong>(LC3.3) What is the correct definition for the standard error (<span class="math inline">\(SE_{\bar x}\)</span>)?</strong></p>
<ul class="task-list">
<li><label><input type="checkbox" checked="" /><span class="math inline">\({s \over \sqrt{n}}\)</span></label></li>
<li><label><input type="checkbox" /><span class="math inline">\({s * \sqrt{n}}\)</span></label></li>
<li><label><input type="checkbox" checked="" /><span class="math inline">\({\sqrt{s^2} \over \sqrt{n}}\)</span></label></li>
<li><label><input type="checkbox" /><span class="math inline">\({\sqrt{s} \over n}\)</span></label></li>
<li><label><input type="checkbox" />None of the above</label></li>
</ul>
<p><strong>(LC3.4) Which of the following do you need to compute a confidence interval around a sample mean?</strong></p>
<ul class="task-list">
<li><label><input type="checkbox" checked="" />The critical value of the test statistic given a certain level of confidence</label></li>
<li><label><input type="checkbox" checked="" />A continuous variable (i.e., at least measured at the interval level)</label></li>
<li><label><input type="checkbox" checked="" />The sample the mean</label></li>
<li><label><input type="checkbox" checked="" />The standard error</label></li>
<li><label><input type="checkbox" />None of the above</label></li>
</ul>
<p><strong>(LC3.5) What is the correct definition for the confidence interval?</strong></p>
<ul class="task-list">
<li><label><input type="checkbox" /><span class="math inline">\(CI=\bar{x} \pm \frac{z_{1-\frac{a}{n}}}{\sigma_{\bar{x}}}\)</span></label></li>
<li><label><input type="checkbox" /><span class="math inline">\(CI=\bar{x} * z_{1-\frac{a}{n}}*\sigma_{\bar{x}}\)</span></label></li>
<li><label><input type="checkbox" /><span class="math inline">\(CI= z_{1-\frac{a}{n}}*\sigma_{\bar{x}}-\bar{x}\)</span></label></li>
<li><label><input type="checkbox" checked="" /><span class="math inline">\(CI=\bar{x} \pm z_{1-\frac{a}{n}}*\sigma_{\bar{x}}\)</span></label></li>
<li><label><input type="checkbox" />None of the above</label></li>
</ul>
<p><em>As a marketing manager at Spotify you wish to find the average listening time of your users. Based on a random sample of 180 users you found that the mean listening time for the sample is 7.34 hours per week and the standard deviation is 6.87 hours.</em></p>
<p><strong>(LC3.6) What is the 95% confidence interval for the mean listening time (the corresponding z-value for the 95% CI is 1.96)?</strong></p>
<ul class="task-list">
<li><label><input type="checkbox" checked="" />[6.34;8.34]</label></li>
<li><label><input type="checkbox" />[7.15;7.55]</label></li>
<li><label><input type="checkbox" />[6.25;8.15]</label></li>
<li><label><input type="checkbox" />[6.54;8.54]</label></li>
<li><label><input type="checkbox" />None of the above</label></li>
</ul>
<p><strong>(LC3.7) How can you compute the standardized variate of a variable X?</strong></p>
<ul class="task-list">
<li><label><input type="checkbox" /><span class="math inline">\(z = {s \over \sqrt{n}}\)</span></label></li>
<li><label><input type="checkbox" checked="" /><span class="math inline">\(z = {X_i - \bar{X} \over s}\)</span></label></li>
<li><label><input type="checkbox" /><span class="math inline">\(z = {s \over X_i-\bar{X}}\)</span></label></li>
<li><label><input type="checkbox" />$z = s * (X_i-{X}) $</label></li>
</ul>
<p><strong>(LC3.8) For a variable that follows a normal distribution, within how many standard deviations of the mean are 95% of values?</strong></p>
<ul class="task-list">
<li><label><input type="checkbox" checked="" />1.960</label></li>
<li><label><input type="checkbox" />1.645</label></li>
<li><label><input type="checkbox" />2.580</label></li>
<li><label><input type="checkbox" />3.210</label></li>
</ul>
<p><strong>(LC3.9) The Null Hypothesis (<span class="math inline">\(H_0\)</span>) is a statement of:</strong></p>
<ul class="task-list">
<li><label><input type="checkbox" checked="" />The status-quo/no effect</label></li>
<li><label><input type="checkbox" />The desired status</label></li>
<li><label><input type="checkbox" />The expected status</label></li>
<li><label><input type="checkbox" />None of the above</label></li>
</ul>
<p><strong>(LC3.10) Which statements about the Null Hypothesis (<span class="math inline">\(H_0\)</span>) are TRUE?</strong></p>
<ul class="task-list">
<li><label><input type="checkbox" />In scientific research, the goal is usually to confirm it</label></li>
<li><label><input type="checkbox" checked="" />In scientific research, the goal is usually to reject it</label></li>
<li><label><input type="checkbox" />It can be confirmed with one test</label></li>
<li><label><input type="checkbox" />None of the above</label></li>
</ul>
<p><strong>(LC3.11) In which setting would you reject the null hypothesis when conducting a statistical test?</strong></p>
<ul class="task-list">
<li><label><input type="checkbox" checked="" />When the absolute value of the calculated test-statistic (e.g., t-value) exceeds the critical value of the test statistic at your specified significance level (e.g., 0.05)</label></li>
<li><label><input type="checkbox" checked="" />When the p-value is smaller than your specified significance level (e.g., 0.05)</label></li>
<li><label><input type="checkbox" checked="" />When the confidence interval associated with the test does not contain the value suggested by the null hypothesis</label></li>
<li><label><input type="checkbox" />When the test-statistic (e.g., t-value) is lower than the critical value of the test statistic at your specified significance level (e.g., 0.05)</label></li>
<li><label><input type="checkbox" />None of the above</label></li>
</ul>
<p><strong>(LC3.8) What does a significant test result tell you?</strong></p>
<ul class="task-list">
<li><label><input type="checkbox" />The importance of an effect</label></li>
<li><label><input type="checkbox" />That the null hypothesis is false</label></li>
<li><label><input type="checkbox" />That the null hypothesis is true</label></li>
<li><label><input type="checkbox" checked="" />None of the above</label></li>
</ul>
</div>
<div id="references-2" class="section level2 unnumbered hasAnchor">
<h2>References<a href="statistical-inference.html#references-2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>Field, A., Miles J., &amp; Field, Z. (2012). Discovering Statistics Using R. Sage Publications.</li>
<li>Malhotra, N. K.(2010). Marketing Research: An Applied Orientation (6th. ed.). Prentice Hall.</li>
<li>Vasishth, S. (2014). An introduction to statistical data analysis (lecture notes)</li>
</ul>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="summarizing-data.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="supervised-learning.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
