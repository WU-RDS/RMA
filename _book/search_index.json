[["index.html", "Retail Marketing Analytics 2024 Welcome!", " Retail Marketing Analytics 2024 Welcome!              Welcome to the course Retail Marketing Analytics! This course covers the conceptual foundations of data analysis techniques for retail marketing managers and applications of these methods to different data sets. The course will be delivered in parts using the flipped classroom teaching method. This means that students are required to familiarize themselves with the contents by means of self-study before each session (i.e., by going through the assigned materials on their own). This website is intended to aid the self-study process by providing you with explanations regarding the relevant concepts and methods in text and video format along with code files and commented outputs that will show you how to implement these methods using the statistical software R. The self-learning process will be complemented with compulsory in-person weekly interactive sessions in the PC lab, which provide ample opportunities to train the acquired knowledge and clarify points that require further discussion. The following pages outline the course schedule and explain how to use this tutorial in detail. If you have any questions, feel free to send me a short email. Daria Yudaeva daria.yudaeva@wu.ac.at "],["introduction.html", "Introduction Course structure Course materials", " Introduction Course structure This course combines asynchronous teaching elements (e.g., texts and pre-recorded videos on this website) with synchronous elements (e.g., weekly in-person interactive sessions in the PC Lab). The syllabus consists of three main parts, as reflected by the structure of this website: Lecture notes: the lecture part will explain the theory behind the concepts and methods and provide you with example applications using the statistical software R. Individual assignments: the individual assignments require you to apply the acquired knowledge to new data sets. Group project: in the group you will design and conduct your own market research project and transfer the knowledge to a real business setting. Schedule In-person lecture The contents on this website are divided into weekly readings. To be able to follow the curriculum and complete the assignments, you need to read the materials assigned for the respective week. The relevant chapters are indicated in the table below. The weekly readings will be complemented with weekly interactive sessions in the PC lab, which provide you with an opportunity to ask questions about the assigned readings. Please note that you need to go through the materials on your own in the week before the respective session. For example, chapters 2, 3 &amp; 4 will be discussed in the second session. The dates and times for the classroom sessions are indicated in the table below for each group separately. It is highly recommended to prepare questions or comments about the materials for these sessions that you think might be interesting and helpful to the class. As a preparation for the in-class discussions and quizzes, you should go through the Learning check section at the end of each chapter. By working through these questions, you may self-assess your progress and identify knowledge gaps regarding the materials that were assigned for the previous week. Learning checks will also be performed in class as means of tracking participation as well as students’ understanding of the material. Lecture dates Date Day Time Room Topics Chapters Mar 7 Thursday 08:30 AM - 11:00 AM TC.-1.61 Introduction to the course Basic concepts of research design 1 Mar 14 Thursday 08:30 AM - 11:30 AM TC.-1.61 Introduction to R &amp; R Markdown I 2, 3, 4 Mar 21 Thursday 08:30 AM - 11:30 AM TC.-1.61 Introduction to R &amp; R Markdown II 2, 3, 4 Apr 11 Thursday 08:30 AM - approx. 11:50 AM (off. 11:30 AM) TC.-1.61 Introduction to inferential statistics 5 Apr 18 Thursday 08:30 AM - 11:30 AM TC.-1.61 Supervised learning I 6 Apr 25 Thursday 08:30 AM - 11:30 AM TC.-1.61 Supervised learning II 6 May 2 Thursday 08:30 AM - 11:30 AM TC.-1.61 Unsupervised learning I 7 May 16 Thursday 09:00 AM - 11:00 AM Online Exam – The self-study format might pose challenges to the learning process because we cannot troubleshoot in person outside of the classroom sessions. Remember that it is very unlikely that you are the only student encountering a particular problem. So please make use of the Discussions on Canvas (see below) to interact with your peers or ask me questions so that everyone else will benefit from the answer (there are no stupid questions!). In case you cannot get answers to address a specific problem, I will be available during the in-person classroom sessions for coaching. Individual assignments There will be 3 individual assignments. These assignments need to be submitted in the R Markdown format (see chapter 8) via Canvas. There will be an in-class session dedicated to the R Markdown reporting format, when the first homework is assigned. Assignment schedule Assignment Assigned Submission Assignment 1: R Basics TBA / see Canvas TBA / see Canvas Assignment 2: Supervised learning TBA / see Canvas TBA / see Canvas Assignment 3: Unsupervised learning TBA / see Canvas TBA / see Canvas Group project The group project consists of an extended analysis of a data set using the methods we covered in the course and the reporting of the results using the R markdown format. The submission date for the group project will be announced soon. Again, please make sure that you have exhausted all other resources to solve a particular problem, such as the online tutorial, the forum on Canvas, and other web resources (see below) before you schedule a coaching session. If you feel that other students might have similar questions and would benefit from an answer to a particular question, you should post the question in the forum on Canvas. Grading Grading is based on the following components: Market research group project (data analysis &amp; reporting): 30% Individual take-home computer exercises (statistical analysis of data sets; 3 assignments accounting for 10% each: 30% Final online exam (concepts &amp; methods): 30% Class participation (weekly quizzes; quantity &amp; quality of contributions during the weekly sessions): 10% The final exam will take place online on May 16, 2024 from 09:00 AM - 11:00 AM. Details about the setup of the exam will be provided in the course. The exam covers questions about the concepts and methods (no coding) and I will provide example exams from the previous years to give you an idea about what type of questions you can expect. To ensure an equal contribution of group members for the group assignment, a peer assessment will be conducted among group members, which enters into the computation of the individual grades for the project. This means that the members of a group are required to assess other students regarding their relative contribution. To successfully pass this course, your weighted final grade needs to exceed 60%. Course materials Main reference The main reference for this course is this website along with the corresponding slides and the pre-recorded video lectures. The relevant materials for each week are indicated in the tables above. The aim of the materials is to condense the contents and direct your attention to the most relevant aspects. This should enable students to study the materials on their own and we can focus our attention during the classroom sessions on clarifying points that require further discussion. At the end of each chapter, you will find a section with references. It is highly recommended that you consult these references for further clarification in case you require additional information on a topic. Further readings           In addition to these lecture notes, there are many excellent books available (many of them for free) that focus on different aspects of R. In fact, there are so many free resources available by now that a team of R programmers has set up a website that provides an overview over the available resources by topic. You can find this overview here: Big Book of R. In case you would like to learn more about the capabilities of R related to the contents of this course, I can particularly recommend the following books: “R for Data Science” An excellent book by Hadley Wickham, which introduces you to R as a tool for doing data science, focusing on a consistent set of packages known as the tidyverse. [FREE online version] “An Introduction to Statistical Learning” This book provides an introduction to statistical learning methods and covers basic methods (e.g., linear regression) as well as more advanced methods (e.g., Support Vector Machines). [FREE online version] “R for Marketing Research and Analytics” A great book that is designed to teach R to marketing practitioners and data scientists. “Statistical Inference via Data Science” Another great book covering topics around Statistical Inference. [FREE online version] “Text Mining with R” This book explains how you can analyze unstructured data (texts) using R. [FREE online version] “Advanced R” Another great book written by Hadley Wickham. Explains more advanced R concepts. [FREE online version] “Hands-On Machine Learning with R” A great reference to learn about machine learning methods in R. The book favors a hands-on approach, growing an intuitive understanding of machine learning through concrete examples and little bit of theory.[FREE online version] “Hands-On Data Science for Marketing” Another good reference regarding Data Science for Marketing. [FREE Code exercises] “R Markdown” A great book about the reporting format ‘R Markdown’, which we will also use for the assignments in this course. [FREE Code exercises] “R Packages” A book which teaches you how to make the most of R’s fantastic package system. [FREE online version] “R Graphics Cookbook” A practical guide that provides more than 150 recipes to help you generate high-quality graphs quickly. [FREE online version] “Using R For Introductory Econometrics” This book covers a nice introduction to R with a focus on the implementation of standard tools and methods used in econometrics. [FREE online version] “Data Science in a Box” Another book covering topics around Data Science using R. [FREE online version] “Efficient R Programming” A good reference to learn efficient workflows using R. [FREE online version] “Discovering Statistics Using R” (Field, A., Miles, J., &amp; Field Zoe, 2012, 1st Edtn.) This textbook offers an accessible and comprehensive introduction to statistics. Discussion forum I strongly encourage you to ask your questions via the online forum (Discussions section) on Canvas. The purpose of the forum is to allow you to discuss questions related to the contents with your classmates and me. Please make use of this forum as much as possible and ask questions if something remained unclear. Remember that there are no stupid questions! And if you know the answer to a question that is asked in the forum, it is also a good exercise to explain the concepts to your classmates. Other web-resources “https://www.r-project.org/” official website “http://www.statmethods.net/” R reference by the author of “R in action” “http://www.rdocumentation.org/” R documentation aggregator “http://stackoverflow.com/” general discussion forum for programmers incl. R “http://stats.stackexchange.com/” discussion forum on statistics and data analytics “http://www.r-bloggers.com/” R blog aggregator “http://www.cookbook-r.com/” useful examples for all kind of R problems “https://ggplot2.tidyverse.org/reference/index.html” reference for data visualization Contact Feel free to send me an email in case you have questions. However, please make sure that you have exhausted all other resources to solve a particular problem, such as the online tutorial, the forum on Canvas, and other web resources (see below) before you schedule a coaching session. If you feel that other students might have similar questions and would benefit from an answer to a particular question, you should post the question in the forum on Canvas. Acknowledgements This tutorial is supported through Digital Learning Project Funding by WU Vienna. None of the materials covered in this tutorial are new. We intend to provide a summary of existing methods from a marketing research perspective and cite the corresponding sources. If you should have any comments or suggestions, please contact us through the github page of this course. "],["preliminaries.html", "1 Preliminaries 1.1 Marketing foundations (recap) 1.2 The research process Learning check References", " 1 Preliminaries This chapter provides an overview of the parameters you need to consider when planning a marketing research study. It is crucial to carefully consider these parameters before conducting your research because empirical studies can be costly and you need to make sure that you will be able to interpret the results from your research in the desired way. For example, in many cases marketing research is about measuring the effectiveness of a firm’s marketing activities. However, quantifying the return on marketing expenditures is not a trivial task. As the nineteenth century Philadelphia retailer John Wanamaker supposedly said, “Half the money I spend on advertising is wasted; the trouble is I don’t know which half.” — John Wanamaker This quote underlines the high level of uncertainty marketing managers face regarding the effective allocation of marketing budgets. Because marketing budgets are allocated across different channels (TV, out-of-home, online, …) it is challenging to attribute market responses, such as an increase in sales, to one specific channel. Although in digital environments it became generally easier to measure responses to advertising (e.g., by tracing views or clicks of banner ads), other issues remain. For example, the exposure to online ads may be a function of a consumer’s browsing behavior (e.g., through targeting), which may reflect other (unobservable) user characteristics that have nothing to do with the advertising. If these characteristics are not properly controlled for, it is easy to overestimate advertising effects (i.e., advertising would appear more effective than it actually is). This chapter discusses ways to avoid such potential pitfalls through the careful planning of your research. In particular, in this chapter you will learn: Why marketing research is important What type of research design is appropriate in which situation The difference between correlation and causality Which scales of measurement to use in which situation The difference between validity and reliability 1.1 Marketing foundations (recap) You will surely have come across various definitions of the term Marketing during your studies. For example, the popular textbook by Kotler &amp; Armstrong (2009) defines Marketing as: “The process by which companies create value for customers and build strong customer relationships in order to capture value from customers in return.” — Kotler &amp; Armstrong (2009) The corresponding marketing process can be depicted by as follows: Figure 1.1: The marketing process Kotler &amp; Armstrong (2009) As the figure above shows, the goal of marketing is to capture value from customers in order to create profits and customer equity. However, in order to achieve this goal, a company needs to first build profitable relationships by creating customer value through an integrated marketing program. As you will likely also recall, a firm’s set of controllable tactical marketing tools can be described in terms of the ‘four Ps’ taxonomy, which is also referred to as the Marketing Mix, consisting of: Product (design, quality, branding, technology, services, etc.) Price (list price, discounts, payment period, payment methods, etc.) Place (trade channels, locations, logistics, e-commerce, etc.) Promotion (advertising, sales promotion, public relations, etc.) The firm blends these marketing tools to produce the desired response in the target market. As can be seen from the marketing process, the first step towards capturing customer value is to understand the marketplace and customer needs. Thus, gaining an understanding of the target customers through marketing research enables firms to design their marketing mix in accordance with the consumers needs and wants, which in turn will then lead to positive outcomes on the customer side (e.g., trust, loyalty, satisfaction, engagement) and on the firm side (e.g., revenue, sales, profit, stock prices). As such, marketing research can be seen as the foundation of the marketing process. 1.2 The research process Now that it is clear why marketing research is important, let’s have a closer look at the underlying process. The flow chart below shows a general depiction of the research process, which will be discussed subsequently. Figure 1.2: The research process (based on Field et al. 2012) 1.2.1 Research question and hypothesis The first step in the research process is to identify a management problem and to derive a research question from it. As a motivating example, imagine that you are a marketing manager at a firm and you are running online advertising campaigns to promote your products. In order to reach your target group more effectively, you make use of targeting criteria based on user profiles, which are available at an additional cost. The head of your department wonders if the extra expenditures associated with the targeting of the advertising campaigns are justified or if the advertising would be similarly effective without the targeting. As depicted in the stylized flow chart above, the research process often starts with the identification of a management problem; something that needs explaining. In our example, this would be the extra expenditures associated with the targeting of online advertising campaigns. This leads to your research question: Is advertising with behavioral targeting more effective compared to advertising without targeting? Based on this research question, you should, if possible, collect some existing data and look for initial evidence. If you have used behavioral targeting in the past, you may want to compare the performance of campaign that used targeting to campaigns that didn’t. Following this initial data screening, you should try to come up with a theory that could explain the effects of targeting. A theory can be thought of as a hypothesized general principle or set of principles that explains known findings about a topic and from which new hypotheses can be generated. In our example, we could build on a rich body of literature on tailoring communications that consistently indicates that tailoring improves communications’ performance (e.g., Lambrecht &amp; Tucker 2013, Lewis et al. 2011). Following this theory, you should formulate a prediction regarding the direction of the expected effects. This hypothesis can be thought of as a prediction from a theory, i.e., in our example: targeting online ads increases, on average, the probability of purchasing from our store. In a next step, you should identify the variables you need to consider in order to test your hypothesis. Particularly, you should clarify what your dependent variable and independent variables are. The dependent variable is the outcome variable referring to the proposed effect. If we would, for example, conduct an experiment to test our hypothesis regarding targeting of online advertising, the dependent variable would be a relevant response variable we are interested in (e.g., the number of sales). The independent variable, in contrast, is the proposed cause (a predictor variable). In our experiment, this would be the variable we manipulate, i.e., the type of advertising (targeted vs. non-targeted). 1.2.2 Choosing a research design Once you have sufficient clarity on your research hypothesis, you should specify the research design. Research designs can be classified according to different criteria, including the data source, the method of analysis, and by the research objective (Malhotra 2010). Figure 1.3: Research designs (based on Malhotra 2010) 1.2.2.1 By data source A first classification of research designs is by the data source, i.e., whether the research is based on primary data or secondary data. Primary data has the main advantage that it is collected by the researcher for the specific purpose of addressing the research problem at hand. In contrast, with secondary data, the data has been collected for some purpose other than the problem at hand and was published in the form of books, articles, and databases by governments, business sources, or market research firms. Although it may be less costly to obtain, secondary data has the disadvantage that not all details are know about the processing of the data and often the data won’t fit the research question you are trying to answer. Hence, in many cases research questions are so specific to a particular management problem that you will need to collect primary data to answer it. 1.2.2.2 By method of analysis Another high level distinction between methods of data analysis is the distinction between qualitative research and quantitative research. The differences between these two types of analysis are summarized in the table below. One of the core distinctions is that while the aim of qualitative research is to explore underlying reasons and motivations based on a small, non-representative samples, the aim of quantitative research is to generalize the results from a large, representative sample to the population of interest using statistical techniques. Thus, qualitative research is often used as a first step in the research process to gain an understanding of the research problem which is then followed by quantitative research. This is why qualitative research is also referred to as exploratory research, in line with its objective. In this course, we will not cover qualitative research methods and will focus our attention on quantitative research methods. Figure 1.4: Qualitative vs. quantitative research (based on Malhotra 2010) 1.2.2.3 By research objective One of the most important aspects you need to reflect on when choosing an appropriate research design is the nature of the research objective. That is, whether the objective you would like to achieve with your quantitative research can be classified as descriptive, predictive, or causal inference. The following table shows examples of research questions that fall in each of these categories. Description Prediction Causal Inference Example of scientific questions How can the customers of our online store be partitioned in classes defined by their characteristics? What is the probability that users who visited our online store last year will purchase from our store within the next month? Will behavioral targeting in online advertising increase, on average, the probability of purchasing from our store within the next month? Data Features: user characteristics (age, gender, location, …), product characteristics of visited pages, …; Output: making a purchases within the next month Inputs: age, gender, frequency of past purchases, recency of last purchases, monetary value of past purchases, past ad exposures, … Outcome: making a purchases within the next month Treatment: initiation of targeting campaign Confounders: for non-experimental settings (interest in product category, eligibility criteria used for targeting …) Example of analytics Cluster Anaylsis … RegressionDecision treesRandom forestsSupport vector machinesNeural networks … Experiments with random assignment Regression Instrumental variables Regression discontinuity Difference-in-differences … Note: Based on the classification of data science tasks by Hernán et al. (2019) 1.2.2.3.1 Descriptive research Descriptive research is aimed at capturing the structure of your data and representing it in a compact manner. Descriptive modeling differs from explanatory (prescriptive) modeling because an underlying causal theory is either absent or incorporated in a less formal way. This means that there is less emphasize on the theory and hypothesis building part of the research process in descriptive research. Descriptive modeling further differs from predictive modeling because it is not aimed at prediction. Having said that, fitting a regression model could be descriptive if it is used for capturing the association between the dependent and independent variables rather than for causal inference or for prediction. An example of descriptive research is the grouping a firm’s customers according to observable customer characteristics (i.e., features) using cluster analysis. This type of analysis represents the original data in a compact manner by capturing the underlying data structure using clusters. Another example would be the reduction of the dimensionality of a data set using principal component analysis. Similar to cluster analysis, this type of model captures the underlying structure of the data by grouping highly correlated input variables into so-called ‘factors’. This type of analysis can be useful, e.g., if predictors in a regression model are highly correlated as we will see later. It should be noted that the computational capacity of many firms has increased drastically over the past years due to a substantial decline of the costs associated with computational power and data storage. These developments have made it feasible for many firms to analyze very large data sets (‘big data’) with the aim of deriving managerial insights. With the rising relevance of large data sets, descriptive research has received increasing attention. The reason is that identifying patters in such vast amounts of data requires structure and often, an initial exploratory, descriptive investigation of large data sets enables the researcher to discover patterns, which then give rise to predictive or causal inference modeling tasks. This is especially true for unstructured data (social media texts, voice data) which can be analyzed using machine learning methods. 1.2.2.3.2 Predictive research Predictive research uses statistical modeling techniques with the aim of predicting new or future observations based on a training data set. That is, the goal is to predict the output value (Y) for new observations given their input values (X). Back in our motivating example, a predictive modeling task would be to predict whether a customer of our web shop will make a purchase within the next month, given a set of input variables. Different from causal inference, the focus of predictive models is on the prediction of future observations. For example, in a regression model the amount of explained variance (\\(R^2\\)) would be a relevant statistic to inspect after fitting a model. It is important to note that in predictive models, the focus in much less on causality, i.e., explaining the effect of a specific input variable (X) on the outcome (Y). This is the job of causal inference. As a consequence, theory often does not play a major role in predictive research. What matters in predictive models is that the model produces reasonable predictions of the outcome of interest (Y). One major concern in such models is overfitting. Overfitting means that a model is so highly tuned to the particularities of one specific data set that it produces very good predictions within this sample, but it doesn’t generalize to other data sets. To assess the predictive ability of a model outside the sample, it is therefore useful split the data set into a training data set and a test data set. The training data set is used to calibrate the model and the test data set (or ‘hold-out data set’) is used for validation, i.e., to assess how well the model predicts values for observations that were not used to calibrate the model. Over the past years, machine learning methods (e.g., random forests, support vector machines, neural networks) have particularly advanced the field of predictive modeling. Hence, these methods are often used in predictive modeling tasks as the table shows. 1.2.2.3.3 Causal inference (aka ‘prescriptive’ research) In many scientific fields, and especially the social sciences, the focus is almost exclusively on research that tests causal hypotheses. To stay within our motivating example, we could be interested in investigating the effect of targeting of online advertising on the effectiveness of a firm’s online marketing efforts. Notice the different focus compared to predictive modeling: in the predictive modeling example, we use observations from the past to predict the probability that a given customer will purchase in the future. In the causal inference task, we are interested in causal effect, such as ‘(By how much) Can we increase the probability that a customer will make a purchase using targeted online advertising?’ Or in other words, what should managers do differently in order to increase the purchase probability? To answer this type of question, a set of theoretically derived constructs are measured by variables X and these variables are assumed to cause an underlying effect, measured by variable Y. Since the focus here is on explaining causal effects, this type of research is also often referred to as explanatory modeling. Another alternative term you may come across is ‘prescriptive modeling’, empasizing that the goal is to make recommendations to managers or policy makers regarding the course of action. For these type of questions, theory has a much more important role compared to predictive and descriptive research. Hence, the focus when it comes to the model output is more on the coefficients associated with the explanatory variables, rather than on the predictive ability of the model. If you are interested in more details, the article ‘To Explain or to Predict?’ by Shmueli (2010) has a nice discussion regarding the difference between explanatory and predictive models. While managers and marketing researchers care most about causal research questions, finding answers to these type of questions is challenging for various reasons. Consider our targeting example. In order to estimate the effect of targeting on sales, we could, for example, compare the conversion rates between two groups of users: 1) users exposed to the targeted advertising campaign, and 2) users who were not exposed to the same targeted advertising campaign. The problem with this type of comparison is that the targeting algorithm uses unobserved variables to decide which users to show the ad. Thus, the exposed and unexposed users may be different in outcomes for reasons that have nothing to do with the advertising. If we advertise products from a particular category, it might simply mean that the users targeted with our advertising might have a stronger preference for this category and might have purchased from our store even without seeing the ad. With regard to the underlying model, the ‘interest in the product category’ can be seen as an unobserved confounder, as the following figure shows. Note that if this omitted variable is not properly controlled for, we will overestimate the advertising effect (see also Lewis et al. 2011). Figure 1.5: Unobserved confounders In other words, the correlations observed between the dependent measure and advertising are often due to unobserved variables, leading to so-called ‘spurious correlations’, i.e., a connection between two variables that appears causal but is not. There are plenty of examples for spurious correlations and you can see one of them in the figure below. Figure 1.6: Spurious correlation Although it appears from the graphic that there is an association between the Internet Explorer market share and the number of murders in the U.S., it wouldn’t really make sense to assume that one of them causes the other. The main difference between correlation and causality is that with correlation we observe changes in an input variable (X) and a change in the outcome (Y), whereas causality means that we change the input variable (X) and observe the resulting changes in the outcome variable (Y). For a causal relationship, three conditions have to be met (Field et al. 2012): Concomitant variation: A cause, X, and an effect, Y, should vary together in the way predicted by the hypothesis under consideration. Time order of occurrence: The causing event must occur before the effect; it cannot occur afterwards. Absence of other possible causal factors: The factor or variable being investigated should be the only possible causal explanation. However, with explanatory models, the existence of a spurious correlation is often less obvious and we will explore different methods of data collection to avoid such pitfalls in the next section. Another important aspect regarding the research design is to create a sampling plan. This means that we need to decide on which units (e.g., survey participants) we will include in our sample. This aspect will be discussed in chapter 5. 1.2.3 Collecting data Once you have decided on the research design, the next step is to choose 1) the methods of data collection and 2) how to measure the variables of interest, which will be discussed in this section. 1.2.3.1 Methods of data collection There are some standard ways of collecting data and two of them will be discussed here: 1) experimental research and 2) observational research. 1.2.3.1.1 Experimental research In the last section, we saw that causal research questions are the ones that matter most to marketing managers. In our example, the question we asked was ‘(By how much) Can we increase the probability that a customer will make a purchase within the next month using targeted online advertising?’. In this section, we will see that experiments are a proper way of establishing a causal relationship. Or, as the famous quote by Box, Hunter, and Hunter (1978) puts is: “To find out what happens when you change something, it is necessary to change it.” — Box, Hunter, and Hunter 1978 The counterfactual - what would have happened without the intervention? When answering these type of causal questions, we would ideally observe the world in two different states simultaneously. In our example, we would ideally observe the same customers at the same time in two states: 1) with targeted online advertising, and 2) without online advertising. Generally, if a customer is exposed to an ad, we would like to know how this customer would have behaved without seeing the ad. This is often referred to as the counterfactual. However since we can only observe the same customer in one state at a given time, we need to find other ways to get as close as possible to this ideal counterfactual. As already indicated above, experiments are a proper way of establishing a causal relationship. The procedure of conducting experiments is usually as follows: Divide test units into homogeneous subsamples Manipulate independent variables and measure dependent variable Random assignment of test units to experimental groups to control for extraneous (potentially confounding) variables Randomization helps to reduce unsystematic variation To see why this is the preferred method of data collection when the focus is on causal effects, it is useful to acknowledge that there are two types of variation in the data: Systematic variation: Differences in the dependent variable (in our example: sales) created by a specific experimental manipulation (in our example, targeted advertising) Unsystematic variation: Differences in the dependent variable created by unknown factors (age, gender, IQ, time of day, measurement error etc.) In order to measure a causal effect, our goal is to minimize the unsystematic variation while maximizing the systematic variation and the goal of randomization is to minimizes unsystematic variation. That is why running experiments is typically superior to other methods of controlling unobserved variables. Or, as Angrist and Pischke (2009) put it: “The most credible and influential research designs use random assignment.” — Angrist and Pischke 2009 Between-subsjects and within-subjects designs In the example above, different customers are assigned to the different groups and one group sees the targeted ads (test group) while the other doesn’t (control group). By assigning the customers randomly to the test and control groups, we can be fairly certain that the groups are comparable in terms of unobserved factors (assuming a large enough sample size). For example, the groups should be similar in terms of their preferences for certain product categories. Assigning different units (in our case: customers) to the test and control conditions in an experiment is also referred to as a between-subjects design as visually depicted by the following figure: Figure 1.7: Between-subjects design The counterpart of the between-subject design is the within-subject design. Using a within-subject design we would manipulate the independent variable and the same units (in our case: customers) would be exposed to both conditions. In our example, the same customers would first see no advertising and the response (purchase probability) would be recorded. Then the same customers would be exposed to the targeted add and the response would be recorded again as depicted by the figure below. Because we record multiple responses per unit, this design is also referred to as a repeated-measures designs. In some settings, this type of design my be beneficial because it helps us to reduce unsystematic variation due to the fact that different units (customers) are assigned to each group (although they have been assigned randomly to the groups, they will still be slightly different). When we compare the same units (customers) in the two conditions there is less of such unsystematic variation because we know that the units we compare are the same. However, often it is difficult to expose the same units to two experimental condition and to ensure that exposing a unit to the first condition has no effect on the measurement of the second condition (crossover effects). Figure 1.8: Within-subjects design With regard to the specification of the test and control conditions there are different options, as the following figure shows. Measures the effect of a targeted advertising on the entire customer base and compares the purchase probability vs. the historical average Disadvantage: cannot rule out alternative explanations Measures the effect of a targeted advertising on a subset of the customer base (as compared to a group with no advertising) Advantage: controls for alternative explanations using random assignment Disadvantage: can only test one version of advertising (e.g., does not include a strict control group with standard advertising) Measure the effect of multiple actions on different test groups (e.g., include a group with standard (non-targeted) ads) Advantages: controls for alternative explanations using random assignment &amp; allows to test multiple advertising strategies Disadvantage: fewer observations per test group Note that the variables we manipulate are also referred to as factors and a particular combination of factor levels is called treatment in an experimental design. In our case, we only have one factor with three levels (i.e., factor: advertising with levels i. targeted advertising, ii. standard advertising, iii. no advertising). If we would have also systematically varied prices with two levels (i.e., high and low), the variable price would be the second factor and the experimental design would be a 3 x 2 factorial design with three levels of advertising and two levels of price that can be combined in (\\(3*2= 6\\)) possible combinations. Conducting field experiments There are different types of settings for experiments. One option is to run the experiments in a lab (lab experiment), which offers a high degree of control over potentially confounding factors. However, often it is beneficial/more realistic to test marketing strategies in a real business setting using a field experiment. A visual depiction of a typical A/B test, where arriving visitors of a website are randomly assigned to one of three groups in a field experiment is shown in the figure below. Figure 1.9: Stylized depiction of A/B testing process Especially for online firms it is very typical to run a large number of field experiments per day in order to optimize their services according to a specified target (e.g., number of conversions). The figure below summarizes some important issues to consider when running field experiments. Figure 1.10: Guidelines for field experiments Decide on unit of randomization: In the first step, you should decide on the unit of randomization. In our example, the unit of randomization was on the customer level, which ensures a high granularity. This means that each customer is assigned to one of the groups and the resulting analysis could be conducted on the user level (i.e., each user represents one line in our data set). The larger the data set, the more statistical power we have to estimate the effects as we will see later. An alternative would be, for example, to assign the membership to the test and control groups according to the postcode. This would mean a lower granularity because customers with the same postcode would be grouped together. Since this would mean one postcode per line in the data set, the statistical power decreases. At the same time this could also increase the unsystematic variation (systematic error) because there might be systematic (unobserved) differences between postcodes (e.g., with respect to income). Ensure no spillover and crossover effects: When spillover effects are likely, you should consider randomizing at a lower granularity. For example, if you conduct a price experiment and your customers would exchange information that would reveal the different price points and thus, potentially jeopardize the experiment (e.g., if a person in the low price condition would order for a person in the high price condition). Randomization at the postcode level would, for example, ensure that all customers within the same area are assigned to the same condition (i.e., either low or high price). As another example, in an experiment with Uber drivers about tipping behavior, Chandar et al. (2019) randomize at the city level (low granularity) to avoid that the drivers would obtain information about the experiment when talking to their colleagues. In addition, to avoid crossover effects, you should ensure that a customer gets exposed to only one condition to avoid that the exposure to multiple treatments will jeopardize the outcome of the experiment. In an online setting you could, for example, use cookies to ensure that a website visitor will see the same version of the website - even for multiple visits. Decide on complete or stratified randomization: Stratified sampling might help to ensure that the units in each group are comparable across crucial dimensions. For example, if you expect a strong impact of household income and our dependent variable and there are systematic differences regarding this variable (e.g., by postcode), you first divide the individuals into subgroups (stratas) and then sample the units equally from these subgroups (e.g., equal number of treated and control units from one postcode). Although randomly assigning units should also ensure an approximately equal distribution of income groups, using stratified randomization it is less likely that the group composition will systematically differ by chance. 1.2.3.1.2 Observational research In contrast to experimental research, in observational research the researcher observes what happens naturally, without interfering. In the previous section we saw that experiments are a proper way of establishing causal relationships. The question then is the following: when the most interesting research questions are causal questions and experiments are the ‘gold standard’ to infer causal effects, why are researchers often confronted with observational data? As the statistician Andrew Gelman (2010) puts it: “Given the manifest virtues of experiments, why do I almost always analyze observational data? The short answer is almost all data out there are observational.” — Gelman (2010) What’s your identification strategy? The answer to this question is related to the fact that in many situations, experiments are not feasible, not appropriate, or simply too costly to conduct. Going back to the ‘four Ps’ taxonomy from the beginning of this chapter, experimentation is increasingly used to inform advertising decisions where many platforms such as the Google Ad Manager offer easy-to-use solutions for the implementation of A/B testing. However, as Goldfarb &amp; Tucker (2014) note, it is far more difficult for practitioners and researchers to run field experiments to inform channel and product development decisions because such experiments would be too time-consuming or often require a level of measurement of long-term implications that is difficult to attain. In addition, field experiments with varying prices are often challenging to conduct because customers may find them unfair. In these situations, experiments are difficult to conduct. Observational data, in contrast, are often fairly easy to obtain. Many firms, for example, keep records about prices and sales of products over time. With this retrospective observational data it is possible to calculate price elasticity, i.e., the relative change in sales due to a relative change in price. The problem with this type of data is, however, that the price setting behavior of managers may be driven by unobserved factors that could bias the estimates. As an example, consider an ice cream seller who sets her prices according to the weather - if the weather is good, she increases prices and if the weather is bad, she decreases prices. As you can image, the sales pattern reveals that, despite the higher prices, she sells more ice cream on warmer than on colder days. If you would attempt to estimate the price elasticity without controlling for weather, the analysis would suggest a positive relationship between price and sales, i.e., the higher the prices, the more ice cream she will sell. It is easy to see that the weather is an unobserved factor in the analysis that confounds the effect of price on sales. Similar issues can often be observed when estimating advertising effects in settings where managers set advertising budgets according to unobserved factors. This, among other things, is the reason why it is generally more challenging to estimate causal effects from observational data. In cases like this, researchers need to carefully consider their identification strategy, i.e., the procedure of estimating causal effects from observational data. As Angrist and Pischke (2009) put it: “Underlying this is the recognition, description, and presentation of the identification strategy, or the manner in which a researcher uses observational data (i.e., data not generated by a randomized trial) to approximate a real experiment” — Angrist and Pischke (2009), p. 7 There are different approaches available to identify causal effects with observational data. Angrist and Pischke (2009, p. 7) describe the five most common approaches as the “Furious Five methods of causal inference,” and they refer to 1) random assignment, 2) regression, 3) instrumental variables, 4) regression discontinuity, and 5) differences in differences. Varian (2016) provides a concise overview over these identification strategies. While we won’t cover all of the approaches here, we will discuss quasi-experiments and their analysis using difference-in-differences analysis in more detail because of their high practical relevance. Analyzing quasi-experiments As Goldfarb &amp; Tucker (2014) note: “Quasi-experimental tools mimic the random assignment that is inherent in lab experiments and that is often referred to as the `gold standard’ for identifying causal relationships.” — Goldfarb &amp; Tucker (2014), p. 7 Different from an experiment with random assignment of test units, in quasi-experiments the intervention occurs naturally and the units self-select into the test and control conditions without an intervention of the researcher. As an example, assume that you are interested in estimating the effect of music streaming services (e.g., Spotify) on consumers expenditures for music products in other channels (e.g., paid downloads, CDs). This is an example of a typical multi-channel distribution problem in marketing. Wlömert &amp; Papies (2016) study a panel of music consumers by tracking their music expenditures over time. The intervention occurred when a popular streaming service (i.e., Spotify) entered the market. In this study, the research had no control over who would start using the streaming service and the surveyed consumers self-selected into the test and control conditions. A data set like this where you observe multiple units (here: music consumers) over time time is called a panel data set and it has two variance components: 1) the variance across consumers at each given point in time is referred to as the cross-sectional variance, and 2) the variance for each consumer over time is the longitudinal variance. Hence, a research design that focuses on one point in time is also referred to as a cross-sectional design and a research design where the same units are observed at multiple points in time is referred to as a longitudinal design as shown in the figure below. Figure 1.11: Cross-sectional vs. longitudinal design The main advantage of a longitudinal design is that it allows researcher to observe changes over time, which is particularly helpful for the identification of causal effects. Although the observational units self-select into the test and control conditions, it is often possible to account for unobserved individual-level differences by focusing on the change over time. A method that lends itself very well for this purpose is the difference-in-differences (DiD) estimator. Continuing with the example from above, imagine we would have only conducted one cross-sectional study at T2 (after the streaming service had been introduced to the market). As can be seen from the figure below, Spotify users spend, on average, more money on music products from other channels (CDs &amp; downloads). However, you cannot conclude from this observation that Spotify causes them to spend more money on music from other channels. Rather, in the absence of random assignment, the users self-selected into the test and control conditions and it is likely that music enthusiasts (i.e., consumers with a high interest in music) started using the streaming service. The unobserved factor in this case is the ‘interest in music products’. Figure 1.12: Example of DID study design (Wlömert &amp; Papies 2016) The DiD estimator exploits the longitudinal dimension of the data and focuses on the change over time. This is shown in the following figure: Figure 1.13: Difference-In-Differences Estimation In this stylized example, we observe both groups the test group and the control group at two points in time - before the intervention (pre) and after the intervention (post). If the intervention has no effect on the outcome variable, the difference between the groups should be the same before and after (assuming, of course, that no other intervention occurred that might have induced a change in one of the groups). In the figure above, the blue dotted line represents our expectation regarding the outcome variable for the test group after the intervention and the green solid line represents the observations for the control group. We can see that the actually observed outcome (purple line) for the test group is larger then expected based on the difference before the intervention. This difference (denoted as \\(\\Delta\\)) is the difference-in-differences estimate and in this example, the intervention had a positive effect on the outcome. It can be derived as follows (see Varian 2016): \\(Yt,pre\\): outcome before intervention for treated groups \\(Yt,post\\): outcome after intervention for treated groups \\(Yc,pre\\): outcome before intervention for control groups \\(Yc,post\\): outcome after intervention for control groups As mentioned above, the counterfactual is based on the assumption that the (unobserved) change in the outcome by the treated would be the same as the (observed) change in purchases by the control group. To get the impact of the intervention, we then compare the predicted counterfactual outcome to the actual outcome base on the following table: Period Treatment Control Counterfactual Before Yt,pre Yc,pre Yt,pre After Yt,post Yc,post Yt,pre + (Yc,post - Yc,pre) Hence, the effect of the treatment on the treated is \\((Yt,post - Yt,pre) − (Yc,post - Yc,pre)\\). Going back to our music example above, the effect of the treatment on the treated can be computed as follows: \\((7.90 - 10.40) - (4.60 - 4.60) = -2.50\\). This means that the introduction of Spotify reduces the expenditures in other channels by 2.50EUR per user per month on average. There are other approaches that researchers may use to mimic random assignment using data from quasi-experiments, which we won’t discuss in detail. For example, matching procedures may be used to make the control and test groups comparable across a set of observable characteristics. In their study of the music market, Datta et al. (2017), for example, investigate whether the adoption of streaming services leads users to diversify their tastes. Similar to the example above, consumers self-selected into the conditions of Spotify users and non-users. Through statistical matching procedures they identified ‘statistical twins’, i.e., for each Spotify user they identified one person from the group of non-users who is as comparable as possible to the respective user across the set of observable characteristics. The goal is to make the two groups as comparable as possible except for the fact that users in one group adopted the music services and the users in the other group didn’t. Although causal models for observational data are often challenging to implement, there are some packages that make it easier for researchers to apply fairly complex models using just a few lines of code. One such example is the Causal Impact Package which has been developed by Google. We will cover regression models later, but if you are interested you could already have a look at the package description to see what it can do. The corresponding video summarizes the underlying intuition nicely. 1.2.3.2 Measurement and scaling After deciding on the method of data collection, you need to clarify how you will measure the specified variables. 1.2.3.2.1 Levels of measurement The first distinction you need to consider pertains to the level of measurement. This step is important because the level of measurement determines what type of analysis you will be able to apply once the data has been collected. Thus, you should make sure that your plan for the data analysis is aligned with the levels of measurement of the dependent and independent variables. There are different characteristics that a scale can have: Description: Unique labels or descriptors that are used to designate each value of the scale. Order: Relative sizes or positions of the descriptors. Described “greater than”, “less than”, and “equal to”. Distance: Absolute differences between the scale descriptors are known and may be expressed in units. Origin: Scale has a unique or fixed beginning or true zero point. These characteristics determine a variable’s level of measurement as we will see next. Categorical (non-metric) variables There are two types of categorical scales: nominal scales and ordinal scales. Nominal scales only exhibit the most basic of the characteristics above, namely description. Thus, the values we observe on a nominal scale only serve as labels for identification and categorization. If the scale values are numbers, these numbers do not reflect the amount of the characteristic possessed by the objects. As an example, consider the starting number of boats in a boat race as the picture below shows. Figure 1.14: Example: nominal scale (starting numbers) Note that for a scale with two categories, a nominal variable is also called a binary variable. The only permissible mathematical operation with nominal scales is counting. For example, you may count how many participants you have from different occupations. Besides description, ordinal scales also exhibit the characteristic of order. That is, the numbers indicate the relative position of objects. As an example, consider the order that boats cross the finish line in a boat race. Figure 1.15: Example: ordinal scale (finishing order) Besides counting, you can also order objects. However, note that while the order of objects is known, the scale does not reveal the magnitude of difference between objects. For ordinal variables, all statistics that are based on ranking the data are permissible, such as computing the median, percentiles, ranges, and minimum/maximum values. For example, if you have a 7-point Likert-scale ranging from “fully agree” to “fully disagree” you may compute the median response across objects. Continuous (metric) variables There are two types of continuous scales: interval scales and ratio scales. Besides description and order, interval scales also possess the characteristic of distance. Hence, with interval scales, the differences between objects can be compared. As an example, an expert jury might rate the design of the boats on a scale from 0 to 10. Figure 1.16: Example: interval scale (design rating 1-10) For interval scales - as the name suggests - the intervals at different levels of the scale need to be the equal. For example, the difference between, say 8 and 9 is the same as the difference between, say 3 and 4. However, the zero point for interval scales is arbitrary. In our example, the jury might have also judged the boats on a scale from 10 to 20 and the result would have been the same. Also then, the difference between, say 18 and 19 would be the same as the difference between, say 13 and 14 (1 scale point). Note, however, that it is not meaningful to take ratios of scale values. Because the zero point is arbitrary it is not meaningful, for example, to say that the second boat (rating: 8.4) is twice as good as the third boat (rating: 4.2). If you would change the scale to the range between, say 10 and 20, the ratio wouldn’t be preserved (i.e., \\(8.4/4.2 \\neq 18.4/14.2\\)) but the difference in scale points would be the same (4.2 points). Because the distance between the objects is know, we may compute statistics such as mean and standard deviation with interval level data. Lastly, ratio scales possess all properties of nominal, ordinal and interval scales. What’s more, ratio scales have an absolute zero point. As an example, consider the time it takes a boat in a race to cross the finish line as shown below. Figure 1.17: Example: ratio scale (time to finish) In this case ratios are a meaningful way fo comparing objects. For example, the first boat (7.1 sec.) is twice as fast as the second boat (14.2 sec.). A good way to check whether a variable in measure on a ratio scale is to think about the interpretation of the zero point. With ratio scales, the zero points indicates the absence of something (time in our example). All statistical techniques can be applied to ratio data. The following table summarizes which statistics may be applied to which scale type. Don’t worry if not all of it makes sense now. We will revisit this table in chapter 4 when we will go through the different statistics. Figure 1.18: Permissible statistics for different levels of measurement As already mentioned, a good understanding of scale types is important to decide which method to apply to test your hypothesis given the data at hand. There are many different flow charts like the one below that can be used as a guide to decide which method to select based on the scale types. Figure 1.19: Flow chart for test selection (McElreath 2015) There are also many web resources that are useful to determine what type of test is appropriate given the scale type of the dependent and independent variables. For example, check out this website by UCLA which not only shows which type of test is appropriate but also has R code available for each of these tests. For survey-based research, there are many different scale types to choose from. Below you can find a summary of some of the most common scaling techniques. For details regarding these scaling techniques, please refer to chapter 10, which is dedicated to the topic of questionnaire design. Figure 1.20: Scaling techniques 1.2.3.2.2 Measurement accuracy Once you have decided on which scale types to use, you should ensure that your measures accurately represent the variable you intended to measure. More precisely, the two goals of measurement are validity (truthfulness; i.e., our measure captures the variable we intended to measure), and reliability (consistency; i.e., the measure consistently measures the same variable). However, it is important to acknowledge that a measurement is not the true value of the characteristic of interest but rather an observation of it. Hence, the difference between the information sought by the researcher and the information generated by the measurement process is the measurement error. There are two types of measurement errors: 1) random error, leading to an overall less precise measurement of the variable of interest, and 2) systematic error, causing the observed value to consistently deviate from the variable of interest. Thus, the observed measurement of a variable \\(X\\) can be expressed in terms of the deviation from the true value with respect to the random and systematic measurement errors as follows: \\(X_O = X_T + X_S + X_R\\), where \\(X_O\\) = Observed value of a variable X \\(X_T\\) = True value of the variable X \\(X_S\\) = Systematic error in measuring X \\(X_R\\) = Random error in measuring X Hence, the relationship between the two types of errors and our goals of reliability and validity can be expressed as follows. Reliability (= consistency) Reliability refers to the extent to which a scale produces consistent results in repeated measurements. This implies the absence of random error: \\(X_R \\rightarrow 0\\). However, this does not necessarily imply the absence of systematic error. Thus, \\(X_0 = X_T + X_S | \\rightarrow X_R = 0\\). In the lower left corner of the figure below, you can find an example for a measurement with a high reliability that consistently misses the true value of the variable (i.e., low validity). As an example, consider you wish to measure the intelligence of a person using the head circumference. While the head circumference should produce consistent results in repeated measurements, it is probably not a good proxy for intelligence (low validity). This shows that perfect while perfect reliability requires the absence of random error, it doesn’t require the absence of systematic error. Validity (= truthfulness) Validity refer to the extent to which differences in observed scale scores reflect true differences among objects on the characteristic being measured. Hence, perfect validity requires the absence of both type of errors. Thus, \\(X_0 = X_T | \\rightarrow X_S = 0, X_R = 0\\). The figure below visualizes the to goals of reliability and validity as a function of the two types of measurement error. Figure 1.21: Validity vs. reliability Single vs. multi-item scales Related to the concept of measurement accuracy are some choices that the researcher can make to increase the reliability and validity of the measures. The first choice is between single and multi-item scales to measure a construct. A construct is a specific type of concept that exists at a higher level of abstraction than everyday concepts. The construct is unobservable (‘latent’) but can be inferred from other measurable variables (‘items’) that together comprise a scale (latent construct). A multi-item scale consists of multiple items, where an item is a single question or statement to be evaluated. The following figure depicts and example of a multi-item scales to measure the construct ‘satisfaction’. Figure 1.22: Multi-item scales Instead of using three different items, we could have also simply used one general item to measure satisfaction.The decision whether to use one or more items depends on the complexity of the construct and usually the rule of thumb is to use as few items as necessary given the complexity of a construct. Given that the empirical evidence regarding the use of single-item vs. multi-item scales is mixed (Bergkvist &amp; Rossiter 2007, Bergvist 2015, Kamakura 2015), the decision should be made on a case-by-case basis, taking the advantages and disadvantages of both approaches into account (see table below). Figure 1.23: Formative vs. reflective measurement As another example, consider the two versions for measuring a person’s statistical ability below. The first version uses a single item scale while the second uses a multi-item scale. Note that the last item of the multi-item scale is reverse-coded, meaning that while all other statements are worded positively, this item is worded negatively. This is often done as a reliability check to prevent that, for example, respondents become inattentive and always provide answers in the same response category. If the reverse-coded item shows a low correlation with the remaining items, this signals a low reliability of the scale. Figure 1.24: Single vs. multi-item scale Formative vs. reflective scales Another decision related to the concept of measurement accuracy is the choice between formative and reflective measurement. As can be see from the following example, the construct “degree of drunkenness” can be measured in two different ways. While a formative measurement uses items that cause the construct, the reflective measurement uses items that reflect the construct. One advantage of formative measurement is that managers are often interested to know which specific aspects cause a change in the underlying latent construct so that they can address these aspects. However, this approach also has a downside. Imagine in the example below that a person got drunk by drinking wine. Because our measure doesn’t include this specific item, it would have a low validity. This example shows that using formative measures it is important that you consider all possible aspects that might possibly cause the construct of interest. Because this is often difficult to ensure, reflective measurements tend to be more popular. Here, the goal is to have a set of highly correlated items and even if we would remove one of them, we would still end up with a fairly accurate measurement of the construct. Reflective measurements have the additional advantage that it is possible to test their reliability using statistical tests, as we will see later. Figure 1.25: Formative vs. reflective measurement After discussing all the steps in the research process up to the data collection, the subsequent sections will be concerned with data analysis. Learning check (LC1.1) Indicate the level of measurement of the following variables: Occupation of survey participants Willingness-to-pay for a product Your grade in the marketing research course (1, 2, 3, 4, 5) Rank order of most important product attributes Student registration number Gender of survey participants Consumer preferences measured on a 5-point Likert scale Mileage (kilometers per liter) a car gets Age of survey participants Temperature in °C Number of products sold (LC1.2) Which of the answers is correct? A nominal scale … …has an absolute zero point …possesses all properties of an ordinal scale …can have numerical values …serves as a label to classify/categorize objects None of the above (LC1.3) Which of these statements regarding formative constructs are true? Indicators (items) measure the cause for the change of the not directly observable construct Indicators (items) measure the effect of the change in a not directly observable construct The indicators (items) of the scale should be highly correlated Multi-item measurement is particularly important for the increase in the reliability of the measurement None of the above (LC1.4) In causal inference tasks, … …the main concern is to generate predictions of future outcomes …the main concern is to maximize the explained variance …observational research is the ‘gold standard’ …randomization minimizes unsystematic variation None of the above (LC1.5) True or false? Reliability refers to the consistency of a measurement. True False (LC1.5) Using a between-subjects design, we … … manipulate the independent variable (treatment) using different persons for each group … manipulate independent variable using the same participants for each group … manipulate the dependent variable (treatment) using different persons for each group … manipulate dependent variable using the same participants for each group None of the above (LC1.6) When conducting field experiments, the term crossover effect means … … that an individual who was supposed to be assigned to one treatment is accidentally exposed to another treatment … that a treated individual affects the outcomes for other untreated individuals … that individuals can be part of both the test and the control condition None of the above (LC1.7) Conditions for causality are … … a cause and an effect should vary together in the way predicted by the hypothesis under consideration … random assignment of test units … the causing event must occur before the effect … absence of other possible causal factors None of the above (LC1.8) The effect of the treatment on the treated in DID models can be written as … \\((Yt,pre - Yt,post) − (Yc,post - Yc,pre)\\) \\((Yt,post - Yt,pre) − (Yc,post - Yc,pre)\\) \\((Yt,pre - Yt,post) − (Yc,pre - Yc,post)\\) \\((Yt,post - Yt,pre) − (Yc,pre - Yc,post)\\) None of the above (LC1.9) In predictive modeling tasks … … once major concern are unobserved confounders … one major concern is overfitting … we primarily care about the predictive ability of the model … we primarily care about causal effects None of the above (LC1.10) True or false? For ordinal scales, the magnitude of difference between scale points is known True False (LC1.11) For which scale types is it meaningful to compute the mean? Ratio Nominal Interval Ordinal None of the above (LC1.12) In predictive modeling tasks … … we primarily care about the predictive ability of the model … one major concern is overfitting … we primarily care about causal effects … one major concern are unobserved confounders References Angrist J.D. &amp; Pischke, J.S. (2009) Mostly Harmless Econometrics (Princeton Univ Press,Princeton). Bergkvist, L., &amp; J.R. Rossiter (2007), “The Predictive Validity of Multiple-Item Versus Single-Item Measures of the Same Constructs,” Journal of Marketing Research. Bergvist, L. (2015): Appropriate Use of Single-Item Measures is Here to Stay, Marketing Letters, 26(3). Box, Hunter, &amp; Hunter (1978). Statistics for experimenters, John Wiley &amp; Sons, Inc. Chandar, B., Gneezy, U., List, J.A., Muir, I. (2019). The Drivers of Social Preferences: Evidence from a Nationwide Tipping Field Experiment, Working Paper. Datta et al. (2017). Changing Their Tune: How Consumers’ Adoption of Online Streaming Affects Music Consumption and Discovery, Marketing Science, 37(1), 1-175. Field, A., Miles J., &amp; Field, Z. (2012). Discovering Statistics Using R. Sage Publications. Gelman (2010). Experimental Reasoning in Social Science in Field Experiments and their Critics. Yale University Press. Goldfarb, A. &amp; Tucker, C. E. (2014). Conducting Research with Quasi-Experiments: A Guide for Marketers. Working Paper. Kamakura, W. (2015): Measure twice and cut once: the carpenter’s rule still applies , Marketing Letters, 26(3). Kotler, P. &amp; Armstrong, G. (2009). Principles of Marketing (13th ed.), Prentice Hall. Lambrecht, A., &amp; Tucker, C. E. (2018). Field experiments. In: Hanssens, D M &amp; Mizik, N. (eds.), Handbook of Marketing Analytics, Edward Elgar, 32-51. Lambrecht, A. &amp; Tucker, C. E. (2013). When Does Retargeting Work? Information Specificity in Online Advertising, Journal of Markering Research 50(5), 561–576. Lewis, Randall A., Justin M. Rao, and David H. Reiley (2011). Here, There, Everywhere: Correlated Online Behaviors Can Lead to Overestimates of the Effects of Advertising,” in Proceedings of the International Conference on World Wide Web. New York: Association for Computing Machinery, 156–66. Malhotra, N. K.(2010). Marketing Research: An Applied Orientation (6th. ed.). Prentice Hall. Miguel A. Hernán, John Hsu &amp; Brian Healy (2019). A Second Chance to Get Causal Inference Right: A Classification of Data Science Tasks, CHANCE, 32:1, 42-49. Pearl, J. (2009). Causal inference in statistics: An overview. Statistics Surveys, 3, 96–146. Pearl, J. (2018). The Book of Why: The New Science of Cause and Effect, Basic Books. Shmueli, G. (2010). To Explain or to Predict?, Statistical Science, 25(3), 289-310. Varian, H.R. (2016). Causal inference in economics and marketing, Proceedings of the National Academy of Sciences, 113(27), 7310-7315. Wlömert N, Papies D (2016) On-demand streaming services and music industry revenues - Insights from Spotify’s market entry. Internat. J. Res. Marketing 33(2):314-327. "],["getting-started-with-r.html", "2 Getting started with R 2.1 How to download and install R and RStudio 2.2 The R Studio interface 2.3 Functions 2.4 Packages 2.5 A typical R session 2.6 Getting help", " 2 Getting started with R In this course, we will work with the statistical software package R. Please make sure R is already installed on your computer before the tutorials start. The Comprehensive R Archive Network (CRAN) contains compiled versions of the program that are ready to use free of charge: Download R [FREE download] RStudio provides a graphical user interface (GUI) that makes working with R easier. You can also download RStudio for free: Download R Studio (Windows, Linux, OSX, …). The R Studio software is built on top of R, which means that you can use R without R Studio, but you cannot use R Studio without R. The reason why we will use R Studio is that it provides a nicer user interface compared to the standard R interface. There are several advantages of R over other statistical software packages: It`s free A lot of free training material Runs on a variety of platforms (Windows, Linux, OSX, …) Contains statistical routines not yet available in other programs. Active global community (e.g., https://www.r-bloggers.com/). Many specialized user-written packages. It has its own journal (e.g., http://journal.r-project.org). Highly integrated and interfaces to other programs. It is becoming increasingly popular among practitioners. It is a valuable skill to have on the job market. It is not as complicated as you might think. R is powerful. … 2.1 How to download and install R and RStudio The following video gives you an overview of how to download and install the R and R Studio software. 2.2 The R Studio interface The following video gives you an introduction to the R Studio interface. 2.3 Functions You can download the corresponding R-Code here When analyzing data in R, you will access most of the functionalities by calling functions. A function is a piece of code written to carry out a specified task (e.g., the lm()-function to run a linear regression). It may or may not accept arguments or parameters and it may or may not return one or more values. Functions are generally called like this: function_name(arg1 = val1, arg2 = val2, ...) To give you an example, let’s use the built-in seq()-function to generate a sequence of numbers. RStudio has some nice features that help you when writing code. For example, when you type “se” and hit TAB, a pop-up shows you possible completions. The more letters you type in, the more precise the suggestions will become and you will notice that after typing in the third letter, a pop-up with possible completions will appear automatically and you can select the desired function using the ↑/↓ arrows and hitting ENTER. The pop-up even reminds you of the arguments that a function takes. If you require more details, you may either press the F1 key or type in ?seq and you will find the details for the function in the help tab in the lower right pane. When you have selected the desired function from the pop-up, RStudio will automatically add matching opening and closing parentheses (i.e., go from seq to seq()). Within the parentheses you may now type in the arguments that the function takes. Let’s use seq() to generate a sequence of numbers from 1 to 10. To do this, you may include the argument names (i.e., from =, to =), or just the desired values in the correct order. An important thing to note is that R is case-sensitive, meaning that Seq() and seq() are viewed as two different functions by R. # &lt;- this is a comment and is ignored by the R-interpreter seq(from = 1, to = 10) #creates sequence from 1 to 10 ## [1] 1 2 3 4 5 6 7 8 9 10 seq(1,10) #same result ## [1] 1 2 3 4 5 6 7 8 9 10 Note that if you specify the argument names, you may enter them in any order. However, if do not include the argument names you must adhere to the order that is specified for the respective function. seq(to = 10,from = 1) #produces desired results ## [1] 1 2 3 4 5 6 7 8 9 10 seq(10,1) #produces reversed sequence ## [1] 10 9 8 7 6 5 4 3 2 1 2.4 Packages Most of the R functionalities are contained in distinct modules called packages. When R is installed, a small set of packages is also installed. For example, the Base R package contains the basic functions which let R function as a language: arithmetic, input/output, basic programming support, etc.. However, a large number of packages exist that contain specialized functions that will help you to achieve specific tasks. To access the functions outside the scope of the pre-installed packages, you have to install the package first using the install.packages()-function. For example, to install the tidyverse package to manipulate data and create graphics, type in install.packages(\"tidyverse\"). Note that you only have to install a package once. After you have installed a package, you may load it to access its functionalities using the library()-function. E.g., to load the tidyverse-package, type in library(tidyverse). # Only run for the first time: # install.packages(&quot;tidyverse&quot;) # Run to load package: library(&quot;tidyverse&quot;) ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ── ## ✔ dplyr 1.1.3 ✔ readr 2.1.4 ## ✔ forcats 1.0.0 ✔ stringr 1.5.0 ## ✔ ggplot2 3.4.4 ✔ tibble 3.2.1 ## ✔ lubridate 1.9.3 ✔ tidyr 1.3.0 ## ✔ purrr 1.0.2 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors # Now we can use functionality provided by &quot;tidyverse&quot; # We will see in the coming lectures how the following code works: ggplot(economics, aes(x = date, y = pop)) + geom_line() The number of R packages is rapidly increasing and there are many specialized packages to perform different types of analytics.   2.5 A typical R session Open RStudio. Make sure that your working directory is set correctly. The working directory is the location where R will look for files you would like to load and where any files you write to disk will be saved. If you open an existing R script from a specific folder, this folder will, by default, be the working directory. You can check your working directory by using the getwd()-function. In case you wish to change your working directory, you can use the setwd()-function and specify the desired location (i.e., setwd(path_to_project_folder)). Notice that you have to use / instead of \\ to specify the path (i.e., Windows paths copied from the explorer will not work before you change the backward slashes with forward slashes). Alternatively, you can set the working directory with R-Studio by clicking on the “Sessions” tab and selecting “Set Working Directory”. Load your data that you wish to analyze (using procedures that we will cover later) Perform statistical analysis on your data (using methods that we will cover later) Save your workspace. The R workspace is your current working environment incl. any user-defined objects (e.g., data frames, functions). You can save an image of the current workspace to a file called ”.RData”. In fact, RStudio will ask you automatically if you would like to save the workspace when you close the program at the end of the session. In addition, you may save an image of the workspace at any time during the session using the save.image()-function. This saves the workspace image to the current working directory. When you re-open R from that working directory, the workspace will be loaded, and all these things will be available to you again. You may also save the image to any other location by specifying the path to the folder explicitly (i.e., save.image(path_to_project_folder)). If you open R from a different location, you may load the workspace manually using the load(\"\")-function which points to the image file in the respective directory (e.g., load(\"path_to_project_folder/.RData\"). Although it is quite common, saving your workspace is not always required. Especially when you save your work in an R script file (which is highly recommended), you will be able to restore your latest results by simply executing the code contained therein again. This also prevents you from carrying over potential mistakes from one session to the next. 2.6 Getting help Source: Allison Horst Errors &amp; warnings: because R is interactive, consider errors your friends! Most importantly: the more time you spend using R, the more comfortable you become with it and it will be easier to see its advantages Built-in R tutorial: type in “help.start()” to get to the official R tutorial Questions regarding specific functions: type in “?function_name” to get to the help page of specific functions (e.g., “?lm” gives you help on the lm() function) Video tutorials: Make use of one of the many video tutorials on YouTube (e.g., http://www.r-bloggers.com/learn-r-from-the-ground-up/) R Cheatsheets: Cheat sheets make it easy to learn about and use some popular packages (https://www.rstudio.com/resources/cheatsheets/). They can also be accessed from within RStudio under the “help” menu "],["data-handling.html", "3 Data handling 3.1 Basic data handling 3.2 Data import and export Learning check", " 3 Data handling This chapter covers the basics of data handling in R. 3.1 Basic data handling 3.1.1 Creating objects Anything created in R is an object. You can assign values to objects using the assignment operator &lt;-: x &lt;- &quot;hello world&quot; #assigns the words &quot;hello world&quot; to the object x #this is a comment Note that comments may be included in the code after a #. The text after # is not evaluated when the code is run; they can be written directly after the code or in a separate line. To see the value of an object, simply type its name into the console and hit enter: x #print the value of x to the console ## [1] &quot;hello world&quot; You can also explicitly tell R to print the value of an object: print(x) #print the value of x to the console ## [1] &quot;hello world&quot; Note that because we assign characters in this case (as opposed to e.g., numeric values), we need to wrap the words in quotation marks, which must always come in pairs. Although RStudio automatically adds a pair of quotation marks (i.e., opening and closing marks) when you enter the opening marks it could be that you end up with a mismatch by accident (e.g., x &lt;- \"hello). In this case, R will show you the continuation character “+”. The same could happen if you did not execute the full command by accident. The “+” means that R is expecting more input. If this happens, either add the missing pair, or press ESCAPE to abort the expression and try again. To change the value of an object, you can simply overwrite the previous value. For example, you could also assign a numeric value to “x” to perform some basic operations: x &lt;- 2 #assigns the value of 2 to the object x print(x) ## [1] 2 x == 2 #checks whether the value of x is equal to 2 ## [1] TRUE x != 3 #checks whether the value of x is NOT equal to 3 ## [1] TRUE x &lt; 3 #checks whether the value of x is less than 3 ## [1] TRUE x &gt; 3 #checks whether the value of x is greater than 3 ## [1] FALSE Note that the name of the object is completely arbitrary. We could also define a second object “y”, assign it a different value and use it to perform some basic mathematical operations: y &lt;- 5 #assigns the value of 2 to the object x x == y #checks whether the value of x to the value of y ## [1] FALSE x*y #multiplication of x and y ## [1] 10 x + y #adds the values of x and y together ## [1] 7 y^2 + 3*x #adds the value of y squared and 3x the value of x together ## [1] 31 Object names Please note that object names must start with a letter and can only contain letters, numbers, as well as the ., and _ separators. It is important to give your objects descriptive names and to be as consistent as possible with the naming structure. In this tutorial we will be using lower case words separated by underscores (e.g., object_name). There are other naming conventions, such as using a . as a separator (e.g., object.name), or using upper case letters (objectName). It doesn’t really matter which one you choose, as long as you are consistent. 3.1.2 Data types The most important types of data are: Data type Description Numeric Approximations of the real numbers, \\(\\normalsize\\mathbb{R}\\) (e.g., mileage a car gets: 23.6, 20.9, etc.) Integer Whole numbers, \\(\\normalsize\\mathbb{Z}\\) (e.g., number of sales: 7, 0, 120, 63, etc.) Character Text data (strings, e.g., product names) Factor Categorical data for classification (e.g., product groups) Logical TRUE, FALSE Date Date variables (e.g., sales dates: 21-06-2015, 06-21-15, 21-Jun-2015, etc.) Variables can be converted from one type to another using the appropriate functions (e.g., as.numeric(),as.integer(),as.character(), as.factor(),as.logical(), as.Date()). For example, we could convert the object y to character as follows: y &lt;- as.character(y) print(y) ## [1] &quot;5&quot; Notice how the value is in quotation marks since it is now of type character. Entering a vector of data into R can be done with the c(x1,x2,..,x_n) (“concatenate”) command. In order to be able to use our vector (or any other variable) later on we want to assign it a name using the assignment operator &lt;-. You can choose names arbitrarily (but the first character of a name cannot be a number). Just make sure they are descriptive and unique. Assigning the same name to two variables (e.g. vectors) will result in deletion of the first. Instead of converting a variable we can also create a new one and use an existing one as input. In this case we omit the as. and simply use the name of the type (e.g. factor()). There is a subtle difference between the two: When converting a variable, with e.g. as.factor(), we can only pass the variable we want to convert without additional arguments and R determines the factor levels by the existing unique values in the variable or just returns the variable itself if it is a factor already. When we specifically create a variable (just factor(), matrix(), etc.), we can and should set the options of this type explicitly. For a factor variable these could be the labels and levels, for a matrix the number of rows and columns and so on. #Numeric: top10_sales &lt;- c(163608, 126687, 120480, 110022, 108630, 95639, 94690, 89011, 87869, 85599) #Character: top10_product_names &lt;- c(&quot;Bio-Kaisersemmel&quot;, &quot;Laktosefreie Bio-Vollmilch&quot;, &quot;Ottakringer Helles&quot;, &quot;Milka Ganze Haselnüsse&quot;, &quot;Bio-Toastkäse Scheiben&quot;, &quot;Ottakringer Bio Zwickl&quot;, &quot;Vienna Coffee House Espresso&quot;, &quot;Bio-Mozzarella&quot;, &quot;Basmati Reis&quot;, &quot;Bananen&quot;) # Characters have to be put in &quot;&quot; #Factor variable with two categories: private_label &lt;- c(1,1,0,0,1,0,0,1,1,1) private_label &lt;- factor(private_label, levels = 0:1, labels = c(&quot;national brand&quot;, &quot;private label&quot;)) #Factor variable with more than two categories: top10_brand &lt;- c(&quot;Ja! Natürlich&quot;, &quot;Ja! Natürlich&quot;, &quot;Ottakringer&quot;, &quot;Milka&quot;, &quot;Ja! Natürlich&quot;, &quot;Ottakringer&quot;, &quot;Julius Meinl&quot;, &quot;Ja! Natürlich&quot;, &quot;Billa Bio&quot;, &quot;Clever&quot;) top10_brand &lt;- as.factor(top10_brand) #Date: date_most_sold &lt;- as.Date(c(&quot;2023-05-24&quot;, &quot;2023-06-23&quot;, &quot;2023-09-01&quot;, &quot;2023-06-30&quot;, &quot;2023-05-05&quot;, &quot;2023-06-09&quot;, &quot;2023-07-14&quot;, &quot;2023-06-16&quot;, &quot;2023-05-18&quot;, &quot;2023-05-19&quot;)) #Logical private_label_logical &lt;- c(TRUE,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,TRUE,TRUE) In order to “return” a vector we can now simply enter its name: top10_sales ## [1] 163608 126687 120480 110022 108630 95639 94690 89011 87869 85599 date_most_sold ## [1] &quot;2023-05-24&quot; &quot;2023-06-23&quot; &quot;2023-09-01&quot; &quot;2023-06-30&quot; &quot;2023-05-05&quot; ## [6] &quot;2023-06-09&quot; &quot;2023-07-14&quot; &quot;2023-06-16&quot; &quot;2023-05-18&quot; &quot;2023-05-19&quot; In order to check the type of a variable the class() function is used. class(date_most_sold) ## [1] &quot;Date&quot; 3.1.3 Data structures Now let’s create a table that contains the variables in columns and each observation in a row (like in SPSS or Excel). There are different data structures in R (e.g., Matrix, Vector, List, Array). In this course, we will mainly use data frames. Data frames are similar to matrices but are more flexible in the sense that they may contain different data types (e.g., numeric, character, etc.), where all values of vectors and matrices have to be of the same type (e.g. character). It is often more convenient to use characters instead of numbers (e.g. when indicating a persons sex: “F”, “M” instead of 1 for female , 2 for male). Thus we would like to combine both numeric and character values while retaining the respective desired features. This is where “data frames” come into play. Data frames can have different types of data in each column. For example, we can combine the vectors created above in one data frame using data.frame(). This creates a separate column for each vector, which is usually what we want (similar to SPSS or Excel). sales_data &lt;- data.frame(top10_sales, top10_product_names, private_label, top10_brand, date_most_sold, private_label_logical) 3.1.3.1 Accessing data in data frames When entering the name of a data frame, R returns the entire data frame: sales_data # Returns the entire data frame Hint: You may also use the View()-function to view the data in a table format (like in SPSS or Excel), i.e. enter the command View(data). Note that you can achieve the same by clicking on the small table icon next to the data frame in the “Environment”-window on the right in RStudio. Sometimes it is convenient to return only specific values instead of the entire data frame. There are a variety of ways to identify the elements of a data frame. One easy way is to explicitly state, which rows and columns you wish to view. The general form of the command is data.frame[rows,columns]. By leaving one of the arguments of data.frame[rows,columns] blank (e.g., data.frame[rows,]) we tell R that we want to access either all rows or columns, respectively. Note that a:b (where a and b are numbers and a &lt; b) is short hand notation for seq(from = a, to = b, by = 1). Here are some examples: sales_data[ , 2:4] # all rows and columns 2,3,4 sales_data[5:7, ] # rows 5,6,7 and all columns Typically we don’t want to remember which row or column number is needed but use names and conditions (e.g, all explicit tracks). In order to make that easier we will add more functions to R by installing a package (sometimes also referred to as “library”) called tidyverse. We only have to install it once (per computer) and subsequently we can add the functions the package provides by calling library(tidyverse). Typically library(PACKAGENAME) has to be called again whenever you restart R/RStudio. If you see the error message could not find function ... make sure you have loaded all the required packages. The tidyverse provides us with convenient tools to manipulate data. You may create subsets of the data frame, e.g., using mathematical expressions using the filter function: library(tidyverse) filter(sales_data, private_label == &quot;private label&quot;) # show only products that belong to private labels filter(sales_data, top10_sales &gt; 100000) # show only products that sold more than 100,000 units filter(sales_data, top10_product_names == &#39;Bio-Kaisersemmel&#39;) # returns all observations from product &quot;Bio-Kaisersemmel&quot; private_labels &lt;- filter(sales_data, private_label == &quot;private label&quot;) # creates a new data.frame by assigning only observations belonging to private labels You may also change the order of the rows in a data.frame by using the arrange()-function # Arrange by sales (descending: most - least) arrange(sales_data, desc(top10_sales)) You can order observations by several characteristics. Please note that the order of variables in the arrange()-function specifies the order of arranging the data set. For example, here we first arrange the observations by the brand of the product, and only then require ordering by sales amounts: # Arrange by brand (ascending by default) and then sales (descending: most - least) arrange(sales_data, top10_brand, desc(top10_sales)) 3.1.3.2 Inspecting the content of a data frame The head() function displays the first X elements/rows of a vector, matrix, table, data frame or function. head(sales_data, 3) # returns the first X rows (here, the first 3 rows) The tail() function is similar, except it displays the last elements/rows. tail(sales_data, 3) # returns the last X rows (here, the last 3 rows) names() returns the names of an R object. When, for example, it is called on a data frame, it returns the names of the columns. names(sales_data) # returns the names of the variables in the data frame ## [1] &quot;top10_sales&quot; &quot;top10_product_names&quot; &quot;private_label&quot; ## [4] &quot;top10_brand&quot; &quot;date_most_sold&quot; &quot;private_label_logical&quot; str() displays the internal structure of an R object. In the case of a data frame, it returns the class (e.g., numeric, factor, etc.) of each variable, as well as the number of observations and the number of variables. str(sales_data) # returns the structure of the data frame ## &#39;data.frame&#39;: 10 obs. of 6 variables: ## $ top10_sales : num 163608 126687 120480 110022 108630 ... ## $ top10_product_names : chr &quot;Bio-Kaisersemmel&quot; &quot;Laktosefreie Bio-Vollmilch&quot; &quot;Ottakringer Helles&quot; &quot;Milka Ganze Haselnüsse&quot; ... ## $ private_label : Factor w/ 2 levels &quot;national brand&quot;,..: 2 2 1 1 2 1 1 2 2 2 ## $ top10_brand : Factor w/ 6 levels &quot;Billa Bio&quot;,&quot;Clever&quot;,..: 3 3 6 5 3 6 4 3 1 2 ## $ date_most_sold : Date, format: &quot;2023-05-24&quot; &quot;2023-06-23&quot; ... ## $ private_label_logical: logi TRUE TRUE FALSE FALSE TRUE FALSE ... nrow() and ncol() return the rows and columns of a data frame or matrix, respectively. dim() displays the dimensions of an R object. nrow(sales_data) # returns the number of rows ## [1] 10 ncol(sales_data) # returns the number of columns ## [1] 6 dim(sales_data) # returns the dimensions of a data frame ## [1] 10 6 ls() can be used to list all objects that are associated with an R object. ls(sales_data) # list all objects associated with an object ## [1] &quot;date_most_sold&quot; &quot;private_label&quot; &quot;private_label_logical&quot; ## [4] &quot;top10_brand&quot; &quot;top10_product_names&quot; &quot;top10_sales&quot; 3.1.3.3 Select, group, append and delete variables to/from data frames To return a single column in a data frame, use the $ notation. For example, this returns all values associated with the variable “top10_track_streams”: sales_data$top10_sales ## [1] 163608 126687 120480 110022 108630 95639 94690 89011 87869 85599 If you want to select more than one variable you can use the select function. It takes the data.frame containing the data as its first argument and the variables that you need after it: select(sales_data, top10_product_names, top10_sales, private_label) select can also be used to remove columns by prepending a - to their name: select(sales_data, -date_most_sold, -private_label_logical) Assume that you wanted to add an additional variable to the data frame. You may use the $ notation to achieve this: # Create new variable as the log of sales sales_data$log_sales &lt;- log(sales_data$top10_sales) # Create an ascending count variable which might serve as an ID sales_data$obs_number &lt;- 1:nrow(sales_data) head(sales_data) In order to add a function (e.g., log) of multiple existing variables to the data.frame use mutate. Multiple commands can be chained using so called pipes - operators that can be read as “then”. Since R version 4.1 there are native pipes (|&gt;) as well as the ones provided by the tidyverse (%&gt;%): music_data_new &lt;- mutate(sales_data, sqrt_sales = sqrt(top10_sales), # &quot;%m&quot; extracts the month, format returns a character so we convert it to integer most_sales_month = as.integer(format(date_most_sold, &quot;%m&quot;)) ) %&gt;% select(top10_product_names, sqrt_sales, most_sales_month) Two other important functions of tidyverse help calculating important summary statistics, such as totals, averages, etc. By using group_by function, we can ask R to pay attention to group-specific observations (e.g., by brand, label, date, …) to then obtain values of interest by calling summarize: sales_total &lt;- sales_data %&gt;% group_by(top10_brand) %&gt;% summarize(total_sales = sum(top10_sales), avg_sales = mean(top10_sales)) head(sales_total) You can also rename variables in a data frame, e.g., using the rename()-function. In the following code “::” signifies that the function “rename” should be taken from the package “dplyr” (note: this package is part of the tidyverse). This can be useful if multiple packages have a function with the same name. Calling a function this way also means that you can access a function without loading the entire package via library(). sales_data &lt;- dplyr::rename(sales_data, brand = top10_brand) head(sales_data) Note that the same can be achieved using: names(sales_data)[names(sales_data)==&quot;brand&quot;] &lt;- &quot;top10_brand&quot; head(sales_data) Or by referring to the index of the variable: names(sales_data)[4] &lt;- &quot;brand&quot; head(sales_data) 3.2 Data import and export Before you can start your analysis in R, you first need to import the data you wish to perform the analysis on. You will often be faced with different types of data formats (usually produced by some other statistical software like SPSS or Excel or a text editor). Fortunately, R is fairly flexible with respect to the sources from which data may be imported and you can import the most common data formats into R with the help of a few packages. R can, among others, handle data from the following sources: In the previous chapter, we saw how we may use the keyboard to input data in R. In the following sections, we will learn how to import data from text files and other statistical software packages. 3.2.1 Getting data for this course Most of the data sets we will be working with in this course will be stored in text files (i.e., .dat, .txt, .csv). All data sets we will be working with are stored in a repository on GitHub (similar to other cloud storage services such as Dropbox). You can directly import these data sets from GitHub without having to copy data sets from one place to another. If you know the location, where the files are stored, you may conveniently load the data directly from GitHub into R using the read.csv() function. To figure out the structure of the data you can read the first couple of lines of a file using the readLines function. The header=TRUE argument in the read.csv function indicates that the first line of data represents the header, i.e., it contains the names of the columns. The sep=\";\"-argument specifies the delimiter (the character used to separate the columns), which is a “;” in this case. readLines(&quot;https://short.wu.ac.at/ma22_musicdata&quot;, n = 3) ## [1] &quot;\\&quot;isrc\\&quot;;\\&quot;artist_id\\&quot;;\\&quot;streams\\&quot;;\\&quot;weeks_in_charts\\&quot;;\\&quot;n_regions\\&quot;;\\&quot;danceability\\&quot;;\\&quot;energy\\&quot;;\\&quot;speechiness\\&quot;;\\&quot;instrumentalness\\&quot;;\\&quot;liveness\\&quot;;\\&quot;valence\\&quot;;\\&quot;tempo\\&quot;;\\&quot;song_length\\&quot;;\\&quot;song_age\\&quot;;\\&quot;explicit\\&quot;;\\&quot;n_playlists\\&quot;;\\&quot;sp_popularity\\&quot;;\\&quot;youtube_views\\&quot;;\\&quot;tiktok_counts\\&quot;;\\&quot;ins_followers_artist\\&quot;;\\&quot;monthly_listeners_artist\\&quot;;\\&quot;playlist_total_reach_artist\\&quot;;\\&quot;sp_fans_artist\\&quot;;\\&quot;shazam_counts\\&quot;;\\&quot;artistName\\&quot;;\\&quot;trackName\\&quot;;\\&quot;release_date\\&quot;;\\&quot;genre\\&quot;;\\&quot;label\\&quot;;\\&quot;top10\\&quot;;\\&quot;expert_rating\\&quot;&quot; ## [2] &quot;\\&quot;BRRGE1603547\\&quot;;3679;11944813;141;1;50,9;80,3;4;0,05;46,3;65,1;166,018;3,11865;228,285714285714;0;450;51;145030723;9740;29613108;4133393;24286416;3308630;73100;\\&quot;Luan Santana\\&quot;;\\&quot;Eu, VocÃª, O Mar e Ela\\&quot;;\\&quot;2016-06-20\\&quot;;\\&quot;other\\&quot;;\\&quot;Independent\\&quot;;1;\\&quot;excellent\\&quot;&quot; ## [3] &quot;\\&quot;USUM71808193\\&quot;;5239;8934097;51;21;35,3;75,5;73,3;0;39;43,7;191,153;3,228;144,285714285714;0;768;54;13188411;358700;3693566;18367363;143384531;465412;588550;\\&quot;Alessia Cara\\&quot;;\\&quot;Growing Pains\\&quot;;\\&quot;2018-06-14\\&quot;;\\&quot;Pop\\&quot;;\\&quot;Universal Music\\&quot;;0;\\&quot;good\\&quot;&quot; test_data &lt;- read.csv(&quot;https://short.wu.ac.at/ma22_musicdata&quot;, sep = &quot;;&quot;, header = TRUE) head(test_data) Note that it is also possible to download the data, placing it in the working directory and importing it from there. However, this requires an additional step to download the file manually first. If you chose this option, please remember to put the data file in the working directory first. If the import is not working, check your working directory setting using getwd(). Once you placed the file in the working directory, you can import it using the same command as above. Note that the file must be given as a character string (i.e., in quotation marks) and has to end with the file extension (e.g., .csv, .tsv, etc.). test_data &lt;- read.csv(&quot;data/music_data_fin.csv&quot;, header=TRUE, sep = &quot;;&quot;) head(test_data) 3.2.2 Export data Exporting to different formats is also easy, as you can just replace “read” with “write” in many of the previously discussed functions (e.g. write.csv(object, \"file_name\")). This will save the data file to the working directory. To check what the current working directory is you can use getwd(). By default, the write.csv(object, \"file_name\")function includes the row number as the first variable. By specifying row.names = FALSE, you may exclude this variable since it doesn’t contain any useful information. write.csv(music_data, &quot;musicData.csv&quot;, row.names = FALSE) #writes to a comma-separated value file write_sav(music_data, &quot;musicData.sav&quot;) Learning check (LC3.1) Which of the following are data types are recognized by R? Factor Date Decimal Vector None of the above (LC3.2) What function should you use to check if an object is a data frame? type() str() class() object.type() None of the above (LC3.3) You would like to combine three vectors (student, grade, date) in a data frame. What would happen when executing the following code? student &lt;- c(&quot;Max&quot;, &quot;Jonas&quot;, &quot;Saskia&quot;, &quot;Victoria&quot;) grade &lt;- c(3, 2, 1, 2) date &lt;- as.Date(c(&quot;2020-10-06&quot;, &quot;2020-10-08&quot;, &quot;2020-10-09&quot;)) df &lt;- data.frame(student, grade, date) Error because a data frame can not have different data types Error because you should use as.data.frame() instead of data.frame() Error because all vectors need to have the same length Error because the column names are not specified This code should not report an error You would like to analyze the following data frame (LC3.4) How can you obtain Christina’s grade from the data frame? df[4,2] df[2,4] filter(df, student = Christina) %&gt;% select(grade) filter(df, student == \"Christina\") %&gt;% select(grade) None of the above (LC3.5) How can you add a new variable ‘student_id’ to the data frame that assigns numbers to students in an ascending order? df$student_id &lt;- 1:nrow(df) df&amp;student_id &lt;- 1:nrow(df) mutate(df, student_id = 1:nrow(df)) mutate(df, student_id = 1:length(df)) None of the above (LC3.6) How could you obtain all rows with students who obtained a 1? filter(df, grade == 1) filter(df, grade == min(df$grade, na.rm = TRUE)) select(df, grade == 1) filter(df, grade == min(df$grade)) None of the above (LC3.7) How could you create a subset of observations where the grade is not missing (NA) df_subset &lt;- filter(df, !is.na(grade)) df_subset &lt;- filter(df, isnot.na(grade)) df_subset &lt;- filter(df, grade != NA) df_subset &lt;- filter(df, grade != \"NA\") None of the above (LC3.8) What is the share of students with a grade better than 3? filter(df, grade &lt; 3)/nrow(df) nrow(filter(df, grade &lt; 3))/length(df) nrow(filter(df, grade &lt; 3))/nrow(df) filter(df, grade &lt; 3)/length(df) None of the above (LC3.9) You would like to load a .csv file from your working directory. What function would you use do it? read.table(file_name.csv) load.csv(\"file.csv\") read.table(\"file.csv\") get.table(file_name.csv) None of the above (LC3.10) After you loaded the file, you would like to inspect the types of data contained in it. How would you do it? ncol(df) nrow(df) dim(df) str(df) None of the above "],["summarizing-data.html", "4 Summarizing data 4.1 Summary statistics 4.2 Data visualization Learning check References", " 4 Summarizing data 4.1 Summary statistics This section discusses how to produce and analyze basic summary statistics. Summary statistics are often used to describe variables in terms of 1) the central tendency of the frequency distribution, and 2) the dispersion of values. A measure of central tendency is a single value that attempts to describe the data by identifying the central position within the data. There are various measures of central tendency as the following table shows. Statistic Description Definition Mean The average value when you sum up all elements and divide by the number of elements \\(\\bar{X}=\\frac{\\sum_{i=1}^{n}{X_i}}{n}\\) Mode The value that occurs most frequently (i.e., the highest peak of the frequency distribution) Median The middle value when the data are arranged in ascending or descending order (i.e., the 50th percentile) The dispersion refers to the degree to which the data is distributed around the central tendency and can be described in terms of the range, interquartile range, variance, and standard deviation. Statistic Description Definition Range The difference between the largest and smallest values in the sample \\(Range=X_{largest}-X_{smallest}\\) Interquartile range The range of the middle 50% of scores \\(IQR=Q_3-Q_1\\) Variance The mean squared deviation of all the values of the mean \\(s^2=\\frac{1}{n-1}*\\sum_{i=1}^{n}{(X_i-\\bar{X})^2}\\) Standard deviation The square root of the variance \\(s_x=\\sqrt{s^2}\\) The answer to the question which measures to use depends on the level of measurement. Based on the discussion in chapter 1, we make a distinction between categorical and continuous variables, for which different statistics are permissible as summarized in the following table. OK to compute… Nominal Ordinal Interval Ratio frequency distribution Yes Yes Yes Yes median and percentiles No Yes Yes Yes mean, standard deviation, standard error of the mean No No Yes Yes ratio, or coefficient of variation No No No Yes As an example data set, we will be using a data set containing music streaming data from a popular streaming service. Let’s load and inspect the data first. # read.csv2 is shorthand for read.csv(file, sep = &quot;;&quot;) music_data &lt;- read.csv2(&quot;https://short.wu.ac.at/ma22_musicdata&quot;) dim(music_data) ## [1] 66796 31 head(music_data) names(music_data) ## [1] &quot;isrc&quot; &quot;artist_id&quot; ## [3] &quot;streams&quot; &quot;weeks_in_charts&quot; ## [5] &quot;n_regions&quot; &quot;danceability&quot; ## [7] &quot;energy&quot; &quot;speechiness&quot; ## [9] &quot;instrumentalness&quot; &quot;liveness&quot; ## [11] &quot;valence&quot; &quot;tempo&quot; ## [13] &quot;song_length&quot; &quot;song_age&quot; ## [15] &quot;explicit&quot; &quot;n_playlists&quot; ## [17] &quot;sp_popularity&quot; &quot;youtube_views&quot; ## [19] &quot;tiktok_counts&quot; &quot;ins_followers_artist&quot; ## [21] &quot;monthly_listeners_artist&quot; &quot;playlist_total_reach_artist&quot; ## [23] &quot;sp_fans_artist&quot; &quot;shazam_counts&quot; ## [25] &quot;artistName&quot; &quot;trackName&quot; ## [27] &quot;release_date&quot; &quot;genre&quot; ## [29] &quot;label&quot; &quot;top10&quot; ## [31] &quot;expert_rating&quot; The data set contains information about all songs that appeared in the Top200 charts of a popular streaming service between 2017 and 2020. The dim()-function returns the dimensions of the data frame (i.e., the number of rows and columns). As can be seen, the data set comprises information for 66,796 songs and 31 variables. The variables in the data set are: isrc: unique song id artist_id: unique artist ID streams: the number of streams of the song received globally between 2017-2021 weeks_in_charts: the number of weeks the song was in the top200 charts in this period n_regions: the number of markets where the song appeared in the top200 charts audio features, see: (see: https://developer.spotify.com/documentation/web-api/reference/*category-tracks) danceability energy speechiness instrumentalness liveness valence tempo song_length: the duration of the song (in minutes) song_age: the age of the song (in weeks since release) explicit: indicator for explicit lyrics n_playlists: number of playlists a song is featured on sp_popularity: the Spotify popularity index of an artist youtube_views: the number of views the song received on YouTube tiktok_counts: the number of Tiktok views the song received on TikTok ins_followers_artist: the number of Instagram followers of the artist monthly_listeners_artist: the number of monthly listeners of an artist playlist_total_reach_artist: the number of playlist followers of the playlists the song is on sp_fans_artist: the number of fans of the artist on Spotify shazam_counts: the number of times a song is shazamed artistName: name of the artist trackName: name of the song release_date: release date of song genre: genre associated with the song label: music label associated with the song top10: indicator whether the song was in the top 10 expert_rating: 5-scale rating by a music expert (poor, fair, good, excellent, masterpiece) In a first step, we need to make sure all variables are in the correct format, according to these variable definitions: library(tidyverse) music_data &lt;- music_data |&gt; # pipe music data into mutate mutate(release_date = as.Date(release_date), # convert to date explicit = factor(explicit, levels = 0:1, labels = c(&quot;not explicit&quot;, &quot;explicit&quot;)), # convert to factor w. new labels label = as.factor(label), # convert to factor with values as labels genre = as.factor(genre), top10 = as.logical(top10), # Create an ordered factor for the ratings (e.g., for arranging the data) expert_rating = factor(expert_rating, levels = c(&quot;poor&quot;, &quot;fair&quot;, &quot;good&quot;, &quot;excellent&quot;, &quot;masterpiece&quot;), ordered = TRUE) ) head(music_data) In the following sections, we will inspect the data in more detail. 4.1.1 Categorical variables Categorical variables contain a finite number of categories or distinct groups and are also known as qualitative or non-metric variables. There are different types of categorical variables: Nominal variables: variables that have two or more categories but no logical order (e.g., music genres). A dichotomous variable (also referred to as dummy variable or binary variable) is simply a nominal variable that only has two categories (e.g., indicator for explicit lyrics). Ordinal variables: variables that have two or more categories that can also be ordered or ranked (e.g., expert ratings). Let’s now start to investigate the nominal variables in our data set (i.e., explicit, genre, label). As the table above shows, the only permissible operation with nominal variables is counting. That is, we can inspect the frequency distribution, which tells us how many observations we have per category. The table() function creates a frequency table that counts how many observations we have in each category. table(music_data$genre) #absolute frequencies ## ## Classics/Jazz Country Electro/Dance German Folk HipHop/Rap ## 80 504 2703 539 21131 ## other Pop R&amp;B Reggae Rock ## 5228 30069 1881 121 4214 ## Soundtrack ## 326 table(music_data$label) #absolute frequencies ## ## Independent Sony Music Universal Music Warner Music ## 22570 12390 21632 10204 table(music_data$explicit) #absolute frequencies ## ## not explicit explicit ## 58603 8193 The numbers associated with the factor level in the output tell you, how many observations there are per category. For example, there are 21131 songs from the HipHop &amp; Rap genre. Often, we are interested in the relative frequencies, which can be obtained by using the prop.table() function. prop.table(table(music_data$genre)) #relative frequencies ## ## Classics/Jazz Country Electro/Dance German Folk HipHop/Rap ## 0.001197677 0.007545362 0.040466495 0.008069345 0.316351279 ## other Pop R&amp;B Reggae Rock ## 0.078268160 0.450161686 0.028160369 0.001811486 0.063087610 ## Soundtrack ## 0.004880532 prop.table(table(music_data$label)) #relative frequencies ## ## Independent Sony Music Universal Music Warner Music ## 0.3378945 0.1854901 0.3238517 0.1527636 prop.table(table(music_data$explicit)) #relative frequencies ## ## not explicit explicit ## 0.877343 0.122657 Now the output gives you the relative frequencies. For example, the market share of Warner Music in the Top200 charts is ~15.3%, ~6.3% of songs are from the Rock genre, and ~12.3% of the songs have explicit lyrics. Note that the above output shows the overall relative frequencies. In many cases, it is meaningful to consider conditional relative frequencies. This can be achieved by adding a ,1 to the prop.table() command, which tells R to compute the relative frequencies by row (which is in our case the genre variable). The following code can be used to show the relative frequency of songs with explicit lyrics by genre. prop.table(table(select(music_data, genre, explicit)),1) #conditional relative frequencies ## explicit ## genre not explicit explicit ## Classics/Jazz 0.82500000 0.17500000 ## Country 0.98015873 0.01984127 ## Electro/Dance 0.66000740 0.33999260 ## German Folk 0.70129870 0.29870130 ## HipHop/Rap 0.94846434 0.05153566 ## other 0.92214996 0.07785004 ## Pop 0.84472380 0.15527620 ## R&amp;B 0.92238171 0.07761829 ## Reggae 0.90909091 0.09090909 ## Rock 0.82819174 0.17180826 ## Soundtrack 0.86809816 0.13190184 As can be seen, the presence of explicit lyrics greatly varies across genres. While in the Electro/Dance genre ~34% of songs have explicit lyrics, in the Country genre, this share is only ~2%. The ‘expert_rating’ variable is an example of an ordinal variable. Although we can now rank order the songs with respect to their rating, this variable doesn’t contain information about the distance between two songs. To get a measure of central tendency, we could, for example, compute the median of this variable using the quantile()-function (recall that the 50th percentile is the median). For ordinal factors we also have to set the algorithm that calculates the percentiles to type=1 (see ?quantile for more details). median_rating &lt;- quantile(music_data$expert_rating, 0.5, type = 1) median_rating ## 50% ## good ## Levels: poor &lt; fair &lt; good &lt; excellent &lt; masterpiece This means that the “middle” value when the data are arranged is expert rating “good” (median = 50th percentile). Note that you could also compute other percentiles using the quanile()-function. For example, to get the median and the interquartile range, we could compute the 25th, 50th, and 75th percentile. quantile(music_data$expert_rating,c(0.25,0.5,0.75), type = 1) ## 25% 50% 75% ## fair good excellent ## Levels: poor &lt; fair &lt; good &lt; excellent &lt; masterpiece This means that the interquartile range is between “fair” and “excellent”. If you wanted to compare different genres according to these statistics, you could do this using the group_by()-function as follows: percentiles &lt;- c(0.25, 0.5, 0.75) rating_percentiles &lt;- music_data |&gt; group_by(explicit) |&gt; summarize( percentile = percentiles, value = quantile(expert_rating, percentiles, type = 1)) rating_percentiles In this case, we don’t observe any differences in the first, second, or third quantile of expert ratings between explicit and non-explicit songs. 4.1.2 Continuous variables 4.1.2.1 Descriptive statistics Continuous variables (also know as metric variables) are numeric variables that can take on any value on a measurement scale (i.e., there is an infinite number of values between any two values). There are different types of continuous variables as we have seen in chapter 1: Interval variables: while the zero point is arbitrary, equal intervals on the scale represent equal differences in the property being measured. E.g., on a temperature scale measured in Celsius the difference between a temperature of 15 degrees and 25 degrees is the same difference as between 25 degrees and 35 degrees but the zero point is arbitrary (there are different scales to measure temperature, such as Fahrenheit or Celsius, and zero in this case doesn’t indicate the absence of temperature). Ratio variables: has all the properties of an interval variable, but also has an absolute zero point. When the variable equals 0.0, it means that there is none of that variable (e.g., the number of streams or duration variables in our example). For interval and ratio variables we can also compute the mean as a measure of central tendency, as well as the variance and the standard deviation as measures of dispersion. Computing descriptive statistics for continuous variables is easy and there are many functions from different packages that let you calculate summary statistics (including the summary() function from the base package). In this tutorial, we will use the describe() function from the psych package. Note that you could just as well use other packages to compute the descriptive statistics (e.g., the stat.desc() function from the pastecs package). Which one you choose depends on what type of information you seek (the results provide slightly different information) and on personal preferences. We could, for example, compute the summary statistics for the variables “streams”, “danceability”, and “valence” in our data set as follows: library(psych) psych::describe(select(music_data, streams, danceability, valence)) ## vars n mean sd median trimmed mad ## streams 1 66796 7314673.94 39956263.68 333335.5 1309559.27 471342.26 ## danceability 2 66796 66.32 14.71 68.0 67.15 14.83 ## valence 3 66796 50.42 22.26 50.0 50.16 25.35 ## min max range skew kurtosis se ## streams 1003 2165692479.0 2165691476.0 16.74 452.05 154600.05 ## danceability 0 98.3 98.3 -0.53 0.03 0.06 ## valence 0 99.0 99.0 0.08 -0.84 0.09 You can see that the output contains measures of central tendency (e.g., the mean) and dispersion (e.g., sd) for the selected variables. It can be seen, for example, that the mean of the streams variable is 7,314,674 while the median is 333,336. This already tells us something about the distribution of the data. Because the mean is substantially higher than the median, we can conclude that there are a few songs with many streams, resulting in a right skew of the distribution. The median as a measure of central tendency is generally less susceptible to outliers. In the above command, we used the psych:: prefix to avoid confusion and to make sure that R uses the describe() function from the psych package since there are many other packages that also contain a desribe() function. Note that you could also compute these statistics separately by using the respective functions (e.g., mean(), sd(), median(), min(), max(), etc.). There are many options for additional statistics for this function. For example, you could add the argument IQR = TRUE to add the interquartile range to the output. The psych package also contains the describeBy() function, which lets you compute the summary statistics by sub-groups separately. For example, we could compute the summary statistics by genre as follows: describeBy(select(music_data, streams, danceability, valence), music_data$genre,skew = FALSE, range = FALSE) ## ## Descriptive statistics by group ## group: Classics/Jazz ## vars n mean sd se ## streams 1 80 735685.05 2590987.28 289681.18 ## danceability 2 80 46.00 18.34 2.05 ## valence 3 80 38.24 24.30 2.72 ## ------------------------------------------------------------ ## group: Country ## vars n mean sd se ## streams 1 504 15029908.45 43603853.23 1942269.99 ## danceability 2 504 59.67 11.98 0.53 ## valence 3 504 58.90 21.08 0.94 ## ------------------------------------------------------------ ## group: Electro/Dance ## vars n mean sd se ## streams 1 2703 12510460.33 56027266.04 1077646.71 ## danceability 2 2703 66.55 11.87 0.23 ## valence 3 2703 47.50 21.49 0.41 ## ------------------------------------------------------------ ## group: German Folk ## vars n mean sd se ## streams 1 539 2823274.57 10037803.62 432358.81 ## danceability 2 539 63.03 15.36 0.66 ## valence 3 539 56.07 24.07 1.04 ## ------------------------------------------------------------ ## group: HipHop/Rap ## vars n mean sd se ## streams 1 21131 6772815.16 37100200.64 255220.90 ## danceability 2 21131 73.05 12.30 0.08 ## valence 3 21131 49.04 20.73 0.14 ## ------------------------------------------------------------ ## group: other ## vars n mean sd se ## streams 1 5228 12615232.06 38126472.04 527301.29 ## danceability 2 5228 64.53 15.39 0.21 ## valence 3 5228 60.16 22.73 0.31 ## ------------------------------------------------------------ ## group: Pop ## vars n mean sd se ## streams 1 30069 5777165.76 36618010.00 211171.47 ## danceability 2 30069 63.74 14.46 0.08 ## valence 3 30069 50.33 22.57 0.13 ## ------------------------------------------------------------ ## group: R&amp;B ## vars n mean sd se ## streams 1 1881 15334008.40 54013527.81 1245397.95 ## danceability 2 1881 67.97 13.43 0.31 ## valence 3 1881 52.83 23.01 0.53 ## ------------------------------------------------------------ ## group: Reggae ## vars n mean sd se ## streams 1 121 6413030.64 20384605.84 1853145.99 ## danceability 2 121 75.06 9.33 0.85 ## valence 3 121 69.73 18.38 1.67 ## ------------------------------------------------------------ ## group: Rock ## vars n mean sd se ## streams 1 4214 6902054.06 54383538.37 837761.11 ## danceability 2 4214 54.75 13.98 0.22 ## valence 3 4214 45.65 22.53 0.35 ## ------------------------------------------------------------ ## group: Soundtrack ## vars n mean sd se ## streams 1 326 12676756.22 71865892.69 3980283.67 ## danceability 2 326 52.82 16.25 0.90 ## valence 3 326 37.99 22.44 1.24 In this example, we used the arguments skew = FALSE and range = FALSE to exclude some statistics from the output. R is open to user contributions and various users have contributed packages that aim at making it easier for researchers to summarize statistics. For example, the summarytools package can be used to summarize the variables. If you would like to use this package and you are a Mac user, you may need to also install XQuartz (X11) too. To do this, go to this page and download the XQuartz-2.7.7.dmg, then open the downloaded folder and click XQuartz.pkg and follow the instruction on screen and install XQuartz. If you still encouter an error after installing XQuartz, you may find a solution &lt;a href=“href=”https://www.xquartz.org/” target=“_blank”&gt;here. img { background-color: transparent; border: 0; } .st-table td, .st-table th { padding: 8px; } .st-table > thead > tr { background-color: #eeeeee; } .st-cross-table td { text-align: center; } .st-descr-table td { text-align: right; } table.st-table th { text-align: center; } table.st-table > thead > tr { background-color: #eeeeee; } table.st-table td span { display: block; } table.st-table > tfoot > tr > td { border:none; } .st-container { width: 100%; padding-right: 15px; padding-left: 15px; margin-right: auto; margin-left: auto; margin-top: 15px; } .st-multiline { white-space: pre; } .st-table { width: auto; table-layout: auto; margin-top: 20px; margin-bottom: 20px; max-width: 100%; background-color: transparent; border-collapse: collapse; } .st-table > thead > tr > th, .st-table > tbody > tr > th, .st-table > tfoot > tr > th, .st-table > thead > tr > td, .st-table > tbody > tr > td, .st-table > tfoot > tr > td { vertical-align: middle; } .st-table-bordered { border: 1px solid #bbbbbb; } .st-table-bordered > thead > tr > th, .st-table-bordered > tbody > tr > th, .st-table-bordered > thead > tr > td, .st-table-bordered > tbody > tr > td { border: 1px solid #cccccc; } .st-table-bordered > thead > tr > th, .st-table-bordered > thead > tr > td, .st-table thead > tr > th { border-bottom: none; } .st-freq-table > thead > tr > th, .st-freq-table > tbody > tr > th, .st-freq-table > tfoot > tr > th, .st-freq-table > thead > tr > td, .st-freq-table > tbody > tr > td, .st-freq-table > tfoot > tr > td, .st-freq-table-nomiss > thead > tr > th, .st-freq-table-nomiss > tbody > tr > th, .st-freq-table-nomiss > tfoot > tr > th, .st-freq-table-nomiss > thead > tr > td, .st-freq-table-nomiss > tbody > tr > td, .st-freq-table-nomiss > tfoot > tr > td, .st-cross-table > thead > tr > th, .st-cross-table > tbody > tr > th, .st-cross-table > tfoot > tr > th, .st-cross-table > thead > tr > td, .st-cross-table > tbody > tr > td, .st-cross-table > tfoot > tr > td { padding-left: 20px; padding-right: 20px; } .st-table-bordered > thead > tr > th, .st-table-bordered > tbody > tr > th, .st-table-bordered > thead > tr > td, .st-table-bordered > tbody > tr > td { border: 1px solid #cccccc; } .st-table-striped > tbody > tr:nth-of-type(odd) { background-color: #ffffff; } .st-table-striped > tbody > tr:nth-of-type(even) { background-color: #f9f9f9; } .st-descr-table > thead > tr > th, .st-descr-table > tbody > tr > th, .st-descr-table > thead > tr > td, .st-descr-table > tbody > tr > td { padding-left: 24px; padding-right: 24px; word-wrap: break-word; } .st-freq-table, .st-freq-table-nomiss, .st-cross-table { border: medium none; } .st-freq-table > thead > tr:nth-child(1) > th:nth-child(1), .st-cross-table > thead > tr:nth-child(1) > th:nth-child(1), .st-cross-table > thead > tr:nth-child(1) > th:nth-child(3) { border: none; background-color: #ffffff; text-align: center; } .st-protect-top-border { border-top: 1px solid #cccccc !important; } .st-ws-char { display: inline; color: #999999; letter-spacing: 0.2em; } /* Optional classes */ .st-small { font-size: 13px; } .st-small td, .st-small th { padding: 8px; } .st-small > thead > tr > th, .st-small > tbody > tr > th, .st-small > thead > tr > td, .st-small > tbody > tr > td { padding-left: 12px; padding-right: 12px; } library(summarytools) print(dfSummary(select(music_data, streams, valence, genre, label, explicit), plain.ascii = FALSE, style = &quot;grid&quot;,valid.col = FALSE, tmp.img.dir = &quot;tmp&quot;, graph.magnif = .65), method = &#39;render&#39;,headings = FALSE,footnote= NA) No Variable Stats / Values Freqs (% of Valid) Graph Missing 1 streams [numeric] Mean (sd) : 7314674 (39956264)min &le; med &le; max:1003 &le; 333335.5 &le; 2165692479IQR (CV) : 2125326 (5.5) 63462 distinct values 0 (0.0%) 2 valence [numeric] Mean (sd) : 50.4 (22.3)min &le; med &le; max:0 &le; 50 &le; 99IQR (CV) : 34.2 (0.4) 1420 distinct values 0 (0.0%) 3 genre [factor] 1. Classics/Jazz2. Country3. Electro/Dance4. German Folk5. HipHop/Rap6. other7. Pop8. R&B9. Reggae10. Rock11. Soundtrack 80(0.1%)504(0.8%)2703(4.0%)539(0.8%)21131(31.6%)5228(7.8%)30069(45.0%)1881(2.8%)121(0.2%)4214(6.3%)326(0.5%) 0 (0.0%) 4 label [factor] 1. Independent2. Sony Music3. Universal Music4. Warner Music 22570(33.8%)12390(18.5%)21632(32.4%)10204(15.3%) 0 (0.0%) 5 explicit [factor] 1. not explicit2. explicit 58603(87.7%)8193(12.3%) 0 (0.0%) The ‘Missing’ column in the output above gives us information about missing values. It this case, there are no missing values; however, in reality there are usually at least a couple of lost or not recorded values. To get more precise analysis results, we might want to exclude these observations by creating a “complete” subset of our data. Imagine that we have a missing value in the variable “valence”; we would create a subset by filtering that hypothetical observation out: music_data_valence &lt;- filter(music_data, !is.na(valence)) In the command above, !is.na() is used to filter the rows for observations where the respective variable does not have missing values. The “!” in this case translates to “is not” and the function is.na() checks for missing values. Hence, the entire statement can be read as “select the rows from the ‘music_data’ data set where the values of the ‘valence’ and ‘duration_ms’ variables are not missing”. As you can see, the output also includes a visualization of the frequency distribution using a histogram for the continuous variables and a bar chart for the categorical variables. The frequency distribution is an important element that allows us to assign probabilities to observed values if the observations come from a known probability distribution. How to derive these probability statements will be discussed next. 4.1.2.2 Using frequency distributions to go beyond the data The frequency distribution can be used to make statements about the probability that a certain observed value will occur if the observations come from a known probability distribution. For normally distributed data, the following table can be used to look up the probability that a certain value will occur. For example, the value of -1.96 has a probability of 2.5% (i.e., .0250). Figure 1.14: Standard normal table There are two things worth noting. First, the normal distribution has two tails as the following figure shows and we need to take the probability mass at each side of the distribution into account. Hence, there is a 2.5% probability of observing a value of -1.96 or smaller and a 2.5% of observing a value of 1.96 or larger. Hence, the probability mass within this interval is 0.95. Figure 1.15: Standard normal distribution The second point is related to the scale of the distribution. Since the variables that we will collect can be measured at many different scales (e.g., number of streams, duration in milliseconds), we need a way to convert the scale into a standardized measure that would allow us to compare the observations against the values from the probability table. The standardized variate, or z-score, allows us to do exactly that. It is computed as follows: \\[\\begin{align} Z=\\frac{X_i-\\bar{X}}{s} \\end{align} \\] By subtracting the mean of the variable from each observation and dividing by the standard deviation, the data is converted to a scale with mean = 0 and SD = 1, so we can use the tables of probabilities for the normal distribution to see how likely it is that a particular score will occur in the data. In other words, the z-score tells us how many standard deviations above or below the mean a particular x-value is. To see how this works in practice, let’s inspect the distribution of the ‘tempo’ variable from the music data set, which is defined as the overall estimated tempo of a track in beats per minute (BPM). The hist()-function can be used to draw the corresponding histogram. hist(music_data$tempo) Figure 1.16: Histogram of tempo variable In this case, the variable is measured on the scale “beats per minute”. To standardize this variable, we will subtract the mean of this variable from each observation and then divide by the standard deviation. We can compute the standardized variable by hand as follows: music_data$tempo_std &lt;- (music_data$tempo - mean(music_data$tempo))/sd(music_data$tempo) If we create the histogram again, we can see that the scale has changed and now we can compare the standardized values to the values we find in the probability table. hist(music_data$tempo_std) Figure 1.18: Histogram of standardized tempo variable Note that you could have also used the scale()-function instead of computing the z-scores manually, which leads to the same result: music_data$tempo_std &lt;- scale(music_data$tempo) Instead of manually comparing the observed values to the values in the table, it is much easier to use the in-built functions to obtain the probabilities. The pnorm()-function gives the probability of obtaining values lower than the indicated values (i.e., the probability mass left of that value). For the value of 1.96, this probability mass is ~0.025, in line with the table above. pnorm(-1.96) ## [1] 0.0249979 To also take the other end of the distribution into consideration, we would need to multiply this value by to. This way, we arrive at a value of 5%. pnorm(-1.96)*2 ## [1] 0.04999579 Regarding the standard normal distribution, it is helpful to remember the following numbers, indicating the points on the standard normal distribution, where the sum of the probability mass to the left at the lower end and to the right of the upper end exceed a certain threshold: +/-1.645 - 10% of probability mass outside this region +/-1.960 - 5% of probability mass outside this region +/-2.580 - 1% of probability mass outside this region Going back to our example, we could also ask: what is the probability of obtaining the minimum (or maximum) observed value in our data? The minimum value on the standardized scale is: min(music_data$tempo_std) ## [1] -4.253899 And the associated probability is: pnorm(min(music_data$tempo_std))*2 ## [1] 0.00002100803 Although the probability of observing this minimum value is very low, there are very few observations in the extreme regions at each end of the histogram, so this doesn’t seem too unusual. As a rule of thumb, you can remember that 68% of the observations of a normally distributed variable should be within 1 standard deviation of the mean, 95% within 2 standard deviations, and 99.7% within 3 standard deviations. This is also shown in the following plot: Figure 1.24: The 68, 95, 99.7 rule (source: Wikipedia) In case of our ‘tempo’ variable, we do not observe values that are more than 3 standard deviations away from the mean. In other instances, checking the standardized values of a variable may help you to identify outliers. For example, if you conducted a survey and you would like to exclude respondents who answered the survey too fast, you may exclude cases with a low probability based on the distribution of the duration variable. 4.2 Data visualization This section discusses the important topic of data visualization and how to produce appropriate graphics to describe your data visually. You should always visualize your data first. Figure 1.2: source: https://twitter.com/heyblake/status/1432070055949258752?s=20 The plots we created in the previous chapters used R’s in-built functions. In this section, we will be using the ggplot2 package by Hadley Wickham. It has the advantage of being fairly straightforward to learn and being very flexible when it comes to building more complex plots. For a more in depth discussion you can refer to chapter 4 of the book “Discovering Statistics Using R” by Andy Field et al. or read the following chapter from the book “R for Data science” by Hadley Wickham as well as “R Graphics Cookbook” by Winston Chang. ggplot2 is built around the idea of constructing plots by stacking layers on top of one another. Every plot starts with the ggplot(data) function, after which layers can be added with the “+” symbol. The following figures show the layered structure of creating plots with ggplot.     4.2.1 Categorical variables 4.2.1.1 Bar plot To give you an example of how the graphics are composed, let’s go back to the frequency table from the previous chapter, where we created a table showing the relative frequencies of songs in the Austrian streaming charts by genre. library(tidyverse) music_data &lt;- read.csv2(&quot;https://short.wu.ac.at/ma22_musicdata&quot;) |&gt; # pipe music data into mutate mutate(release_date = as.Date(release_date), # convert to date explicit = factor(explicit, levels = 0:1, labels = c(&quot;not explicit&quot;, &quot;explicit&quot;)), # convert to factor w. new labels label = as.factor(label), # convert to factor with values as labels genre = as.factor(genre), top10 = as.logical(top10), # Create an ordered factor for the ratings (e.g., for arranging the data) expert_rating = factor(expert_rating, levels = c(&quot;poor&quot;, &quot;fair&quot;, &quot;good&quot;, &quot;excellent&quot;, &quot;masterpiece&quot;), ordered = TRUE) ) |&gt; filter(!is.na(valence)) head(music_data) How can we plot this kind of data? Since we have a categorical variable, we will use a bar plot. However, to be able to use the table for your plot, you first need to assign it to an object as a data frame using the as.data.frame()-function. table_plot_rel &lt;- as.data.frame(prop.table(table(music_data$genre))) #relative frequencies head(table_plot_rel) Since Var1 is not a very descriptive name, let’s rename the variable to something more meaningful table_plot_rel &lt;- rename(table_plot_rel, Genre = Var1) head(table_plot_rel) Once we have our data set we can begin constructing the plot. As mentioned previously, we start with the ggplot() function, with the argument specifying the data set to be used. Within the function, we further specify the scales to be used using the aesthetics argument, specifying which variable should be plotted on which axis. In our example, we would like to plot the categories on the x-axis (horizontal axis) and the relative frequencies on the y-axis (vertical axis). bar_chart &lt;- ggplot(table_plot_rel, aes(x = Genre,y = Freq)) bar_chart Figure 1.5: Bar chart (step 1) You can see that the coordinate system is empty. This is because so far, we have told R only which variables we would like to plot but we haven’t specified which geometric figures (points, bars, lines, etc.) we would like to use. This is done using the geom_xxx() function. ggplot includes many different geoms, for a wide range of plots (e.g., geom_line, geom_histogram, geom_boxplot, etc.). A good overview of the various geom functions can be found here. In our case, we would like to use a bar chart for which geom_col is appropriate. bar_chart + geom_col() Figure 1.6: Bar chart (step 2) Now we have specified the data, the scales and the shape. Specifying this information is essential for plotting data using ggplot. Everything that follows now just serves the purpose of making the plot look nicer by modifying the appearance of the plot. How about some more meaningful axis labels? We can specify the axis labels using the ylab() and xlab() functions: bar_chart + geom_col() + ylab(&quot;Relative frequency&quot;) + xlab(&quot;Genre&quot;) Figure 1.7: Bar chart (step 3) How about adding some value labels to the bars? This can be done using geom_text(). Note that the sprintf() function is not mandatory and is only added to format the numeric labels here. The function takes two arguments: the first specifies the format wrapped in two % signs. Thus, %.0f means to format the value as a fixed point value with no digits after the decimal point, and %% is a literal that prints a “%” sign. The second argument is simply the numeric value to be used. In this case, the relative frequencies multiplied by 100 to obtain the percentage values. Using the vjust = argument, we can adjust the vertical alignment of the label. In this case, we would like to display the label slightly above the bars. bar_chart + geom_col() + ylab(&quot;Relative frequency&quot;) + xlab(&quot;Genre&quot;) + geom_text(aes(label = sprintf(&quot;%.0f%%&quot;, Freq * 100)), vjust=-0.2) Figure 1.8: Bar chart (step 4) We could go ahead and specify the appearance of every single element of the plot now. However, there are also pre-specified themes that include various formatting steps in one singe function. For example theme_bw() would make the plot appear like this: bar_chart + geom_col() + ylab(&quot;Relative frequency&quot;) + xlab(&quot;Genre&quot;) + geom_text(aes(label = sprintf(&quot;%.1f%%&quot;, Freq/sum(Freq) * 100)), vjust=-0.2) + theme_bw() Figure 1.9: Bar chart (step 5) and theme_minimal() looks like this: bar_chart + geom_col() + ylab(&quot;Relative frequency&quot;) + xlab(&quot;Genre&quot;) + geom_text(aes(label = sprintf(&quot;%.1f%%&quot;, Freq/sum(Freq) * 100)), vjust=-0.2) + theme_minimal() Figure 1.10: Bar chart (options 1) In a next step, let’s prevent the axis labels from overlapping by rotating the labels. bar_chart + geom_col() + ylab(&quot;Relative frequency&quot;) + xlab(&quot;Genre&quot;) + geom_text(aes(label = sprintf(&quot;%.1f%%&quot;, Freq/sum(Freq) * 100)), vjust=-0.2) + theme_minimal() + theme(axis.text.x = element_text(angle=45,vjust=0.75)) Figure 1.11: Bar chart (options 1) We could also add a title and combine all labels using the labs function. bar_chart + geom_col() + labs(x = &quot;Genre&quot;, y = &quot;Relative frequency&quot;, title = &quot;Chart songs by genre&quot;) + geom_text(aes(label = sprintf(&quot;%.1f%%&quot;, Freq/sum(Freq) * 100)), vjust=-0.2) + theme_minimal() + theme(axis.text.x = element_text(angle=45,vjust=0.75), plot.title = element_text(hjust = 0.5, color = &quot;#666666&quot;) ) Figure 1.12: Bar chart (options 1) We could also add some color to the bars using the colorspace library, which comes with a range of color palettes. For example the shading of the bar could reflect the frequency: library(colorspace) bar_chart + geom_col(aes(fill = Freq)) + labs(x = &quot;Genre&quot;, y = &quot;Relative frequency&quot;, title = &quot;Chart share by genre&quot;) + geom_text(aes(label = sprintf(&quot;%.1f%%&quot;, Freq/sum(Freq) * 100)), vjust=-0.2) + theme_minimal() + ylim(0,0.5) + scale_fill_continuous_sequential(palette = &quot;Blues&quot;) + theme(axis.text.x = element_text(angle=45,vjust=0.75), plot.title = element_text(hjust = 0.5,color = &quot;#666666&quot;), legend.title = element_blank() ) Figure 1.13: Bar chart (options 1) Finally, we can reorder the bars by size using fct_reorder. The first argument to the function is the factor we want to reorder (genre) and the second the variable by which we want to order it (frequency): bar_chart + geom_col(aes(x=fct_reorder(Genre, Freq), fill = Freq)) + labs(x = &quot;Genre&quot;, y = &quot;Relative frequency&quot;, title = &quot;Chart share by genre&quot;) + geom_text(aes(label = sprintf(&quot;%.1f%%&quot;, Freq/sum(Freq) * 100)), vjust=-0.2) + theme_minimal() + ylim(0,0.5) + scale_fill_continuous_sequential(palette = &quot;Blues&quot;) + theme(axis.text.x = element_text(angle=45,vjust=0.75), plot.title = element_text(hjust = 0.5,color = &quot;#666666&quot;), legend.title = element_blank() ) The default theme in ggplot is theme_classic(). For even more options, check out the ggthemes package, which includes formats for specific publications. You can check out the different themes here. For example theme_economist() uses the formatting of the journal “The Economist”: library(ggthemes) bar_chart + geom_col(aes(x=fct_reorder(Genre, Freq))) + labs(x = &quot;Genre&quot;, y = &quot;Relative frequency&quot;, title = &quot;Chart songs by genre&quot;) + geom_text(aes(label = sprintf(&quot;%.1f%%&quot;, Freq/sum(Freq) * 100)), vjust=-0.2) + theme_economist() + ylim(0,0.5) + theme(axis.text.x = element_text(angle=45,vjust=0.55), plot.title = element_text(hjust = 0.5,color = &quot;#666666&quot;) ) Figure 1.14: Bar chart (options 2) There are various similar packages with pre-specified themes, like the ggthemr package, the ggtech package, the rockthemes package, or the tvthemes package. In a next step, we might want to investigate whether the relative frequencies differ between songs with explicit and song without explicit lyrics. For this purpose we first construct the conditional relative frequency table from the previous chapter again. Recall that the latter gives us the relative frequency within a group (in our case genres), as compared to the relative frequency within the entire sample. table_plot_cond_rel &lt;- as.data.frame(prop.table(table(select(music_data, genre, explicit)),2)) #conditional relative frequencies table_plot_cond_rel We can now take these tables to construct plots grouped by explicitness. To achieve this we simply need to add the facet_wrap() function, which replicates a plot multiple times, split by a specified grouping factor. Note that the grouping factor has to be supplied in R’s formula notation, hence it is preceded by a “~” symbol. ggplot(table_plot_cond_rel, aes(x = fct_reorder(genre, Freq), y = Freq)) + geom_col(aes(fill = Freq)) + facet_wrap(~explicit) + labs(x = &quot;&quot;, y = &quot;Relative frequency&quot;, title = &quot;Distribution of genres for explicit and non-explicit songs&quot;) + geom_text(aes(label = sprintf(&quot;%.0f%%&quot;, Freq * 100)), vjust = -0.2) + theme_minimal() + ylim(0, 1) + scale_fill_continuous_sequential(palette = &quot;Blues&quot;) + theme(axis.text.x = element_text(angle = 45, vjust = 1.1, hjust = 1), plot.title = element_text(hjust = 0.5, color = &quot;#666666&quot;), legend.position = &quot;none&quot;) Figure 1.16: Grouped bar chart (facet_wrap) Alternatively, we might be interested to investigate the relative frequencies of explicit and non-explicit lyrics for each genre. To achieve this, we can also use the fill argument, which tells ggplot to fill the bars by a specified variable (in our case “explicit”). The position = “dodge” argument causes the bars to be displayed next to each other (as opposed to stacked on top of one another). table_plot_cond_rel_1 &lt;- as.data.frame(prop.table(table(select(music_data, genre, explicit)),1)) #conditional relative frequencies ggplot(table_plot_cond_rel_1, aes(x = genre, y = Freq, fill = explicit)) + #use &quot;fill&quot; argument for different colors geom_col(position = &quot;dodge&quot;) + #use &quot;dodge&quot; to display bars next to each other (instead of stacked on top) geom_text(aes(label = sprintf(&quot;%.0f%%&quot;, Freq * 100)),position=position_dodge(width=0.9), vjust=-0.25) + scale_fill_discrete_qualitative(palette = &quot;Dynamic&quot;) + labs(x = &quot;Genre&quot;, y = &quot;Relative frequency&quot;, title = &quot;Explicit lyrics share by genre&quot;) + theme_minimal() + theme(axis.text.x = element_text(angle=45,vjust=1.1,hjust=1), plot.title = element_text(hjust = 0.5,color = &quot;#666666&quot;), legend.position = &quot;none&quot; ) Figure 1.17: Grouped bar chart (fill) 4.2.2 Continuous variables 4.2.2.1 Histogram Histograms can be created for continuous data using the geom_histogram() function. Note that the aes() function only needs one argument here, since a histogram is a plot of the distribution of only one variable. As an example, let’s consider our data set containing the music data: head(music_data) Now we can create the histogram using geom_histogram(). The argument binwidth specifies the range that each bar spans, col = \"black\" specifies the border to be black and fill = \"darkblue\" sets the inner color of the bars to dark blue. For brevity, we have now also started naming the x and y axis with the single function labs(), instead of using the two distinct functions xlab() and ylab(). Let’s look at the distribution of streams of R&amp;B songs: music_data |&gt; filter(genre==&quot;R&amp;B&quot;) |&gt; ggplot(aes(streams)) + geom_histogram(binwidth = 20000000, col = &quot;black&quot;, fill = &quot;darkblue&quot;) + labs(x = &quot;Number of streams&quot;, y = &quot;Frequency&quot;, title = &quot;Distribution of streams&quot;) + theme_bw() Figure 1.19: Histogram If you would like to highlight certain points of the distribution, you can use the geom_vline (short for vertical line) function to do this. For example, we may want to highlight the mean and the median of the distribution. music_data |&gt; filter(genre==&quot;R&amp;B&quot;) |&gt; ggplot(aes(streams)) + geom_histogram(binwidth = 20000000, col = &quot;black&quot;, fill = &quot;darkblue&quot;) + labs(x = &quot;Number of streams&quot;, y = &quot;Frequency&quot;, title = &quot;Distribution of streams&quot;, subtitle = &quot;Red vertical line = mean, green vertical line = median&quot;) + geom_vline(aes(xintercept = mean(streams)), color = &quot;red&quot;, size = 1) + geom_vline(aes(xintercept = median(streams)), color = &quot;green&quot;, size = 1) + theme_bw() Figure 1.20: Histogram 2 In this case, you should indicate what the lines refer to. In the plot above, the ‘subtitle’ argument was used to add this information to the plot. Note the difference between a bar chart and the histogram. While a bar chart is used to visualize the frequency of observations for categorical variables, the histogram shows the frequency distribution for continuous variables. 4.2.2.2 Boxplot Another common way to display the distribution of continuous variables is through boxplots. ggplot will construct a boxplot if given the geom geom_boxplot(). In our case we might want to show the difference in streams between the genres. For this analysis, we will transform the streaming variable using a logarithmic transformation, which is common with such data (as we will see later). So let’s first create a new variable by taking the logarithm of the streams variable. music_data$log_streams &lt;- log(music_data$streams) Now, let’s create a boxplot based on these variables and plot the log-transformed number of streams by genre. ggplot(music_data, aes(x = fct_reorder(genre, log_streams), y = log_streams)) + geom_boxplot(coef = 3) + labs(x = &quot;Genre&quot;, y = &quot;Number of streams (log-scale)&quot;) + theme_minimal() + theme(axis.text.x = element_text(angle = 45, vjust = 1.1, hjust = 1), plot.title = element_text(hjust = 0.5, color = &quot;#666666&quot;), legend.position = &quot;none&quot;) Figure 1.22: Boxplot by group The following graphic shows you how to interpret the boxplot: Information contained in a Boxplot Note that you could also flip the boxplot. To do this, you only need to exchange the x- and y-variables. If we provide the categorical variable to the y-axis as follows, the axis will be flipped. ggplot(music_data, aes(x = log_streams, y = fct_reorder(genre, log_streams))) + geom_boxplot(coef = 3) + labs(x = &quot;Number of streams (log-scale)&quot;, y = &quot;Genre&quot;) + theme_minimal() + theme(plot.title = element_text(hjust = 0.5, color = &quot;#666666&quot;), legend.position = &quot;none&quot;) Figure 1.23: Boxplot by group It is often meaningful to augment the boxplot with the data points using geom_jitter(). This way, differences in the distribution of the variable between the genres become even more apparent. ggplot(music_data, aes(x = log_streams, y = fct_reorder(genre, log_streams))) + geom_jitter(colour = &quot;red&quot;, alpha = 0.1) + geom_boxplot(coef = 3, alpha = 0.1) + labs(x = &quot;Number of streams (log-scale)&quot;, y = &quot;Genre&quot;) + theme_minimal() Figure 1.24: Boxplot by group In case you would like to create the boxplot on the total data (i.e., not by group), just leave the y = argument within the aes() function empty: ggplot(music_data,aes(x = log_streams, y = &quot;&quot;)) + geom_boxplot(coef = 3,width=0.3) + labs(x = &quot;Number of streams (log-scale)&quot;, y = &quot;&quot;) Figure 1.25: Single Boxplot 4.2.2.3 Plot of means Another way to get an overview of the difference between two groups is to plot their respective means with confidence intervals. The mean and confidence intervals will enter the plot separately, using the geoms geom_bar() and geom_errorbar(). Don’t worry if you don’t know exactly how to interpret the confidence interval at this stage - we will cover this topic in the next chapter. Let’s assume we would like to plot the difference in streams between the HipHop &amp; Rap genre and all other genres. For this, we first need to create a dummy variable (i.e., a categorical variable with two levels) that indicates if a song is from the HipHop &amp; Rap genre or from any of the other genres. We can use the ifelse() function to do this, which takes 3 arguments, namely 1) the if-statement, 2) the returned value if this if-statement is true, and 3) the value if the if-statement is not true. music_data$genre_dummy &lt;- as.factor(ifelse(music_data$genre==&quot;HipHop/Rap&quot;,&quot;HipHop &amp; Rap&quot;,&quot;other&quot;)) To make plotting the desired comparison easier, we can compute all relevant statistics first, using the summarySE() function from the Rmisc package. library(Rmisc) mean_data &lt;- summarySE(music_data, measurevar=&quot;streams&quot;, groupvars=c(&quot;genre_dummy&quot;)) mean_data The output tells you how many observations there are per group, the mean number of streams per group, as well as the group-specific standard deviation, the standard error, and the confidence interval (more on this in the next chapter). You can now create the plot as follows: ggplot(mean_data,aes(x = genre_dummy, y = streams)) + geom_bar(position=position_dodge(.9), colour=&quot;black&quot;, fill = &quot;#CCCCCC&quot;, stat=&quot;identity&quot;, width = 0.65) + geom_errorbar(position=position_dodge(.9), width=.15, aes(ymin=streams-ci, ymax=streams+ci)) + theme_bw() + labs(x = &quot;Genre&quot;, y = &quot;Average number of streams&quot;, title = &quot;Average number of streams by genre&quot;)+ theme_bw() + theme(plot.title = element_text(hjust = 0.5,color = &quot;#666666&quot;)) Figure 4.1: Plot of means As can be seen, there is a large difference between the genres with respect to the average number of streams. 4.2.2.4 Grouped plot of means We might also be interested to investigate a second factor. Say, we would like to find out if there is a difference between genres with respect to the lyrics (i.e., whether the lyrics are explicit or not). Can we find evidence that explicit lyrics affect streams of songs from the HipHop &amp; Rap genre differently compared to other genres. We can compute the statistics using the summarySE() function by simply adding the second variable to the ‘groupvars’ argument. mean_data2 &lt;- summarySE(music_data, measurevar=&quot;streams&quot;, groupvars=c(&quot;genre_dummy&quot;,&quot;explicit&quot;)) mean_data2 Now we obtained the results for four different groups (2 genres x 2 lyric types) and we can amend the plot easily by adding the ‘fill’ argument to the ggplot() function. The scale_fill_manual() function is optional and specifies the color of the bars manually. ggplot(mean_data2,aes(x = genre_dummy, y = streams, fill = explicit)) + geom_bar(position=position_dodge(.9), colour=&quot;black&quot;, stat=&quot;identity&quot;) + geom_errorbar(position=position_dodge(.9), width=.2, aes(ymin=streams-ci, ymax=streams+ci)) + scale_fill_manual(values=c(&quot;#CCCCCC&quot;,&quot;#FFFFFF&quot;)) + theme_bw() + labs(x = &quot;Genre&quot;, y = &quot;Average number of streams&quot;, title = &quot;Average number of streams by genre and lyric type&quot;)+ theme_bw() + theme(plot.title = element_text(hjust = 0.5,color = &quot;#666666&quot;)) Figure 4.2: Grouped plot of means The results of the analysis show that also in the HipHop &amp; Rap genre, songs with non-explicit lyrics obtain more streams on average, contrary to our expectations. 4.2.2.5 Scatter plot The most common way to show the relationship between two continuous variables is a scatterplot. As an example, let’s investigate if there is an association between the number of streams a song receives and the speechiness of the song. The following code creates a scatterplot with some additional components. The geom_smooth() function creates a smoothed line from the data provided. In this particular example we tell the function to draw the best possible straight line (i.e., minimizing the distance between the line and the points) through the data (via the argument method = \"lm\"). Note that the “shape = 1” argument passed to the geom_point() function produces hollow circles (instead of solid) and the “fill” and “alpha” arguments passed to the geom_smooth() function specify the color and the opacity of the confidence interval, respectively. ggplot(music_data, aes(speechiness, log_streams)) + geom_point(shape =1) + labs(x = &quot;Genre&quot;, y = &quot;Relative frequency&quot;) + geom_smooth(method = &quot;lm&quot;, fill = &quot;blue&quot;, alpha = 0.1) + labs(x = &quot;Speechiness&quot;, y = &quot;Number of streams (log-scale)&quot;, title = &quot;Scatterplot of streams and speechiness&quot;) + theme_bw() + theme(plot.title = element_text(hjust = 0.5,color = &quot;#666666&quot;)) Figure 4.3: Scatter plot As you can see, there appears to be a positive relationship between advertising and sales. 4.2.2.5.1 Grouped scatter plot It could be that customers from different store respond differently to advertising. We can visually capture such differences with a grouped scatter plot. By adding the argument colour = store to the aesthetic specification, ggplot automatically treats the two stores as distinct groups and plots accordingly. ggplot(music_data, aes(speechiness, log_streams, colour = explicit)) + geom_point(shape =1) + geom_smooth(method=&quot;lm&quot;, alpha = 0.1) + labs(x = &quot;Speechiness&quot;, y = &quot;Number of streams (log-scale)&quot;, title = &quot;Scatterplot of streams and speechiness by lyric type&quot;, colour=&quot;Explicit&quot;) + scale_color_manual(values=c(&quot;lightblue&quot;,&quot;darkblue&quot;)) + theme_bw() + theme(plot.title = element_text(hjust = 0.5,color = &quot;#666666&quot;)) Figure 4.4: Grouped scatter plot It appears from the plot that the association between speechiness and the number of streams is stronger for songs without explicit lyrics. 4.2.2.6 Line plot Another important type of plot is the line plot used if, for example, you have a variable that changes over time and you want to plot how it develops over time. To demonstrate this we will investigate a data set that show the development of the number of streams of the Top200 songs on a popular music streaming service for different region. Let’s investigate the data first and bring all variables to the correct format. music_data_ctry &lt;- read.table(&quot;https://raw.githubusercontent.com/IMSMWU/Teaching/master/MRDA2017/streaming_charts_ctry.csv&quot;, sep = &quot;,&quot;, header = TRUE) |&gt; mutate(week = as.Date(week), region = as.factor(region)) head(music_data_ctry) In a first step, let’s investigate the development for Austria, by filtering the data to region ‘at’. music_data_at &lt;- filter(music_data_ctry, region == &#39;at&#39;) Given the correct aes() and geom specification ggplot constructs the correct plot for us. In order to make large numbers more readable we use the label_comma function from the scales package in the scale_y_continuous layer. ggplot(music_data_at, aes(x = week, y = streams)) + geom_line() + labs(x = &quot;&quot;, y = &quot;Total streams in Austria&quot;, title = &quot;Weekly number of streams in Austria&quot;) + theme_bw() + scale_y_continuous(labels = scales::label_comma()) + theme(plot.title = element_text(hjust = 0.5,color = &quot;#666666&quot;)) Figure 4.5: Line plot There appears to be a positive trend in the market. Now let’s compare Austria to other countries. Note that the %in% operator checks for us if any of the region names specified in the vector are included in the region column. music_data_at_compare &lt;- filter(music_data_ctry, region %in% c(&#39;at&#39;,&#39;de&#39;,&#39;ch&#39;,&#39;se&#39;,&#39;dk&#39;,&#39;nl&#39;)) We can now include the other specified countries in the plot by using the ‘color’ argument. ggplot(music_data_at_compare, aes(x = week, y = streams, color = region)) + geom_line() + labs(x = &quot;Week&quot;, y = &quot;Total streams&quot;, title = &quot;Weekly number of streams by country&quot;) + theme_bw() + theme(plot.title = element_text(hjust = 0.5,color = &quot;#666666&quot;)) + scale_y_continuous(labels = scales::label_comma()) Figure 4.6: Line plot (by region) One issue in plot like this can be that the scales between countries is very different (i.e., in Germany there are many more streams because Germany is larger than the other countries). You could also use the facet_wrap() function to create one individual plot by region and specify ‘scales = “free_y”’ so that each individual plot has its own scale on the y-axis. We should also indicate the number of streams in millions by dividing the number of streams. ggplot(music_data_at_compare, aes(x = week, y = streams/1000000)) + geom_line() + facet_wrap(~region, scales = &quot;free_y&quot;) + labs(x = &quot;Week&quot;, y = &quot;Total streams (in million)&quot;, title = &quot;Weekly number of streams by country&quot;) + theme_bw() + theme(plot.title = element_text(hjust = 0.5,color = &quot;#666666&quot;)) Figure 4.7: Line plot (facet wrap) Now it’s easier to see that the trends are different between countries. While Sweden and Denmark appear to be saturated, the other market show strong growth. 4.2.2.7 Area plots A slightly different way to plot this data is through area plot using the geom_area() function. ggplot(music_data_at_compare, aes(x = week, y = streams/1000000)) + geom_area(fill = &quot;steelblue&quot;, color = &quot;steelblue&quot;, alpha = 0.5) + facet_wrap(~region, scales = &quot;free_y&quot;) + labs(x = &quot;Week&quot;, y = &quot;Total streams (in million)&quot;, title = &quot;Weekly number of streams by country&quot;) + theme_bw() + theme(plot.title = element_text(hjust = 0.5,color = &quot;#666666&quot;)) Figure 4.8: Line plot (facet wrap) If the relative share of the overall streaming volume is of interest, you could use a stacked area plot to visualize this. ggplot(music_data_at_compare, aes(x = week, y = streams/1000000,group=region,fill=region,color=region)) + geom_area(position=&quot;stack&quot;,alpha = 0.65) + labs(x = &quot;Week&quot;, y = &quot;Total streams (in million)&quot;, title = &quot;Weekly number of streams by country&quot;) + theme_bw() + theme(plot.title = element_text(hjust = 0.5,color = &quot;#666666&quot;)) Figure 4.9: Line plot (facet wrap) In this type of plot it is easy to spot the relative size of the regions. In some cases it could also make sense to add a secondary y-axis, for example, if you would like to compare two regions with very different scales in one plot. Let’s assume, we would like to compare Austria and Sweden and take the corresponding subset. music_data_at_se_compare &lt;- filter(music_data_ctry, region %in% c(&#39;at&#39;,&#39;se&#39;)) In order to add the secondary y-axis, we need the data in a slightly different format where we have one column for each country. This is called the ‘wide format’ as opposed to the ‘long format’ where the data is stacked on top of each other for all regions. We can easily convert the data to the wide format by using the pivot_wider() function from the tidyr package. library(tidyr) data_wide &lt;- pivot_wider(music_data_at_se_compare, names_from = region, values_from = streams) data_wide As another intermediate step, we need to compute the ratio between the two variables we would like to plot on the two axis, since the scale of the second axis is determined based on the ratio with the other variable. ratio &lt;- mean(data_wide$at/1000000)/mean(data_wide$se/1000000) Now we can create the plot with the secondary y-axis as follows: ggplot(data_wide) + geom_area(aes(x = week, y = at/1000000, colour = &quot;Austria&quot;, fill = &quot;Austria&quot;), alpha = 0.5) + geom_area(aes(x = week, y = (se/1000000)*ratio, colour = &quot;Sweden&quot;, fill = &quot;Sweden&quot;), alpha = 0.5) + scale_y_continuous(sec.axis = sec_axis(~./ratio, name = &quot;Total streams SE (in million)&quot;)) + scale_fill_manual(values = c(&quot;#999999&quot;, &quot;#E69F00&quot;)) + scale_colour_manual(values = c(&quot;#999999&quot;, &quot;#E69F00&quot;),guide=FALSE) + theme_minimal() + labs(x = &quot;Week&quot;, y = &quot;Total streams AT (in million)&quot;, title = &quot;Weekly number of streams by country&quot;) + theme(plot.title = element_text(hjust = 0.5,color = &quot;#666666&quot;), legend.title = element_blank(), legend.position = &quot;bottom&quot; ) Figure 4.10: Secondary y-axis In this plot it is easy to see the difference in trends between the countries. 4.2.3 Saving plots To save the last displayed plot, simply use the function ggsave(), and it will save the plot to your working directory. Use the arguments heightand width to specify the size of the file. You may also choose the file format by adjusting the ending of the file name. E.g., file_name.jpg will create a file in JPG-format, whereas file_name.png saves the file in PNG-format, etc.. ggsave(&quot;test_plot.jpg&quot;, height = 5, width = 8.5) 4.2.4 ggplot extensions As the ggplot2 package became more and more popular over the past years, more and more extensions have been developed by users that can be used for specific purposes that are not yet covered by the standard functionality of ggplot2. You can find a list of the extensions here. Below, you can find some example for the additional options. 4.2.4.1 Results of statistical tests (ggstatsplot) You may use the ggstatplot package to augment your plots with the results from statistical tests, such as an ANOVA. You can find a presentation about the capabilities of this package here. The boxplot below shows an example. library(ggstatsplot) music_data_subs &lt;- filter(music_data, genre %in% c(&quot;HipHop/Rap&quot;, &quot;Soundtrack&quot;,&quot;Pop&quot;,&quot;Rock&quot;)) ggbetweenstats( data = music_data_subs, title = &quot;Number of streams by genre&quot;, # title for the plot plot.type = &quot;box&quot;, x = genre, # 4 groups y = log_streams, type = &quot;p&quot;, # default messages = FALSE, bf.message = FALSE, pairwise.comparisons = TRUE # display results from pairwise comparisons ) Figure 4.11: Boxplot using ggstatsplot package 4.2.4.1.1 Combination of plots (ggExtra) Using the ggExtra() package, you may combine two type of plots. For example, the following plot combines a scatterplot with a histogram: library(ggExtra) p &lt;- ggplot(music_data, aes(x = speechiness, y = log_streams)) + geom_point() + labs(x = &quot;Speechiness&quot;, y = &quot;Number of streams (log-scale)&quot;, title = &quot;Scatterplot &amp; histograms of streams and speechiness&quot;) + theme_bw() + theme(plot.title = element_text(hjust = 0.5,color = &quot;#666666&quot;)) ggExtra::ggMarginal(p, type = &quot;histogram&quot;) Figure 4.12: Scatter plot with histogram In this case, the type = \"histogram\" argument specifies that we would like to plot a histogram. However, you could also opt for type = \"boxplot\" or type = \"density\" to use a boxplot or density plot instead. 4.2.4.2 Appendix 4.2.4.2.1 Covariation plots To visualize the co-variation between categorical variables, you’ll need to count the number of observations for each combination stored in the frequency table. Say, we wanted to investigate the association between the popularity of a song and the level of ‘speechiness’. For this exercise, let’s assume we have both variables measured as categorical (factor) variables. We can use the quantcut() function to create categorical variables based on the continuous variables. All we need to do is tell the function how many categories we would like to obtain and it will divide the data based on the percentiles equally. library(gtools) music_data$streams_cat &lt;- as.numeric(quantcut(music_data$streams, 5, na.rm=TRUE)) music_data$speech_cat &lt;- as.numeric(quantcut(music_data$speechiness, 3, na.rm=TRUE)) music_data$streams_cat &lt;- factor(music_data$streams_cat, levels = 1:5, labels = c(&quot;low&quot;, &quot;low-med&quot;, &quot;medium&quot;, &quot;med-high&quot;, &quot;high&quot;)) #convert to factor music_data$speech_cat &lt;- factor(music_data$speech_cat, levels = 1:3, labels = c(&quot;low&quot;, &quot;medium&quot;, &quot;high&quot;)) #convert to factor Now we have multiple ways to visualize a relationship between the two variables with ggplot. One option would be to use a variation of the scatterplot which counts how many points overlap at any given point and increases the dot size accordingly. This can be achieved with geom_count() as the example below shows where the stat(prop) argument assures that we get relative frequencies and with the group argument we tell R to compute the relative frequencies by speechiness. ggplot(data = music_data) + geom_count(aes(x = speech_cat, y = streams_cat, size = stat(prop), group = speech_cat)) + ylab(&quot;Popularity&quot;) + xlab(&quot;Speechiness&quot;) + labs(size = &quot;Proportion&quot;) + theme_bw() Figure 4.13: Covariation between categorical data (1) The plot shows that there appears to be a positive association between the popularity of a song and its level of speechiness. Another option would be to use a tile plot that changes the color of the tile based on the frequency of the combination of factors. To achieve this, we first have to create a dataframe that contains the relative frequencies of all combinations of factors. Then we can take this dataframe and pass it to geom_tile(), while specifying that the fill of each tile should be dependent on the observed frequency of the factor combination, which is done by specifying the fill in the aes() function. table_plot_rel &lt;- prop.table(table(music_data[,c(&quot;speech_cat&quot;, &quot;streams_cat&quot;)]),1) table_plot_rel &lt;- as.data.frame(table_plot_rel) ggplot(table_plot_rel, aes(x = speech_cat, y = streams_cat)) + geom_tile(aes(fill = Freq)) + ylab(&quot;Populartiy&quot;) + xlab(&quot;Speechiness&quot;) + theme_bw() Figure 4.14: Covariation between categorical data (2) 4.2.4.2.2 Location data (ggmap) Now that we have covered the most important plots, we can look at what other type of data you may come across. One type of data that is increasingly available is the geo-location of customers and users (e.g., from app usage data). The following data set contains the app usage data of Shazam users from Germany. The data contains the latitude and longitude information where a music track was “shazamed”. library(ggmap) library(dplyr) geo_data &lt;- read.table(&quot;https://raw.githubusercontent.com/IMSMWU/Teaching/master/MRDA2017/geo_data.dat&quot;, sep = &quot;\\t&quot;, header = TRUE) head(geo_data) There is a package called “ggmap”, which is an augmentation for the ggplot packages. It lets you load maps from different web services (e.g., Google maps) and maps the user location within the coordination system of ggplot. With this information, you can create interesting plots like heat maps. We won’t go into detail here but you may go through the following code on your own if you are interested. However, please note that you need to register an API with Google in order to make use of this package. #register_google(key = &quot;your_api_key&quot;) # Download the base map de_map_g_str &lt;- get_map(location=c(10.018343,51.133481), zoom=6, scale=2) # results in below map (wohoo!) # Draw the heat map ggmap(de_map_g_str, extent = &quot;device&quot;) + geom_density2d(data = geo_data, aes(x = lon, y = lat), size = 0.3) + stat_density2d(data = geo_data, aes(x = lon, y = lat, fill = ..level.., alpha = ..level..), size = 0.01, bins = 16, geom = &quot;polygon&quot;) + scale_fill_gradient(low = &quot;green&quot;, high = &quot;red&quot;) + scale_alpha(range = c(0, 0.3), guide = FALSE) Learning check (LC4.1) For which data types is it meaningful to compute the mean? Nominal Ordinal Interval Ratio (LC4.2) How can you compute the standardized variate of a variable X? \\(Z=\\frac{X_i-\\bar{X}}{s}\\) \\(Z=\\frac{\\bar{X}+X_i}{s}\\) \\(Z=\\frac{s}{\\bar{X}+X_i}\\) \\(Z=s*({\\bar{X}+X_i)}\\) None of the above You wish to analyze the following data frame ‘df’ containing information about cars (LC4.3) How could you add a new variable containing the z-scores of the variable ‘mpg’ in R? df$mpg_std &lt;- zscore(df$mpg) df$mpg_std &lt;- stdv(df$mpg) df$mpg_std &lt;- std.scale(df$mpg) df$mpg_std &lt;- scale(df$mpg) None of the above (LC4.4) How could you produce the below output? describe(select(mtcars, hp, mpg, qsec)) summary(select(mtcars, hp, mpg, qsec)) table(select(mtcars, hp, mpg, qsec)) str(select(mtcars, hp, mpg, qsec)) None of the above (LC4.5) The last column “carb” indicates the number of carburetors each model has. By using a function we got to know the number of car models that have a certain number carburetors. Which function helped us to obtain this information? ## ## 1 2 3 4 6 8 ## 7 10 3 10 1 1 describe(mtcars$carb) table(mtcars$carb) str(mtcars$carb) prop.table(mtcars$carb) None of the above (LC4.6) What type of data can be meaningfully depicted in a scatter plot? Two categorical variables One categorical and one continuous variable Two continuous variables One continuous variable None of the above (LC4.7) Which statement about the graph below is true? This is a bar chart This is a histogram It shows the frequency distribution of a continuous variable It shows the frequency distribution of a categorical variable None of the above (LC4.8) Which statement about the graph below is true? This is a bar chart 50% of observations are contained in the gray area The horizontal black line indicates the mean This is a boxplot None of the above (LC4.9) Which function can help you to save a graph made with ggplot()? ggsave() write.plot() save.plot() export.plot() (LC4.10) For a variable that follows a normal distribution, within how many standard deviations of the mean are 95% of values? 1.645 1.960 2.580 3.210 None of the above References Field, A., Miles J., &amp; Field, Z. (2012). Discovering Statistics Using R. Sage Publications. Chang, W. (2020). R Graphics Cookbook, 2nd edition (https://r-graphics.org/) Grolemund, G. &amp; Wickham, H. (2020). R for Data Science (https://r4ds.had.co.nz/) "],["statistical-inference.html", "5 Statistical inference 5.1 If we knew it all 5.2 The Central Limit Theorem 5.3 Using what we actually know 5.4 Confidence Intervals for the Sample Mean 5.5 Null Hypothesis Statistical Testing (NHST) Learning check References", " 5 Statistical inference This chapter will provide you with a basic intuition on statistical inference. As marketing researchers we are usually faced with “imperfect” data in the sense that we cannot collect all the data we would like. Imagine you are interested in the average amount of time WU students spend listening to music every month. Ideally, we could force all WU students to fill out our survey. Realistically we will only be able to observe a small fraction of students (maybe 500 out of the \\(25.000+\\)). With the data from this small fraction at hand, we want to make an inference about the true average listening time of all WU students. We are going to start with the assumption that we know everything. That is, we first assume that we know all WU students’ listening times and analyze the distribution of the listening time in the entire population. Subsequently, we are going to look at the uncertainty that is introduced by only knowing some of the students’ listening times (i.e., a sample from the population) and how that influences our analysis. 5.1 If we knew it all Assume there are \\(25,000\\) students at WU and every single one has kindly provided us with the hours they listened to music in the past month. Using the code below, the rnorm() function will be used to generate 25,000 observations from a normal distribution with a mean of 50 and a standard deviation of 10. Although you might not be used to working with this type of simulated (i.e., synthetic) data, it is useful when explaining statistical concepts because the properties of the data are known. In this case, for example, we know the true mean (\\(49.93\\) hours) and the true standard deviation (SD = \\(10.02\\)) and thus we can easily summarize the entire distribution. Since the data follows a normal distribution, roughly 95% of the values lie within 2 standard deviations from the mean, as the following plot shows: In this case, we refer to all WU students as the population. In general, the population is the entire group we are interested in. This group does not have to necessarily consist of people, but could also be companies, stores, animals, etc.. The parameters of the distribution of population values (in hour case: “hours”) are called population parameters. As already mentioned, we do not usually know population parameters but use inferential statistics to infer them based on our sample from the population, i.e., we measure statistics from a sample (e.g., the sample mean \\(\\bar x\\)) to estimate population parameters (the population mean \\(\\mu\\)). Here, we will use the following notation to refer to either the population parameters or the sample statistic: Variable Sample statistic Population parameter Size n N Mean \\(\\bar{x} = {1 \\over n}\\sum_{i=1}^n x_i\\) \\(\\mu = {1 \\over N}\\sum_{i=1}^N x_i\\) Variance \\(s^2 = {1 \\over n-1}\\sum_{i=1}^n (x_i-\\bar{x})^2\\) \\(\\sigma^2 = {1 \\over N}\\sum_{i=1}^N (x_i-\\mu)^2\\) Standard deviation \\(s = \\sqrt{s^2}\\) \\(\\sigma = \\sqrt{\\sigma^2}\\) Standard error \\(SE_{\\bar x} = {s \\over \\sqrt{n}}\\) \\(\\sigma_{\\bar x} = {\\sigma \\over \\sqrt{n}}\\) Using this notation, \\(N\\) refers to the number of observations in the entire population (i.e., 25,000 in our example) and \\(n\\) refers to a subset of the population (i.e., a sample). As you can see, we will use different Greek letters to denote the sample statistics and the population parameters. Another difference, you might have noticed is that in the computation of the sample variance, we divide by \\(n-1\\), not \\(n\\). This is also know as the ‘Bessel’s correction’ and it corrects the bias in the estimation of the population variance based on a sample. More specifically, due to the correction, the corrected variance will be larger, since the denominator gets smaller by subtracting 1. This is done because the variance will most of the time be smaller when calculated using the sum of squared deviations from the sample mean, compared to using the sum of deviations from the population mean. The intuition is that, the larger your sample is, the more likely it is to get more population-representative points. Or, to put it another way, it is less likely to get a sample mean which results in differences which are too small. Thus, the larger your sample size \\(n\\), the less of a correction you need and, hence, the smaller the impact the correction component will be. 5.1.1 Sampling from a known population In the first step towards a realistic research setting, let us take one sample from this population and calculate the mean listening time. We can simply sample the row numbers of students and then subset the hours vector with the sampled row numbers. The sample() function will be used to draw a sample of size 100 from the population of 25,000 students, and one student can only be drawn once (i.e., replace = FALSE). The following plot shows the distribution of listening times for our sample. Observe that in this first draw the mean (\\(\\bar x =\\) 49.67) is quite close to the actual mean (\\(\\mu =\\) 49.93). It seems like the sample mean is a decent estimate of the population mean. However, we could just be lucky this time and the next sample could turn out to have a different mean. Let us continue by looking at four additional random samples, consisting of 100 students each. The following plot shows the distribution of listening times for the four different samples from the population. It becomes clear that the mean is slightly different for each sample. This is referred to as sampling variation and it is completely fine to get a slightly different mean every time we take a sample. We just need to find a way of expressing the uncertainty associated with the fact that we only have data from one sample, because in a realistic setting you are most likely only going to have access to a single sample. So in order to make sure that the first draw is not just pure luck and the sample mean is in fact a good estimate for the population mean, let us take many (e.g., \\(20,000\\)) different samples from the population. That is, we repeatedly draw 100 students randomly from the population without replacement (that is, once a student has been drawn she or he is removed from the pool and cannot be drawn again) and calculate the mean of each sample. This will show us a range within which the sample mean of any sample we take is likely going to be. We are going to store the means of all the samples in a matrix and then plot a histogram of the means to observe the likely values. As you can see, on average the sample mean (“mean of sample means”) is extremely close to the population mean, despite only sampling \\(100\\) people at a time. This distribution of sample means is also referred to as sampling distribution of the sample mean. However, there is some uncertainty, and the means are slightly different for the different samples and range from 45.95 to 54.31. 5.1.2 Standard error of the mean Due to the variation in the sample means shown in our simulation, it is never possible to say exactly what the population mean is based on a single sample. However, even with a single sample we can infer a range of values within which the population mean is likely contained. In order to do so, notice that the sample means are approximately normally distributed. Another interesting fact is that the mean of sample means (i.e., 49.94) is roughly equal to the population mean (i.e., 49.93). This tells us already that generally the sample mean is a good approximation of the population mean. However, in order to make statements about the expected range of possible values, we would need to know the standard deviation of the sampling distribution. The formal representation of the standard deviation of the sample means is \\[ \\sigma_{\\bar x} = {\\sigma \\over \\sqrt{n}} \\] where \\(\\sigma\\) is the population SD and \\(n\\) is the sample size. \\(\\sigma_{\\bar{x}}\\) is referred to as the Standard Error of the mean and it expresses the variation in sample means we should expect given the number of observations in our sample and the population SD. That is, it provides a measure of how precisely we can estimate the population mean from the sample mean. 5.1.2.1 Sample size The first thing to notice here is that an increase in the number of observations per sample \\(n\\) decreases the range of possible sample means (i.e., the standard error). This makes intuitive sense. Think of the two extremes: sample size \\(1\\) and sample size \\(25,000\\). With a single person in the sample we do not gain a lot of information and our estimate is very uncertain, which is expressed through a larger standard deviation. Looking at the histogram at the beginning of this chapter showing the number of students for each of the listening times, clearly we would get values below \\(25\\) or above \\(75\\) for some samples. This is way farther away from the population mean than the minimum and the maximum of our \\(100\\) person samples. On the other hand, if we sample every student we get the population mean every time and thus we do not have any uncertainty (assuming the population does not change). Even if we only sample, say \\(24,000\\) people every time, we gain a lot of information about the population and the sample means would not be very different from each other since only up to \\(1,000\\) people are potentially different in any given sample. Thus, with larger (smaller) samples, there is less (more) uncertainty that the sample is a good approximation of the entire population. The following plot shows the relationship between the sample size and the standard error. Samples of increasing size are randomly drawn from the population of WU students. You can see that the standard error is decreasing with the number of observations. Figure 5.1: Relationship between the sample size and the standard error The following plots show the relationship between the sample size and the standard error in a slightly different way. The plots show the range of sample means resulting from the repeated sampling process for different sample sizes. Notice that the more students are contained in the individual samples, the less uncertainty there is when estimating the population mean from a sample (i.e., the possible values are more closely centered around the mean). So when the sample size is small, the sample mean can expected to be very different the next time we take a sample. When the sample size is large, we can expect the sample means to be more similar every time we take a sample. As you can see, the standard deviation of the sample means (\\(\\sigma_{\\bar x}\\)) decreases as the sample size increases as a consequence of the reduced uncertainty about the true sample mean when we take larger samples. 5.1.2.2 Population standard deviation A second factor determining the standard deviation of the distribution of sample means (\\(\\sigma_{\\bar x}\\)) is the standard deviation associated with the population parameter (\\(\\sigma\\)). Again, looking at the extremes illustrates this well. If all WU students listened to music for approximately the same amount of time, the samples would not differ much from each other. In other words, if the standard deviation in the population is lower, we expect the standard deviation of the sample means to be lower as well. This is illustrated by the following plots. In the first plot (panel A), we assume a much smaller population standard deviation (e.g., \\(\\sigma\\) = 1 instead of \\(\\sigma\\) = 10). Notice how the smaller (larger) the population standard deviation, the less (more) uncertainty there is when estimating the population mean from a sample (i.e., the possible values are more closely centered around the mean). So when the population SD is large, the sample mean can expected to be very different the next time we take a sample. When the population SD is small, we can expect the sample means to be more similar. 5.2 The Central Limit Theorem The attentive reader might have noticed that the population above was generated using a normal distribution function. It would be very restrictive if we could only analyze populations whose values are normally distributed. Furthermore, we are unable in reality to check whether the population values are normally distributed since we do not know the entire population. However, it turns out that the results generalize to many other distributions. This is described by the Central Limit Theorem. The central limit theorem states that if (1) the population distribution has a mean (there are examples of distributions that don’t have a mean , but we will ignore these here), and (2) we take a large enough sample, then the sampling distribution of the sample mean is approximately normally distributed. What exactly “large enough” means depends on the setting, but the interactive element at the end of this chapter illustrates how the sample size influences how accurately we can estimate the population parameters from the sample statistics. To illustrate this, let’s repeat the analysis above with a population from a gamma distribution. In the previous example, we assumed a normal distribution so it was more likely for a given student to spend around 50 hours per week listening to music. The following example depicts the case in which most students spend a similar amount of time listening to music, but there are a few students who very rarely listen to music, and some music enthusiasts with a very high level of listening time. In the following code, we will use the rgamma() function to generate 25,000 random observations from the gamma distribution. The gamma distribution is specified by shape and scale parameters instead of the mean and standard deviation of the normal distribution. Here is a histogram of the listening times in the population: The vertical black line represents the population mean (\\(\\mu\\)), which is 19.98 hours. The following plot depicts the histogram of listening times of four random samples from the population, each consisting of 100 students: As in the previous example, the mean is slightly different every time we take a sample due to sampling variation. Also note that the distribution of listening times no longer follows a normal distribution as a result of the fact that we now assume a gamma distribution for the population with a positive skew (i.e., lower values more likely, higher values less likely). Let’s see what happens to the distribution of sample means if we take an increasing number of samples, each drawn from the same gamma population: Two things are worth noting: (1) The more (hypothetical) samples we take, the more the sampling distribution approaches a normal distribution. (2) The mean of the sampling distribution of the sample mean (\\(\\mu_{\\bar x}\\)) is very similar to the population mean (\\(\\mu\\)). From this we can see that the mean of a sample is a good estimate of the population mean. In summary, it is important to distinguish two types of variation: (1) For each individual sample that we may take in real life, the standard deviation (\\(s\\)) is used to describe the natural variation in the data and the data may follow a non-normal distribution. (2) If we would (hypothetically!) repeat the study many times, the sampling distribution of the sample mean follows a normal distribution for large samples sizes (even if data from each individual study are non-normal), and the standard error (\\(\\sigma_{\\bar x}\\)) is used to describe the variation between study results. This is an important feature, since many statistical tests assume that the sampling distribution is normally distributed. As we have seen, this does not mean that the data from one particular sample needs to follow a normal distribution. 5.3 Using what we actually know So far we have assumed to know the population standard deviation (\\(\\sigma\\)). This an unrealistic assumption since we do not know the entire population. The best guess for the population standard deviation we have is the sample standard deviation, denoted \\(s\\). Thus, the standard error of the mean is usually estimated from the sample standard deviation: \\[ \\sigma_{\\bar x} \\approx SE_{\\bar x}={s \\over \\sqrt{n}} \\] Note that \\(s\\) itself is a sample estimate of the population parameter \\(\\sigma\\). This additional estimation introduces further uncertainty. You can see in the interactive element below that the sample SD, on average, provides a good estimate of the population SD. That is, the distribution of sample SDs that we get by drawing many samples is centered around the population value. Again, the larger the sample, the closer any given sample SD is going to be to the population parameter and we introduce less uncertainty. One conclusion is that your sample needs to be large enough to provide a reliable estimate of the population parameters. What exactly “large enough” means depends on the setting, but the interactive element illustrates how the remaining values change as a function of the sample size. We will not go into detail about the importance of random samples but basically the correctness of your estimate depends crucially on having a sample at hand that actually represents the population. Unfortunately, we will usually not notice if the sample is non-random. Our statistics are still a good approximation of “a” population parameter, namely the one for the population that we actually sampled but not the one we are interested in. To illustrate this uncheck the “Random Sample” box below. The new sample will be only from the top \\(50\\%\\) music listeners (but this generalizes to different types of non-random samples). 5.4 Confidence Intervals for the Sample Mean When we try to estimate parameters of populations (e.g., the population mean \\(\\mu\\)) from a sample, the average value from a sample (e.g., the sample mean \\(\\bar x\\)) only provides an estimate of what the real population parameter is. The next time you collect a sample of the same size, you could get a different average. This is sampling variation and it is completely fine to get a slightly different sample mean every time we take a sample as we have seen above. However, this inherent uncertainty about the true population parameter means that coming up with an exact estimate (i.e., a point estimate) for a particular population parameter is really difficult. That is why it is often informative to construct a range around that statistic (i.e., an interval estimate) that likely contains the population parameter with a certain level of confidence. That is, we construct an interval such that for a large share (say 95%) of the sample means we could potentially get, the population mean is within that interval. Let us consider one random sample of 100 students from our population above. From the central limit theorem we know that the sampling distribution of the sample mean is approximately normal and we know that for the normal distribution, 95% of the values lie within about 2 standard deviations from the mean. Actually, it is not exactly 2 standard deviations from the mean. To get the exact number, we can use the quantile function for the normal distribution qnorm(): qnorm(0.975) ## [1] 1.959964 We use 0.975 (and not 0.95) to account for the probability at each end of the distribution (i.e., 2.5% at the lower end and 2.5% at the upper end). We can see that 95% of the values are roughly within 1.96 standard deviations from the mean. Since we know the sample mean (\\(\\bar x\\)) and we can estimate the standard deviation of the sampling distribution (\\(\\sigma_{\\bar x} \\approx {s \\over \\sqrt{n}}\\)), we can now easily calculate the lower and upper boundaries of our confidence interval as: \\[ CI_{lower} = {\\bar x} - z_{1-{\\alpha \\over 2}} * \\sigma_{\\bar x} \\\\ CI_{upper} = {\\bar x} + z_{1-{\\alpha \\over 2}} * \\sigma_{\\bar x} \\] Here, \\(\\alpha\\) refers to the significance level. You can find a detailed discussion of this point at the end of the next chapter. For now, we will adopt the widely accepted significance level of 5% and set \\(\\alpha\\) to 0.05. Thus, \\(\\pm z_{1-{\\alpha \\over 2}}\\) gives us the z-scores (i.e., number of standard deviations from the mean) within which range 95% of the probability density lies. Plugging in the values from our sample, we get: sample_mean &lt;- mean(hours_s) se &lt;- sd(hours_s)/sqrt(sample_size) ci_lower &lt;- sample_mean - qnorm(0.975)*se ci_upper &lt;- sample_mean + qnorm(0.975)*se ci_lower ## [1] 17.67089 ci_upper ## [1] 23.1592 such that if we collected 100 samples and computed the mean and confidence interval for each of them, in \\(95\\%\\) of the cases, the true population mean is going to be within this interval between 17.67 and 23.16. Note the correct interpretation of the confidence interval: If we collected 100 samples, calculated the mean and then calculated a confidence interval for that mean, then, for 95 of these samples, the confidence intervals we constructed would contain the true value of the mean in the population. This is illustrated in the plot below that shows the mean of the first 100 samples and their confidence intervals: Note that this does not mean that for a specific sample there is a \\(95\\%\\) chance that the population mean lies within its confidence interval. The statement depends on the large number of samples we do not actually draw in a real setting. You can view the set of all possible confidence intervals similarly to the sides of a coin or a die. If we throw a coin many times, we are going to observe head roughly half of the times. This does not, however, exclude the possibility of observing tails for the first 10 throws. Similarly, any specific confidence interval might or might not include the population mean but if we take many samples on average \\(95\\%\\) of the confidence intervals are going to include the population mean. 5.5 Null Hypothesis Statistical Testing (NHST) We test hypotheses because we are confined to taking samples – we rarely work with the entire population. In the previous chapter, we introduced the standard error (i.e., the standard deviation of a large number of hypothetical samples) as an estimate of how well a particular sample represents the population. We also saw how we can construct confidence intervals around the sample mean \\(\\bar x\\) by computing \\(SE_{\\bar x}\\) as an estimate of \\(\\sigma_{\\bar x}\\) using \\(s\\) as an estimate of \\(\\sigma\\) and calculating the 95% CI as \\(\\bar x \\pm 1.96 * SE_{\\bar x}\\). Although we do not know the true population mean (\\(\\mu\\)), we might have an hypothesis about it and this would tell us how the corresponding sampling distribution looks like. Based on the sampling distribution of the hypothesized population mean, we could then determine the probability of a given sample assuming that the hypothesis is true. Let us again begin by assuming we know the entire population using the example of music listening times among students from the previous example. As a reminder, the following plot shows the distribution of music listening times in the population of WU students. In this example, the population mean (\\(\\mu\\)) is equal to 19.98, and the population standard deviation \\(\\sigma\\) is equal to 14.15. 5.5.1 The null hypothesis Let us assume that we were planning to take a random sample of 50 students from this population and our hypothesis was that the mean listening time is equal to some specific value \\(\\mu_0\\), say \\(10\\). This would be our null hypothesis. The null hypothesis refers to the statement that is being tested and is usually a statement of the status quo, one of no difference or no effect. In our example, the null hypothesis would state that there is no difference between the true population mean \\(\\mu\\) and the hypothesized value \\(\\mu_0\\) (in our example \\(10\\)), which can be expressed as follows: \\[ H_0: \\mu = \\mu_0 \\] When conducting research, we are usually interested in providing evidence against the null hypothesis. If we then observe sufficient evidence against it and our estimate is said to be significant. If the null hypothesis is rejected, this is taken as support for the alternative hypothesis. The alternative hypothesis assumes that some difference exists, which can be expressed as follows: \\[ H_1: \\mu \\neq \\mu_0 \\] Accepting the alternative hypothesis in turn will often lead to changes in opinions or actions. Note that while the null hypothesis may be rejected, it can never be accepted based on a single test. If we fail to reject the null hypothesis, it means that we simply haven’t collected enough evidence against the null hypothesis to disprove it. In classical hypothesis testing, there is no way to determine whether the null hypothesis is true. Hypothesis testing provides a means to quantify to what extent the data from our sample is in line with the null hypothesis. In order to quantify the concept of “sufficient evidence” we look at the theoretical distribution of the sample means given our null hypothesis and the sample standard error. Using the available information we can infer the sampling distribution for our null hypothesis. Recall that the standard deviation of the sampling distribution (i.e., the standard error of the mean) is given by \\(\\sigma_{\\bar x}={\\sigma \\over \\sqrt{n}}\\), and thus can be computed as follows: mean_pop &lt;- mean(hours) sigma &lt;- sd(hours) #population standard deviation n &lt;- 50 #sample size standard_error &lt;- sigma/sqrt(n) #standard error standard_error ## [1] 2.001639 Since we know from the central limit theorem that the sampling distribution is normal for large enough samples, we can now visualize the expected sampling distribution if our null hypothesis was in fact true (i.e., if the was no difference between the true population mean and the hypothesized mean of 10). We also know that 95% of the probability is within 1.96 standard deviations from the mean. Values higher than that are rather unlikely, if our hypothesis about the population mean was indeed true. This is shown by the shaded area, also known as the “rejection region”. To test our hypothesis that the population mean is equal to \\(10\\), let us take a random sample from the population. The mean listening time in the sample (black line) \\(\\bar x\\) is 18.59. We can already see from the graphic above that such a value is rather unlikely under the hypothesis that the population mean is \\(10\\). Intuitively, such a result would therefore provide evidence against our null hypothesis. But how could we quantify specifically how unlikely it is to obtain such a value and decide whether or not to reject the null hypothesis? Significance tests can be used to provide answers to these questions. 5.5.2 Statistical inference on a sample 5.5.2.1 Test statistic 5.5.2.1.1 z-scores Let’s go back to the sampling distribution above. We know that 95% of all values will fall within 1.96 standard deviations from the mean. So if we could express the distance between our sample mean and the null hypothesis in terms of standard deviations, we could make statements about the probability of getting a sample mean of the observed magnitude (or more extreme values). Essentially, we would like to know how many standard deviations (\\(\\sigma_{\\bar x}\\)) our sample mean (\\(\\bar x\\)) is away from the population mean if the null hypothesis was true (\\(\\mu_0\\)). This can be formally expressed as follows: \\[ \\bar x- \\mu_0 = z \\sigma_{\\bar x} \\] In this equation, z will tell us how many standard deviations the sample mean \\(\\bar x\\) is away from the null hypothesis \\(\\mu_0\\). Solving for z gives us: \\[ z = {\\bar x- \\mu_0 \\over \\sigma_{\\bar x}}={\\bar x- \\mu_0 \\over \\sigma / \\sqrt{n}} \\] This standardized value (or “z-score”) is also referred to as a test statistic. Let’s compute the test statistic for our example above: z_score &lt;- (mean_sample - H_0)/(sigma/sqrt(n)) z_score ## [1] 4.292454 To make a decision on whether the difference can be deemed statistically significant, we now need to compare this calculated test statistic to a meaningful threshold. In order to do so, we need to decide on a significance level \\(\\alpha\\), which expresses the probability of finding an effect that does not actually exist (i.e., Type I Error). You can find a detailed discussion of this point at the end of this chapter. For now, we will adopt the widely accepted significance level of 5% and set \\(\\alpha\\) to 0.05. The critical value for the normal distribution and \\(\\alpha\\) = 0.05 can be computed using the qnorm() function as follows: z_crit &lt;- qnorm(0.975) z_crit ## [1] 1.959964 We use 0.975 and not 0.95 since we are running a two-sided test and need to account for the rejection region at the other end of the distribution. Recall that for the normal distribution, 95% of the total probability falls within 1.96 standard deviations of the mean, so that higher (absolute) values provide evidence against the null hypothesis. Generally, we speak of a statistically significant effect if the (absolute) calculated test statistic is larger than the (absolute) critical value. We can easily check if this is the case in our example: abs(z_score) &gt; abs(z_crit) ## [1] TRUE Since the absolute value of the calculated test statistic is larger than the critical value, we would reject \\(H_0\\) and conclude that the true population mean \\(\\mu\\) is significantly different from the hypothesized value \\(\\mu_0 = 10\\). 5.5.2.1.2 t-statistic You may have noticed that the formula for the z-score above assumes that we know the true population standard deviation (\\(\\sigma\\)) when computing the standard deviation of the sampling distribution (\\(\\sigma_{\\bar x}\\)) in the denominator. However, the population standard deviation is usually not known in the real world and therefore represents another unknown population parameter which we have to estimate from the sample. We saw in the previous chapter that we usually use \\(s\\) as an estimate of \\(\\sigma\\) and \\(SE_{\\bar x}\\) as and estimate of \\(\\sigma_{\\bar x}\\). Intuitively, we should be more conservative regarding the critical value that we used above to assess whether we have a significant effect to reflect this uncertainty about the true population standard deviation. That is, the threshold for a “significant” effect should be higher to safeguard against falsely claiming a significant effect when there is none. If we replace \\(\\sigma_{\\bar x}\\) by it’s estimate \\(SE_{\\bar x}\\) in the formula for the z-score, we get a new test statistic (i.e, the t-statistic) with its own distribution (the t-distribution): \\[ t = {\\bar x- \\mu_0 \\over SE_{\\bar x}}={\\bar x- \\mu_0 \\over s / \\sqrt{n}} \\] Here, \\(\\bar X\\) denotes the sample mean and \\(s\\) the sample standard deviation. The t-distribution has more probability in its “tails”, i.e. farther away from the mean. This reflects the higher uncertainty introduced by replacing the population standard deviation by its sample estimate. Intuitively, this is particularly relevant for small samples, since the uncertainty about the true population parameters decreases with increasing sample size. This is reflected by the fact that the exact shape of the t-distribution depends on the degrees of freedom, which is the sample size minus one (i.e., \\(n-1\\)). To see this, the following graph shows the t-distribution with different degrees of freedom for a two-tailed test and \\(\\alpha = 0.05\\). The grey curve shows the normal distribution. Notice that as \\(n\\) gets larger, the t-distribution gets closer and closer to the normal distribution, reflecting the fact that the uncertainty introduced by \\(s\\) is reduced. To summarize, we now have an estimate for the standard deviation of the distribution of the sample mean (i.e., \\(SE_{\\bar x}\\)) and an appropriate distribution that takes into account the necessary uncertainty (i.e., the t-distribution). Let us now compute the t-statistic according to the formula above: SE &lt;- (sd(music_listening_sample$hours)/sqrt(n)) t_score &lt;- (mean_sample - H_0)/SE t_score ## [1] 4.84204 Notice that the value of the t-statistic is higher compared to the z-score (4.29). This can be attributed to the fact that by using the \\(s\\) as and estimate of \\(\\sigma\\), we underestimate the true population standard deviation. Hence, the critical value would need to be larger to adjust for this. This is what the t-distribution does. Let us compute the critical value from the t-distribution with n - 1degrees of freedom. df = n - 1 t_crit &lt;- qt(0.975, df = df) t_crit ## [1] 2.009575 Again, we use 0.975 and not 0.95 since we are running a two-sided test and need to account for the rejection region at the other end of the distribution. Notice that the new critical value based on the t-distributionis larger, to reflect the uncertainty when estimating \\(\\sigma\\) from \\(s\\). Now we can see that the calculated test statistic is still larger than the critical value. abs(t_score) &gt; abs(t_crit) ## [1] TRUE The following graphics shows that the calculated test statistic (red line) falls into the rejection region so that in our example, we would reject the null hypothesis that the true population mean is equal to \\(10\\). Decision: Reject \\(H_0\\), given that the calculated test statistic is larger than critical value. Something to keep in mind here is the fact the test statistic is a function of the sample size. This, as \\(n\\) gets large, the test statistic gets larger as well and we are more likely to find a significant effect. This reflects the decrease in uncertainty about the true population mean as our sample size increases. 5.5.2.2 P-values In the previous section, we computed the test statistic, which tells us how close our sample is to the null hypothesis. The p-value corresponds to the probability that the test statistic would take a value as extreme or more extreme than the one that we actually observed, assuming that the null hypothesis is true. It is important to note that this is a conditional probability: we compute the probability of observing a sample mean (or a more extreme value) conditional on the assumption that the null hypothesis is true. The pnorm()function can be used to compute this probability. It is the cumulative probability distribution function of the `normal distribution. Cumulative probability means that the function returns the probability that the test statistic will take a value less than or equal to the calculated test statistic given the degrees of freedom. However, we are interested in obtaining the probability of observing a test statistic larger than or equal to the calculated test statistic under the null hypothesis (i.e., the p-value). Thus, we need to subtract the cumulative probability from 1. In addition, since we are running a two-sided test, we need to multiply the probability by 2 to account for the rejection region at the other side of the distribution. p_value &lt;- 2*(1-pt(abs(t_score), df = df)) p_value ## [1] 1.326885e-05 This value corresponds to the probability of observing a mean equal to or larger than the one we obtained from our sample, if the null hypothesis was true. As you can see, this probability is very low. A small p-value signals that it is unlikely to observe the calculated test statistic under the null hypothesis. To decide whether or not to reject the null hypothesis, we would now compare this value to the level of significance (\\(\\alpha\\)) that we chose for our test. For this example, we adopt the widely accepted significance level of 5%, so any test results with a p-value &lt; 0.05 would be deemed statistically significant. Note that the p-value is directly related to the value of the test statistic. The relationship is such that the higher (lower) the value of the test statistic, the lower (higher) the p-value. Decision: Reject \\(H_0\\), given that the p-value is smaller than 0.05. 5.5.2.3 Confidence interval For a given statistic calculated for a sample of observations (e.g., listening times), a 95% confidence interval can be constructed such that in 95% of samples, the true value of the true population mean will fall within its limits. If the parameter value specified in the null hypothesis (here \\(10\\)) does not lie within the bounds, we reject \\(H_0\\). Building on what we learned about confidence intervals in the previous chapter, the 95% confidence interval based on the t-distribution can be computed as follows: \\[ CI_{lower} = {\\bar x} - t_{1-{\\alpha \\over 2}} * SE_{\\bar x} \\\\ CI_{upper} = {\\bar x} + t_{1-{\\alpha \\over 2}} * SE_{\\bar x} \\] It is easy to compute this interval manually: ci_lower &lt;- (mean_sample)-qt(0.975, df = df)*SE ci_upper &lt;- (mean_sample)+qt(0.975, df = df)*SE ci_lower ## [1] 15.02606 ci_upper ## [1] 22.15783 The interpretation of this interval is as follows: if we would (hypothetically) take 100 samples and calculated the mean and confidence interval for each of them, then the true population mean would be included in 95% of these intervals. The CI is informative when reporting the result of your test, since it provides an estimate of the uncertainty associated with the test result. From the test statistic or the p-value alone, it is not easy to judge in which range the true population parameter is located. The CI provides an estimate of this range. Decision: Reject \\(H_0\\), given that the parameter value from the null hypothesis (\\(10\\)) is not included in the interval. To summarize, you can see that we arrive at the same conclusion (i.e., reject \\(H_0\\)), irrespective if we use the test statistic, the p-value, or the confidence interval. However, keep in mind that rejecting the null hypothesis does not prove the alternative hypothesis (we can merely provide support for it). Rather, think of the p-value as the chance of obtaining the data we’ve collected assuming that the null hypothesis is true. You should report the confidence interval to provide an estimate of the uncertainty associated with your test results. 5.5.3 NHST considerations 5.5.3.1 Type I and Type II Errors When choosing the level of significance (\\(\\alpha\\)). It is important to note that the choice of the significance level affects the type 1 and type 2 error: Type I error: When we believe there is a genuine effect in our population, when in fact there isn’t. Probability of type I error (\\(\\alpha\\)) = level of significance = the probability of finding an effect that does not genuinely exist. Type II error: When we believe that there is no effect in the population, when in fact there is. This following table shows the possible outcomes of a test (retain vs. reject \\(H_0\\)), depending on whether \\(H_0\\) is true or false in reality.   Retain H0 Reject H0 H0 is true Correct decision:1-α (probability of correct retention); Type 1 error: α (level of significance) H0 is false Type 2 error:β (type 2 error rate) Correct decision:1-β (power of the test) 5.5.3.2 P-values From my experience, students tend to place a lot of weight on p-values when interpreting their research findings. It is therefore important to note some points that hopefully help to put the meaning of a “significant” vs. “insignificant” test result into perspective. So what does a significant test result actually tell us? The importance of an effect? → No, significance depends on sample size. That the null hypothesis is false? → No, it is always false. That the null hypothesis is true? → No, it is never true. It is important to understand what the p-value actually tells you. A p-value of &lt; 0.05 means that the probability of finding a difference of at least the observed magnitude is less than 5% if the null hypothesis was true. In other words, if there really wouldn’t be a difference between the groups, it tells you the probability of observing the difference that you found in your data (or more extreme differences). The following points provide some guidance on how to interpret significant and insignificant test results. Significant result Even if the probability of the effect being a chance result is small (e.g., less than .05) it doesn’t necessarily mean that the effect is important. Very small and unimportant effects can turn out to be statistically significant if the sample size is large enough. Insignificant result If the probability of the effect occurring by chance is large (greater than .05), the alternative hypothesis is rejected. However, this does not mean that the null hypothesis is true. Although an effect might not be large enough to be anything other than a chance finding, it doesn’t mean that the effect is zero. In fact, two random samples will always have slightly different means that would deemed to be statistically significant if the samples were large enough. Thus, you should not base your research conclusion on p-values alone! Learning check (LC5.1) What is the correct interpretation of a confidence interval for a significance level of \\(\\alpha\\)=0.05? If we take 100 samples and calculate mean and confidence interval for each one of them, then the true population mean would be included in 95% of these intervals. If we take 100 samples and calculate mean and confidence interval for each one of them, then the true population mean would be included in 5% of these intervals. If we take 100 samples and calculate mean and confidence interval for each one of them, then the true population mean would be included in 100% of these intervals. For a given sample, there is a 95% chance that the true population mean lies within the confidence interval. (LC5.2) Which statements regarding standard error are TRUE? There is no connection between the standard deviation and the standard error. The standard error is a function of the sample size and the standard deviation. The standard error of the mean decreases as the sample size increases. The standard error of the mean increases as the standard deviation increases. None of the above (LC5.3) What is the correct definition for the standard error (\\(SE_{\\bar x}\\))? \\({s \\over \\sqrt{n}}\\) \\({s * \\sqrt{n}}\\) \\({\\sqrt{s^2} \\over \\sqrt{n}}\\) \\({\\sqrt{s} \\over n}\\) None of the above (LC5.4) Which of the following do you need to compute a confidence interval around a sample mean? The critical value of the test statistic given a certain level of confidence A continuous variable (i.e., at least measured at the interval level) The sample the mean The standard error None of the above (LC5.5) What is the correct definition for the confidence interval? \\(CI=\\bar{x} \\pm \\frac{z_{1-\\frac{a}{n}}}{\\sigma_{\\bar{x}}}\\) \\(CI=\\bar{x} * z_{1-\\frac{a}{n}}*\\sigma_{\\bar{x}}\\) \\(CI= z_{1-\\frac{a}{n}}*\\sigma_{\\bar{x}}-\\bar{x}\\) \\(CI=\\bar{x} \\pm z_{1-\\frac{a}{n}}*\\sigma_{\\bar{x}}\\) None of the above As a marketing manager at Spotify you wish to find the average listening time of your users. Based on a random sample of 180 users you found that the mean listening time for the sample is 7.34 hours per week and the standard deviation is 6.87 hours. (LC5.6) What is the 95% confidence interval for the mean listening time (the corresponding z-value for the 95% CI is 1.96)? [6.34;8.34] [7.15;7.55] [6.25;8.15] [6.54;8.54] None of the above (LC5.7) How can you compute the standardized variate of a variable X? \\(z = {s \\over \\sqrt{n}}\\) \\(z = {X_i - \\bar{X} \\over s}\\) \\(z = {s \\over X_i-\\bar{X}}\\) $z = s * (X_i-{X}) $ (LC5.8) For a variable that follows a normal distribution, within how many standard deviations of the mean are 95% of values? 1.960 1.645 2.580 3.210 (LC5.9) The Null Hypothesis (\\(H_0\\)) is a statement of: The status-quo/no effect The desired status The expected status None of the above (LC5.10) Which statements about the Null Hypothesis (\\(H_0\\)) are TRUE? In scientific research, the goal is usually to confirm it In scientific research, the goal is usually to reject it It can be confirmed with one test None of the above (LC5.11) In which setting would you reject the null hypothesis when conducting a statistical test? When the absolute value of the calculated test-statistic (e.g., t-value) exceeds the critical value of the test statistic at your specified significance level (e.g., 0.05) When the p-value is smaller than your specified significance level (e.g., 0.05) When the confidence interval associated with the test does not contain zero When the test-statistic (e.g., t-value) is lower than the critical value of the test statistic at your specified significance level (e.g., 0.05) None of the above (LC5.12) What does a significant test result tell you? The importance of an effect That the null hypothesis is false That the null hypothesis is true None of the above References Field, A., Miles J., &amp; Field, Z. (2012). Discovering Statistics Using R. Sage Publications. Malhotra, N. K.(2010). Marketing Research: An Applied Orientation (6th. ed.). Prentice Hall. Vasishth, S. (2014). An introduction to statistical data analysis (lecture notes) "],["supervised-learning.html", "6 Supervised learning 6.1 Linear regression 6.2 Logistic regression Learning check References", " 6 Supervised learning 6.1 Linear regression 6.1.1 Correlation Before we start with regression analysis, we will review the basic concept of correlation first. Correlation helps us to determine the degree to which the variation in one variable, X, is related to the variation in another variable, Y. 6.1.1.1 Correlation coefficient The correlation coefficient summarizes the strength of the linear relationship between two metric (interval or ratio scaled) variables. Let’s consider a simple example. Say you conduct a survey to investigate the relationship between the attitude towards a shop and the duration of being of its customer. The “Attitude” variable can take values between 1 (very unfavorable) and 12 (very favorable), and the “Duration” is measured in months. Let’s further assume for this example that the attitude measurement represents an interval scale (although it is usually not realistic to assume that the scale points on an itemized rating scale have the same distance). To keep it simple, let’s further assume that you only asked 12 people. We can create a short data set like this: library(psych) attitude &lt;- c(6, 9, 8, 3, 10, 4, 5, 2, 11, 9, 10, 2) duration &lt;- c(10, 12, 12, 4, 12, 6, 8, 2, 18, 9, 17, 2) att_data &lt;- data.frame(attitude, duration) att_data &lt;- att_data[order(-attitude), ] att_data$respodentID &lt;- c(1:12) str(att_data) ## &#39;data.frame&#39;: 12 obs. of 3 variables: ## $ attitude : num 11 10 10 9 9 8 6 5 4 3 ... ## $ duration : num 18 12 17 12 9 12 10 8 6 4 ... ## $ respodentID: int 1 2 3 4 5 6 7 8 9 10 ... psych::describe(att_data[, c(&quot;attitude&quot;, &quot;duration&quot;)]) ## vars n mean sd median trimmed mad min max range skew kurtosis ## attitude 1 12 6.58 3.32 7.0 6.6 4.45 2 11 9 -0.14 -1.74 ## duration 2 12 9.33 5.26 9.5 9.2 4.45 2 18 16 0.10 -1.27 ## se ## attitude 0.96 ## duration 1.52 att_data ## attitude duration respodentID ## 9 11 18 1 ## 5 10 12 2 ## 11 10 17 3 ## 2 9 12 4 ## 10 9 9 5 ## 3 8 12 6 ## 1 6 10 7 ## 7 5 8 8 ## 6 4 6 9 ## 4 3 4 10 ## 8 2 2 11 ## 12 2 2 12 Let’s look at the data first. The following graph shows the individual data points for the “duration” variable, where the y-axis shows the duration of residency in years and the x-axis shows the respondent ID. The blue horizontal line represents the mean of the variable (9.33) and the vertical lines show the distance of the individual data points from the mean. Figure 1.3: Scores for duration variable You can see that there are some respondents that have been the store’s customers longer than average and some - shorter than average. Let’s do the same for the second variable (“Attitude”). Again, the y-axis shows the observed scores for this variable and the x-axis shows the respondent ID. Figure 1.4: Scores for attitude variable Again, we can see that some respondents have an above average attitude towards the store (more favorable) and some respondents have a below average attitude. Let’s combine both variables in one graph now to see if there is some co-movement: Figure 5.1: Scores for attitude and duration variables We can see that there is indeed some co-movement here. The variables covary because respondents who have an above (below) average attitude towards the store also appear to have been its customers for an above (below) average amount of time and vice versa. Correlation helps us to quantify this relationship. Before you proceed to compute the correlation coefficient, you should first look at the data. We usually use a scatterplot to visualize the relationship between two metric variables: Figure 1.5: Scatterplot for durationand attitute variables How can we compute the correlation coefficient? Remember that the variance measures the average deviation from the mean of a variable: \\[\\begin{equation} \\begin{split} s_x^2&amp;=\\frac{\\sum_{i=1}^{N} (X_i-\\overline{X})^2}{N-1} \\\\ &amp;= \\frac{\\sum_{i=1}^{N} (X_i-\\overline{X})*(X_i-\\overline{X})}{N-1} \\end{split} \\tag{6.1} \\end{equation}\\] When we consider two variables, we multiply the deviation for one variable by the respective deviation for the second variable: \\((X_i-\\overline{X})*(Y_i-\\overline{Y})\\) This is called the cross-product deviation. Then we sum the cross-product deviations: \\(\\sum_{i=1}^{N}(X_i-\\overline{X})*(Y_i-\\overline{Y})\\) … and compute the average of the sum of all cross-product deviations to get the covariance: \\[\\begin{equation} Cov(x, y) =\\frac{\\sum_{i=1}^{N}(X_i-\\overline{X})*(Y_i-\\overline{Y})}{N-1} \\tag{6.2} \\end{equation}\\] You can easily compute the covariance manually as follows x &lt;- att_data$duration x_bar &lt;- mean(att_data$duration) y &lt;- att_data$attitude y_bar &lt;- mean(att_data$attitude) N &lt;- nrow(att_data) cov &lt;- (sum((x - x_bar) * (y - y_bar)))/(N - 1) cov ## [1] 16.333333 Or you simply use the built-in cov() function: cov(att_data$duration, att_data$attitude) # apply the cov function ## [1] 16.333333 A positive covariance indicates that as one variable deviates from the mean, the other variable deviates in the same direction. A negative covariance indicates that as one variable deviates from the mean (e.g., increases), the other variable deviates in the opposite direction (e.g., decreases). However, the size of the covariance depends on the scale of measurement. Larger scale units will lead to larger covariance. To overcome the problem of dependence on measurement scale, we need to convert the covariance to a standard set of units through standardization by dividing the covariance by the standard deviation (similar to how we compute z-scores). With two variables, there are two standard deviations. We simply multiply the two standard deviations. We then divide the covariance by the product of the two standard deviations to get the standardized covariance, which is known as a correlation coefficient r: \\[\\begin{equation} r=\\frac{Cov_{xy}}{s_x*s_y} \\tag{6.3} \\end{equation}\\] This is known as the product moment correlation (r) and it is straight-forward to compute: x_sd &lt;- sd(att_data$duration) y_sd &lt;- sd(att_data$attitude) r &lt;- cov/(x_sd * y_sd) r ## [1] 0.93607782 Or you could just use the cor() function: cor(att_data[, c(&quot;attitude&quot;, &quot;duration&quot;)], method = &quot;pearson&quot;, use = &quot;complete&quot;) ## attitude duration ## attitude 1.00000000 0.93607782 ## duration 0.93607782 1.00000000 The properties of the correlation coefficient (‘r’) are: ranges from -1 to + 1 +1 indicates perfect linear relationship -1 indicates perfect negative relationship 0 indicates no linear relationship ± .1 represents small effect ± .3 represents medium effect ± .5 represents large effect 6.1.1.2 Significance testing How can we determine if our two variables are significantly related? To test this, we denote the population moment correlation ρ. Then we test the null of no relationship between variables: \\[H_0:\\rho=0\\] \\[H_1:\\rho\\ne0\\] The test statistic is: \\[\\begin{equation} t=\\frac{r*\\sqrt{N-2}}{\\sqrt{1-r^2}} \\tag{6.4} \\end{equation}\\] It has a t distribution with n - 2 degrees of freedom. You can simply use the cor.test() function, which also produces the 95% confidence interval: cor.test(att_data$attitude, att_data$duration, alternative = &quot;two.sided&quot;, method = &quot;pearson&quot;, conf.level = 0.95) ## ## Pearson&#39;s product-moment correlation ## ## data: att_data$attitude and att_data$duration ## t = 8.4144, df = 10, p-value = 0.000007545 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.7826041 0.9822815 ## sample estimates: ## cor ## 0.9360778 To determine the linear relationship between variables, the data only needs to be measured using interval scales. If you want to test the significance of the association, the sampling distribution needs to be normally distributed (we usually assume this when our data are normally distributed or when N is large). If parametric assumptions are violated, you should use non-parametric tests: Spearman’s correlation coefficient: requires ordinal data and ranks the data before applying Pearson’s equation. Kendall’s tau: use when N is small or the number of tied ranks is large. cor.test(att_data$attitude, att_data$duration, alternative = &quot;two.sided&quot;, method = &quot;spearman&quot;, conf.level = 0.95) ## ## Spearman&#39;s rank correlation rho ## ## data: att_data$attitude and att_data$duration ## S = 14.197, p-value = 0.000002183 ## alternative hypothesis: true rho is not equal to 0 ## sample estimates: ## rho ## 0.9503606 cor.test(att_data$attitude, att_data$duration, alternative = &quot;two.sided&quot;, method = &quot;kendall&quot;, conf.level = 0.95) ## ## Kendall&#39;s rank correlation tau ## ## data: att_data$attitude and att_data$duration ## z = 3.9095, p-value = 0.0000925 ## alternative hypothesis: true tau is not equal to 0 ## sample estimates: ## tau ## 0.8960287 Report the results: A Pearson product-moment correlation coefficient was computed to assess the relationship between the duration of being a customer of a store and the attitude toward this store. There was a positive correlation between the two variables, r = 0.936, n = 12, p &lt; 0.05. A scatterplot summarizes the results (Figure XY). A note on the interpretation of correlation coefficients: As we have already seen in Chapter 1, correlation coefficients give no indication of the direction of causality. In our example, we can conclude that the attitude toward the store is more positive as the months of being a customer increase. However, we cannot say that the duration causes the attitudes to be more positive. There are two main reasons for caution when interpreting correlations: Third-variable problem: there may be other unobserved factors that affect both the ‘attitude’ and the ‘duration’ variables Direction of causality: Correlations say nothing about which variable causes the other to change (reverse causality: attitudes may just as well cause the duration variable). 6.1.2 Regression analysis Correlations measure relationships between variables (i.e., how much two variables covary). Using regression analysis we can predict the outcome of a dependent variable (Y) from one or more independent variables (X). For example, we could be interested in how many products will we will sell if we increase the advertising expenditures by 1000 Euros? In regression analysis, we fit a model to our data and use it to predict the values of the dependent variable from one predictor variable (bivariate regression) or several predictor variables (multiple regression). The following table shows a comparison of correlation and regression analysis:   Correlation Regression Estimated coefficient Coefficient of correlation (bounded between -1 and +1) Regression coefficient (not bounded a priori) Interpretation Linear association between two variables; Association is bidirectional (Linear) relation between one or more independent variables and dependent variable; Relation is directional Role of theory Theory neither required nor testable Theory required and testable 6.1.2.1 Simple linear regression In simple linear regression, we assess the relationship between one dependent (regressand) and one independent (regressor) variable. The goal is to fit a line through a scatterplot of observations in order to find the line that best describes the data (scatterplot). Suppose you are a marketing research analyst at a big retail group and your task is to suggest, on the basis of historical data, a marketing plan for the next year that will maximize product sales. The product in question is beer brand Budweiser. The data set that is available to you includes information on the sales of Budweiser move_ounce (in ounces, FYI: 1 oz = 29,57 ml), prices price_ounce (in dollars per ounce), and several other variables: bonus buy - a price reduction if customers buy a certain quantity of a product (sale_B), price reduction in % (sale_S), and others. Let’s load and inspect the data first: regression &lt;- read.table(&quot;https://raw.githubusercontent.com/dariayudaeva/RMA2024/main/data/bud_store102.csv&quot;, sep = &quot;,&quot;, header = TRUE) # read in data str(regression) ## &#39;data.frame&#39;: 220 obs. of 22 variables: ## $ store : int 102 102 102 102 102 102 102 102 102 102 ... ## $ brand_id : int 26 26 26 26 26 26 26 26 26 26 ... ## $ brand : chr &quot;Budweiser&quot; &quot;Budweiser&quot; &quot;Budweiser&quot; &quot;Budweiser&quot; ... ## $ week : int 91 92 93 94 95 96 97 98 99 100 ... ## $ move_ounce : num 41704 44212 28748 26169 35581 ... ## $ price_ounce : num 4.67 4.58 4.58 4.53 4.54 ... ## $ sale_B : num 0.133 0.133 0.125 0 0 ... ## $ sale_C : int 0 0 0 0 0 0 0 0 0 0 ... ## $ sale_S : num 0 0 0 0 0 0 0 0 0 0 ... ## $ summove_ounce : num 158871900 158871900 158871900 158871900 158871900 ... ## $ nweeks : int 220 220 220 220 220 220 220 220 220 220 ... ## $ mean_marketshare: num 0.0893 0.0893 0.0893 0.0893 0.0893 ... ## $ sharerank : int 3 3 3 3 3 3 3 3 3 3 ... ## $ priclow : int 0 0 0 0 0 0 0 0 0 0 ... ## $ pricmed : int 1 1 1 1 1 1 1 1 1 1 ... ## $ prichigh : int 0 0 0 0 0 0 0 0 0 0 ... ## $ logprice_ounce : num 1.54 1.52 1.52 1.51 1.51 ... ## $ logmove_ounce : num 10.6 10.7 10.3 10.2 10.5 ... ## $ saledummy_B : int 1 1 1 0 0 0 0 0 0 0 ... ## $ saledummy_C : int 0 0 0 0 0 0 0 0 0 0 ... ## $ saledummy_S : int 0 0 0 0 0 0 0 0 0 0 ... ## $ promoweek : int 91 92 93 NA NA NA NA NA NA NA ... regression$store &lt;- as.factor(regression$store) #convert grouping variable to factor regression$brand_id &lt;- as.factor(regression$brand_id) #convert grouping variable to factor regression$saledummy_B &lt;- as.factor(regression$saledummy_B) #convert grouping variable to factor regression$saledummy_C &lt;- as.factor(regression$saledummy_C) #convert grouping variable to factor regression$saledummy_S &lt;- as.factor(regression$saledummy_S) #convert grouping variable to factor head(regression) psych::describe(regression) #descriptive statistics using psych ## vars n mean sd median trimmed ## store* 1 220 1.00 0.00 1.00 1.00 ## brand_id* 2 220 1.00 0.00 1.00 1.00 ## brand* 3 220 1.00 0.00 1.00 1.00 ## week 4 220 202.12 65.75 200.50 201.65 ## move_ounce 5 220 19713.01 7837.08 17868.00 18580.18 ## price_ounce 6 220 4.88 0.21 4.90 4.88 ## sale_B 7 220 0.20 0.15 0.19 0.19 ## sale_C 8 220 0.00 0.00 0.00 0.00 ## sale_S 9 220 0.00 0.02 0.00 0.00 ## summove_ounce 10 220 158871900.16 0.00 158871900.16 158871900.16 ## nweeks 11 220 220.00 0.00 220.00 220.00 ## mean_marketshare 12 220 0.09 0.00 0.09 0.09 ## sharerank 13 220 3.00 0.00 3.00 3.00 ## priclow 14 220 0.00 0.00 0.00 0.00 ## pricmed 15 220 1.00 0.00 1.00 1.00 ## prichigh 16 220 0.00 0.00 0.00 0.00 ## logprice_ounce 17 220 1.58 0.04 1.59 1.59 ## logmove_ounce 18 220 9.83 0.34 9.79 9.81 ## saledummy_B* 19 220 1.75 0.43 2.00 1.82 ## saledummy_C* 20 220 1.00 0.00 1.00 1.00 ## saledummy_S* 21 220 1.01 0.10 1.00 1.00 ## promoweek 22 167 212.28 62.15 217.00 212.78 ## mad min max range skew kurtosis ## store* 0.00 1.00 1.00 0.00 NaN NaN ## brand_id* 0.00 1.00 1.00 0.00 NaN NaN ## brand* 0.00 1.00 1.00 0.00 NaN NaN ## week 82.28 91.00 317.00 226.00 0.06 -1.20 ## move_ounce 5538.99 7992.00 71032.00 63040.00 2.32 9.32 ## price_ounce 0.20 4.30 5.45 1.15 -0.15 -0.19 ## sale_B 0.18 0.00 0.67 0.67 0.28 -0.59 ## sale_C 0.00 0.00 0.00 0.00 NaN NaN ## sale_S 0.00 0.00 0.21 0.21 10.68 115.22 ## summove_ounce 0.00 158871900.16 158871900.16 0.00 Inf NaN ## nweeks 0.00 220.00 220.00 0.00 NaN NaN ## mean_marketshare 0.00 0.09 0.09 0.00 -Inf NaN ## sharerank 0.00 3.00 3.00 0.00 NaN NaN ## priclow 0.00 0.00 0.00 0.00 NaN NaN ## pricmed 0.00 1.00 1.00 0.00 NaN NaN ## prichigh 0.00 0.00 0.00 0.00 NaN NaN ## logprice_ounce 0.04 1.46 1.70 0.24 -0.27 -0.15 ## logmove_ounce 0.32 8.99 11.17 2.18 0.67 0.86 ## saledummy_B* 0.00 1.00 2.00 1.00 -1.17 -0.62 ## saledummy_C* 0.00 1.00 1.00 0.00 NaN NaN ## saledummy_S* 0.00 1.00 2.00 1.00 10.27 104.03 ## promoweek 78.58 91.00 316.00 225.00 -0.08 -1.14 ## se ## store* 0.00 ## brand_id* 0.00 ## brand* 0.00 ## week 4.43 ## move_ounce 528.38 ## price_ounce 0.01 ## sale_B 0.01 ## sale_C 0.00 ## sale_S 0.00 ## summove_ounce 0.00 ## nweeks 0.00 ## mean_marketshare 0.00 ## sharerank 0.00 ## priclow 0.00 ## pricmed 0.00 ## prichigh 0.00 ## logprice_ounce 0.00 ## logmove_ounce 0.02 ## saledummy_B* 0.03 ## saledummy_C* 0.00 ## saledummy_S* 0.01 ## promoweek 4.81 As stated above, regression analysis may be used to relate a quantitative response (“dependent variable”) to one or more predictor variables (“independent variables”). In a simple linear regression, we have one dependent and one independent variable and we regress the dependent variable on the independent variable. Here are a few important questions that we might seek to address based on the data: Is there a relationship between prices and sales? How strong is the relationship between prices and sales? Which other variables contribute to sales? How accurately can we estimate the effect of each variable on sales? How accurately can we predict future sales? Is the relationship linear? Is there synergy among the advertising activities? We may use linear regression to answer these questions. We will see later that the interpretation of the results strongly depends on the goal of the analysis - whether you would like to simply predict an outcome variable or you would like to explain the causal effect of the independent variable on the dependent variable (see Chapter 1). Let’s start with the first question and investigate the relationship between advertising and sales. 6.1.2.1.1 Estimating the coefficients A simple linear regression model only has one predictor and can be written as: \\[\\begin{equation} Y=\\beta_0+\\beta_1X+\\epsilon \\tag{6.5} \\end{equation}\\] In our specific context, let’s consider only the influence of prices on sales for now: \\[\\begin{equation} Sales=\\beta_0+\\beta_1*price+\\epsilon \\tag{6.6} \\end{equation}\\] The word “price” represents data on advertising expenditures that we have observed and β1 (the “slope”“) represents the unknown relationship between prices and sales. It tells you by how much sales will increase or decrease for an additional dollar added to price. β0 (the”intercept”) is the number of sales we would expect if the price is set to 0. Note that the last assumption is highly theoretical: in the majority of real world scenarios, we never have such variables as prices, advertising expenditures, kilometers to the nearest store set to 0. Hence, it is incorrect to interpret the intercept like this. Together, β0 and β1 represent the model coefficients or parameters. The error term (ε) captures everything that we miss by using our model, including, (1) misspecifications (the true relationship might not be linear), (2) omitted variables (other variables might drive sales), and (3) measurement error (our measurement of the variables might be imperfect). Once we have used our training data to produce estimates for the model coefficients, we can predict future sales on the basis of a particular value of price by computing: \\[\\begin{equation} \\hat{Sales}=\\hat{\\beta_0}+\\hat{\\beta_1}*price \\tag{6.7} \\end{equation}\\] We use the hat symbol, ^, to denote the estimated value for an unknown parameter or coefficient, or to denote the predicted value of the response (sales). In practice, β0 and β1 are unknown and must be estimated from the data to make predictions. In the case of our pricing example, the data set consists of the prices and product sales for 220 weeks (n = 220). Our goal is to obtain coefficient estimates such that the linear model fits the available data well. In other words, we fit a line through the scatterplot of observations and try to find the line that best describes the data. The following graph shows the scatterplot for our data, where the black line shows the regression line. The grey vertical lines shows the difference between the predicted values (the regression line) and the observed values. This difference is referred to as the residuals (“e”). Figure 6.1: Ordinary least squares (OLS) The estimation of the regression function is based on the idea of the method of least squares (OLS = ordinary least squares). The first step is to calculate the residuals by subtracting the observed values from the predicted values. \\(e_i = Y_i-(\\beta_0+\\beta_1X_i)\\) This difference is then minimized by minimizing the sum of the squared residuals: \\[\\begin{equation} \\sum_{i=1}^{N} e_i^2= \\sum_{i=1}^{N} [Y_i-(\\beta_0+\\beta_1X_i)]^2\\rightarrow min! \\tag{6.8} \\end{equation}\\] ei: Residuals (i = 1,2,…,N) Yi: Values of the dependent variable (i = 1,2,…,N) β0: Intercept β1: Regression coefficient / slope parameters Xni: Values of the nth independent variables and the ith observation N: Number of observations This is also referred to as the residual sum of squares (RSS). Now we need to choose the values for β0 and β1 that minimize RSS. So how can we derive these values for the regression coefficient? The equation for β1 is given by: \\[\\begin{equation} \\hat{\\beta_1}=\\frac{COV_{XY}}{s_x^2} \\tag{6.9} \\end{equation}\\] The exact mathematical derivation of this formula is beyond the scope of this script, but the intuition is to calculate the first derivative of the squared residuals with respect to β1 and set it to zero, thereby finding the β1 that minimizes the term. Using the above formula, you can easily compute β1 using the following code: cov_y_x &lt;- cov(regression$price_ounce, regression$move_ounce) cov_y_x ## [1] -405.2959 var_x &lt;- var(regression$price_ounce) var_x ## [1] 0.04473198 beta_1 &lt;- cov_y_x/var_x beta_1 ## [1] -9060.539 The interpretation of β1 is as follows: For every extra dollar increase of the price, sales can be expected to decrease by -9060.539 oz, which is around 270 liters. Using the estimated coefficient for β1, it is easy to compute β0 (the intercept) as follows: \\[\\begin{equation} \\hat{\\beta_0}=\\overline{Y}-\\hat{\\beta_1}\\overline{X} \\tag{6.10} \\end{equation}\\] The R code for this is: beta_0 &lt;- mean(regression$move_ounce) - beta_1 * mean(regression$price_ounce) beta_0 ## [1] 63950.63 You may also verify this based on a scatterplot of the data. The following plot shows the scatterplot including the regression line, which is estimated using OLS. ggplot(regression, mapping = aes(price_ounce, move_ounce)) + geom_point(shape = 1) + geom_smooth(method = &quot;lm&quot;, fill = &quot;blue&quot;, alpha = 0.1) + labs(x = &quot;Price ($ per oz)&quot;, y = &quot;Sales (oz)&quot;) + theme_bw() Figure 1.16: Scatterplot The slope coefficient (β1) tells you by how much sales (on the y-axis) would decrease if the price (on the x-axis) is increased by one unit ($). 6.1.2.1.2 Significance testing In a next step, we assess if the effect of prices on sales is statistically significant. This means that we test the null hypothesis H0: “There is no relationship between prices and sales” versus the alternative hypothesis H1: “The is some relationship between prices and sales”. Or, to state this formally: \\[H_0:\\beta_1=0\\] \\[H_1:\\beta_1\\ne0\\] How can we test if the effect is statistically significant? Recall the generalized equation to derive a test statistic: \\[\\begin{equation} test\\ statistic = \\frac{effect}{error} \\tag{6.11} \\end{equation}\\] The effect is given by the β1 coefficient in this case. To compute the test statistic, we need to come up with a measure of uncertainty around this estimate (the error). This is because we use information from a sample to estimate the least squares line to make inferences regarding the regression line in the entire population. Since we only have access to one sample, the regression line will be slightly different every time we take a different sample from the population. This is sampling variation and it is perfectly normal! It just means that we need to take into account the uncertainty around the estimate, which is achieved by the standard error. Thus, the test statistic for our hypothesis is given by: \\[\\begin{equation} t = \\frac{\\hat{\\beta_1}}{SE(\\hat{\\beta_1})} \\tag{6.12} \\end{equation}\\] After calculating the test statistic, we compare its value to the values that we would expect to find if there was no effect based on the t-distribution. In a regression context, the degrees of freedom are given by N - p - 1 where N is the sample size and p is the number of predictors. In our case, we have 220 observations and one predictor. Thus, the degrees of freedom is 220 - 1 - 1 = 218. In the regression output below, R provides the exact probability of observing a t value of this magnitude (or larger) if the null hypothesis was true. This probability is the p-value. A small p-value indicates that it is unlikely to observe such a substantial association between the predictor and the outcome variable due to chance in the absence of any real association between the predictor and the outcome. To estimate the regression model in R, you can use the lm() function. Within the function, you first specify the dependent variable (“move_ounce”) and independent variable (“price_ounce”) separated by a ~ (tilde). As mentioned previously, this is known as formula notation in R. The data = regression argument specifies that the variables come from the data frame named “regression”. Strictly speaking, you use the lm() function to create an object called “sales_reg,” which holds the regression output. You can then view the results using the summary() function: sales_reg &lt;- lm(move_ounce ~ price_ounce, data = regression) #estimate linear model summary(sales_reg) #summary of results ## ## Call: ## lm(formula = move_ounce ~ price_ounce, data = regression) ## ## Residuals: ## Min 1Q Median 3Q Max ## -11439 -4798 -1539 2744 50733 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 63951 11892 5.377 0.000000194 *** ## price_ounce -9060 2434 -3.723 0.00025 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 7617 on 218 degrees of freedom ## Multiple R-squared: 0.05979, Adjusted R-squared: 0.05548 ## F-statistic: 13.86 on 1 and 218 DF, p-value: 0.0002504 Note that the estimated coefficients for β0 (63950.625) and β1 (-9060.539) correspond to the results of our manual computation above. The associated t-values and p-values are given in the output. The t-values are larger than the critical t-values for the 95% confidence level, since the associated p-values are smaller than 0.05. In case of the coefficient for β1, this means that the probability of an association between the prices and sales of the observed magnitude (or larger) is smaller than 0.05, if the value of β1 was, in fact, 0. This finding leads us to reject the null hypothesis of no association between prices and sales. The coefficients associated with the respective variables represent point estimates. To obtain a better understanding of the range of values that the coefficients could take, it is helpful to compute confidence intervals. A 95% confidence interval is defined as a range of values such that with a 95% probability, the range will contain the true unknown value of the parameter. For example, for β1, the confidence interval can be computed as. \\[\\begin{equation} CI = \\hat{\\beta_1}\\pm(t_{1-\\frac{\\alpha}{2}}*SE(\\beta_1)) \\tag{6.13} \\end{equation}\\] It is easy to compute confidence intervals in R using the confint() function. You just have to provide the name of you estimated model as an argument: confint(sales_reg) ## 2.5 % 97.5 % ## (Intercept) 40511.67 87389.58 ## price_ounce -13856.72 -4264.36 For our model, the 95% confidence interval for β0 is [40511.67,87389.58], and the 95% confidence interval for β1 is [-13856.72,-4264.36]. Thus, we can conclude that when we do not spend any money on advertising, sales will be somewhere between 40512 and 87390 units on average. In addition, for each increase in advertising expenditures by one Euro, there will be an average increase in sales of between -13856.72 and -4264.36. If you revisit the graphic depiction of the regression model above, the uncertainty regarding the intercept and slope parameters can be seen in the confidence bounds (blue area) around the regression line. 6.1.2.1.3 Assessing model fit Once we have rejected the null hypothesis in favor of the alternative hypothesis, the next step is to investigate how well the model represents (“fits”) the data. How can we assess the model fit? First, we calculate the fit of the most basic model (i.e., the mean) Then, we calculate the fit of the best model (i.e., the regression model) A good model should fit the data significantly better than the basic model R2: Represents the percentage of the variation in the outcome that can be explained by the model The F-ratio measures how much the model has improved the prediction of the outcome compared to the level of inaccuracy in the model Similar to ANOVA, the calculation of model fit statistics relies on estimating the different sum of squares values. SST is the difference between the observed data and the mean value of Y (aka. total variation). In the absence of any other information, the mean value of Y (\\(\\overline{Y}\\)) represents the best guess on where a particular observation \\(Y_{i}\\) at a given level of advertising will fall: \\[\\begin{equation} SS_T= \\sum_{i=1}^{N} (Y_i-\\overline{Y})^2 \\tag{6.14} \\end{equation}\\] The following graph shows the total sum of squares: Figure 1.19: Total sum of squares Based on our linear model, the best guess about the sales level at a given level of prices is the predicted value \\(\\hat{Y}_i\\). The model sum of squares (SSM) therefore has the mathematical representation: \\[\\begin{equation} SS_M= \\sum_{i=1}^{N} (\\hat{Y}_i-\\overline{Y})^2 \\tag{6.15} \\end{equation}\\] The model sum of squares represents the improvement in prediction resulting from using the regression model rather than the mean of the data. The following graph shows the model sum of squares for our example: Figure 1.20: Ordinary least squares (OLS) The residual sum of squares (SSR) is the difference between the observed data points (\\(Y_{i}\\)) and the predicted values along the regression line (\\(\\hat{Y}_{i}\\)), i.e., the variation not explained by the model. \\[\\begin{equation} SS_R= \\sum_{i=1}^{N} ({Y}_{i}-\\hat{Y}_{i})^2 \\tag{6.16} \\end{equation}\\] The following graph shows the residual sum of squares for our example: Figure 1.21: Ordinary least squares (OLS) Based on these statistics, we can determine how well the model fits the data as we will see next. R-squared The R2 statistic represents the proportion of variance that is explained by the model and is computed as: \\[\\begin{equation} R^2= \\frac{SS_M}{SS_T} \\tag{6.16} \\end{equation}\\] It takes values between 0 (very bad fit) and 1 (very good fit). Note that when the goal of your model is to predict future outcomes, a “too good” model fit can pose severe challenges. The reason is that the model might fit your specific sample so well, that it will only predict well within the sample but not generalize to other samples. This is called overfitting and it shows that there is a trade-off between model fit and out-of-sample predictive ability of the model, if the goal is to predict beyond the sample. We will come back to this point later in this chapter. You can get a first impression of the fit of the model by inspecting the scatter plot as can be seen in the plot below. If the observations are highly dispersed around the regression line (left plot), the fit will be lower compared to a data set where the values are less dispersed (right plot). Figure 1.22: Good vs. bad model fit The R2 statistic is reported in the regression output, so you don’t need to compute it manually. Adjusted R-squared Due to the way the R2 statistic is calculated, it will never decrease if a new explanatory variable is introduced into the model. This means that every new independent variable either doesn’t change the R2 or increases it, even if there is no real relationship between the new variable and the dependent variable. Hence, one could be tempted to just add as many variables as possible to increase the R2 and thus obtain a “better” model. However, this actually only leads to more noise and therefore a worse model. To account for this, there exists a test statistic closely related to the R2, the adjusted R2. It can be calculated as follows: \\[\\begin{equation} \\overline{R^2} = 1 - (1 - R^2)\\frac{n-1}{n - k - 1} \\tag{6.17} \\end{equation}\\] where n is the total number of observations and k is the total number of explanatory variables. The adjusted R2 is equal to or less than the regular R2 and can be negative. It will only increase if the added variable adds more explanatory power than one would expect by pure chance. Essentially, it contains a “penalty” for including unnecessary variables and therefore favors more parsimonious models. As such, it is a measure of suitability, good for comparing different models and is very useful in the model selection stage of a project. In R, the standard lm() function automatically also reports the adjusted R2 as you can see above. F-test Similar to the ANOVA, another significance test is the F-test, which tests the null hypothesis: \\[H_0:R^2=0\\] Or, to state it slightly differently: \\[H_0:\\beta_1=\\beta_2=\\beta_3=\\beta_k=0\\] This means that we test whether any of the included independent variables has a significant effect on the dependent variable. So far, we have only included one independent variable, but we will extend the set of predictor variables below. The F-test statistic is calculated as follows: \\[\\begin{equation} F=\\frac{\\frac{SS_M}{k}}{\\frac{SS_R}{(n-k-1)}}=\\frac{MS_M}{MS_R} \\tag{6.16} \\end{equation}\\] which has a F distribution with k number of predictors and n degrees of freedom. In other words, you divide the systematic (“explained”) variation due to the predictor variables by the unsystematic (“unexplained”) variation. The result of the F-test is provided in the regression output as well. However, you might manually compute the F-test using the ANOVA results from the model: f_calc &lt;- anova(sales_reg)$&quot;Mean Sq&quot;[1]/anova(sales_reg)$&quot;Mean Sq&quot;[2] #compute F f_calc ## [1] 14 f_crit &lt;- qf(0.95, df1 = 1, df2 = 100) #critical value f_crit ## [1] 3.9 f_calc &gt; f_crit #test if calculated test statistic is larger than critical value ## [1] TRUE 6.1.2.1.4 Using the model After fitting the model, we can use the estimated coefficients to predict sales of Budweiser for different values of prices. Suppose the store plans to set the price per ounce to 2 dollars. How much will it sell? You can easily compute this either by hand: \\[\\hat{sales}=63950.6 + (-9060.5)*2=45,829.6\\] … or by extracting the estimated coefficients from the model summary: prediction &lt;- summary(sales_reg)$coefficients[1, 1] + summary(sales_reg)$coefficients[2, 1] * 2 # the slope * 2 EUR prediction ## [1] 45830 The predicted value of the dependent variable is 45,829.6 oz, i.e., the store will sell around 45,829.6 oz (~1,355 liters) of Budweiser. 6.1.2.2 Log-Log transformation Have a look at the plots above again. You might notice some data specific pattern, making the data points look odd: they are pulled to lower edge of the scatterplot. In this particular case, we’re dealing with different measurement scales of our independent and dependent variables. Moreover, you could also notice how odd the interpretation of the regression coefficients sounds. It is very rare that in the retailing context, the predictions are made as we did before. The concept that is used instead is familiar to you from the microeconomics course - elasticity is a measure that is used by retail managers and researchers much more often than mere unit changes. Let’s have a look at the plot again: ggplot(regression, mapping = aes(price_ounce, move_ounce)) + geom_point(shape = 1) + geom_smooth(method = &quot;lm&quot;, color = &quot;lavenderblush4&quot;, fill = &quot;red&quot;, alpha = 0.1) + labs(x = &quot;Price (ounce)&quot;, y = &quot;Sales (ounce)&quot;) + theme_minimal() The way of obtaining a more reasonable view and interpretation in this case is called “multiplicative modeling”, or log-log transformation (you can find additional details about log-log transformations below with a slightly different motivation and example). The multiplicative model has the following formal representation: \\[\\begin{equation} Y =\\beta_0 *X_1^{\\beta_1}*X_2^{\\beta_2}*...*X_J^{\\beta_J}*\\epsilon \\tag{6.18} \\end{equation}\\] This functional form can be linearized by taking the logarithm of both sides of the equation: \\[\\begin{equation} log(Y) =log(\\beta_0) + \\beta_1*log(X_1) + \\beta_2*log(X_2) + ...+ \\beta_J*log(X_J) + log(\\epsilon) \\tag{6.19} \\end{equation}\\] This means that taking logarithms of both sides of the equation makes linear estimation possible. The above transformation follows from two logarithm rules that we apply here: the product rule states that \\(log(xy)=log(x)+log(y)\\); thus, when taking the logarithm of the right hand side of the multiplicative model, we can write \\(log(X_1) + log(X_2)... log(X_J)\\) instead of \\(log(X_1*X_2*...X_J)\\), and the power rule states that \\(log(x^y) = ylog(x)\\); thus, we can write \\(\\beta*log(X)\\) instead of \\(X^{\\beta}\\) Let’s test how the scatterplot would look like if we use the logarithm of our variables using the log() function instead of the original values. ggplot(regression, mapping = aes(log(price_ounce), log(move_ounce))) + geom_point(shape = 1) + geom_smooth(method = &quot;lm&quot;, color = &quot;lavenderblush4&quot;, fill = &quot;red&quot;, alpha = 0.1) + labs(x = &quot;Price&quot;, y = &quot;Sales&quot;) + theme_minimal() You can see how the scales changed, and how the observations got more normally distributed. Hence, we can log-transform our variables and estimate the following equation: \\[\\begin{equation} log(sales) = log(\\beta_0) + \\beta_1*log(price) + log(\\epsilon) \\tag{6.20} \\end{equation}\\] Now, let’s estimate a new regression by applying log() function to both sales and prices: sales_reg2 &lt;- lm(log(move_ounce) ~ log(price_ounce), data = regression) summary(sales_reg2) #remember that now the interpretation changed ## ## Call: ## lm(formula = log(move_ounce) ~ log(price_ounce), data = regression) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.8252 -0.2239 -0.0285 0.1818 1.3156 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 13.354 0.796 16.77 &lt; 0.0000000000000002 *** ## log(price_ounce) -2.225 0.502 -4.43 0.000015 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.32 on 218 degrees of freedom ## Multiple R-squared: 0.0826, Adjusted R-squared: 0.0784 ## F-statistic: 19.6 on 1 and 218 DF, p-value: 0.0000148 In this example, you would interpret the coefficient as follows: A 1% increase in price leads to a 2.23% decrease in sales. Hence, the interpretation is in proportional terms and no longer in units. This means that the coefficients in a log-log model can be directly interpreted as elasticities, which also makes communication easier. We can generally also inspect the R2 statistic to see that the model fit has increased compared to the linear specification (i.e., R2 has increased to 0.08 from 0.06). However, please note that the variables are now measured on a different scale, which means that the model fit in theory is not directly comparable. 6.1.2.3 Multiple linear regression Multiple linear regression is a statistical technique that simultaneously tests the relationships between two or more independent variables and an interval-scaled dependent variable. The general form of the equation is given by: \\[\\begin{equation} Y=(\\beta_0+\\beta_1*X_1+\\beta_2*X_2+\\beta_n*X_n)+\\epsilon \\tag{6.5} \\end{equation}\\] Again, we aim to find the combination of predictors that correlate maximally with the outcome variable. Note that if you change the composition of predictors, the partial regression coefficient of an independent variable will be different from that of the bivariate regression coefficient. This is because the regressors are usually correlated, and any variation in Y that was shared by X1 and X2 was attributed to X1. The interpretation of the partial regression coefficients is the expected change in Y when X is changed by one unit and all other predictors are held constant. Let’s extend the previous example. Say, in addition to the influence of price itself, you are interested in estimating the influence of two sales promotion techniques on the amount of Budweiser. The corresponding equation, including bonus buy and price reduction, would then be given by: \\[ Sales=\\beta_0+\\beta_1*price+\\beta_2*bonus\\_buy+\\beta_3*price\\_reduction+\\epsilon\\] β1, β2, and β3 represent the unknown relationship between sales and independent variables (price, bonus buy, and price reduction, respectively). The corresponding coefficients tell you by how much sales will change for an additional dollar increase of price (when the other IVs are held constant) and by how much sales will change for an additional unit of price reduction (when price itself and bonus buy are held constant), etc. Thus, we can make predictions about sales using all these variables. With several predictors, the partitioning of sum of squares is the same as in the bivariate model, except that the model is no longer a 2-D straight line. With two predictors, the regression line becomes a 3-D regression plane. While multiple regression models that have more than two predictors are not as easy to visualize, you may apply the same principles when interpreting the model outcome: Total sum of squares (SST) is still the difference between the observed data and the mean value of Y (total variation) Residual sum of squares (SSR) is still the difference between the observed data and the values predicted by the model (unexplained variation) Model sum of squares (SSM) is still the difference between the values predicted by the model and the mean value of Y (explained variation) R measures the multiple correlation between the predictors and the outcome R2 is the amount of variation in the outcome variable explained by the model Estimating multiple regression models is straightforward using the lm() function. You just need to separate the individual predictors on the right hand side of the equation using the + symbol. In addition, as discussed before, we would need to use log-log transformation for our use case, which can be done in multiple regression context as well. Hence, we would specify the model as follows (note that bonus buy and price reduction are already percentages in our data set, i.e., 0.2 value of price reduction is translated as 20% price decrease - hence, we don’t need to take an additional logarithm of it): \\[ log(Sales) =log(\\beta_0) + \\beta_1*log(Price) + \\beta_2*bonus\\_buy+\\beta_3*price\\_reduction+ log(\\epsilon) \\] This regression could be estimated as follows: multiple_sales_reg &lt;- lm(log(move_ounce) ~ log(price_ounce) + sale_B + sale_S, data = regression) # estimate the model summary(multiple_sales_reg) #summary of results ## ## Call: ## lm(formula = log(move_ounce) ~ log(price_ounce) + sale_B + sale_S, ## data = regression) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.7960 -0.2149 -0.0269 0.1871 1.2959 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 13.318 0.811 16.42 &lt; 0.0000000000000002 *** ## log(price_ounce) -2.221 0.508 -4.37 0.000019 *** ## sale_B 0.123 0.149 0.82 0.411 ## sale_S 3.033 1.223 2.48 0.014 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.32 on 216 degrees of freedom ## Multiple R-squared: 0.11, Adjusted R-squared: 0.0979 ## F-statistic: 8.92 on 3 and 216 DF, p-value: 0.0000134 The interpretation of the coefficients is as follows: price (β1): when price increases by 1%, sales will change by -2.22% bonus buy (β2): when bonus buy increases by 1%, sales will change by 0.12% price reduction (β3): when the price reduction increases by 1%, sales will change by 3.03% The associated t-values and p-values are also given in the output. You can see that the p-values are smaller than 0.05 for price and price reduction coefficients, while bonus sale is insignificant. Moreover, the p-value for F-test is smaller than 0.05. This means that if the null hypothesis was true (i.e., there was no effect between the variables and sales), the probability of observing associations of the estimated magnitudes (or larger) is very small (e.g., smaller than 0.05). Again, to get a better feeling for the range of values that the coefficients could take, it is helpful to compute confidence intervals. confint(multiple_sales_reg) ## 2.5 % 97.5 % ## (Intercept) 11.72 14.92 ## log(price_ounce) -3.22 -1.22 ## sale_B -0.17 0.42 ## sale_S 0.62 5.44 What does this tell you? Recall that a 95% confidence interval is defined as a range of values such that with a 95% probability, the range will contain the true unknown value of the parameter. For example, for β3, the confidence interval is [0.62,5.44]. Thus, although we have computed a point estimate of 3.03 for the effect of price reduction on sales based on our sample, the effect might actually just as well take any other value within this range, considering the sample size and the variability in our data. You could also visualize the output from your regression model including the confidence intervals using the ggstatsplot package as follows: library(ggstatsplot) ggcoefstats(x = multiple_sales_reg, title = &quot;Sales predicted by price, bonus buy, and price reduction&quot;) Figure 4.2: Confidence intervals for regression model The output also tells us that 11.03% of the variation can be explained by our model. You may also visually inspect the fit of the model by plotting the predicted values against the observed values. We can extract the predicted values using the predict() function. So let’s create a new variable yhat, which contains those predicted values. regression$logmove_ounce_hat &lt;- fitted(multiple_sales_reg) We can now use this variable to plot the predicted values against the observed values. In the following plot, the model fit would be perfect if all points would fall on the diagonal line. The larger the distance between the points and the line, the worse the model fit. In other words, if all points would fall exactly on the diagonal line, the model would perfectly predict the observed values. ggplot(data = regression, aes(week, log(move_ounce))) + geom_vline(xintercept = regression$promoweek, colour = &quot;lightgrey&quot;) + geom_line(aes(y = log(move_ounce), colour = &quot;logsales&quot;), size = 1) + geom_line(aes(y = (logmove_ounce_hat), colour = &quot;logsales (predicted)&quot;), size = 1) + scale_color_manual(values = c(&quot;black&quot;, &quot;gold&quot;)) + theme_minimal() Figure 4.4: Model fit Partial plots In the context of a simple linear regression (i.e., with a single independent variable), a scatter plot of the dependent variable against the independent variable provides a good indication of the nature of the relationship. If there is more than one independent variable, however, things become more complicated. The reason is that although the scatter plot still show the relationship between the two variables, it does not take into account the effect of the other independent variables in the model. Partial regression plot show the effect of adding another variable to a model that already controls for the remaining variables in the model. In other words, it is a scatterplot of the residuals of the outcome variable and each predictor when both variables are regressed separately on the remaining predictors. In our example, the partial plot would show the effect of adding price as an explanatory variables while controlling for the variation that is explained by sales promotions in both variables (sales and price). Think of it as the purified relationship between price and sales that remains after controlling for other factors. The partial plots can easily be created using the avPlots() function from the car package: library(car) avPlots(multiple_sales_reg) Figure 6.2: Partial plots 6.1.3 Categorical predictors 6.1.3.1 Two categories categories &lt;- read.table(&quot;https://raw.githubusercontent.com/WU-RDS/RMA2024/main/data/beer_categorical&quot;, sep = &quot;,&quot;, header = TRUE) categories$store &lt;- as.factor(categories$store) categories$brand &lt;- as.factor(categories$brand) str(categories) ## &#39;data.frame&#39;: 3194 obs. of 19 variables: ## $ store : Factor w/ 2 levels &quot;98&quot;,&quot;100&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ brand_id : int 2 2 2 2 2 2 2 2 2 2 ... ## $ brand : Factor w/ 6 levels &quot;Amstel&quot;,&quot;Budweiser&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ week : int 234 235 237 238 239 240 241 242 243 245 ... ## $ move_ounce : num 763.2 741.6 259.2 165.6 21.6 ... ## $ price_ounce : num 0.0773 0.0773 0.0887 0.0918 0.0921 ... ## $ sale_B : num 0.5 0.5 0 0 0 ... ## $ sale_C : int 0 0 0 0 0 0 0 0 0 0 ... ## $ sale_S : num 0 0 0 0 0 0 0 0 0 0 ... ## $ summove_ounce : num 6071191 6071191 6071191 6071191 6071191 ... ## $ mean_marketshare: num 0.00341 0.00341 0.00341 0.00341 0.00341 ... ## $ sharerank : int 24 24 24 24 24 24 24 24 24 24 ... ## $ priclow : int 0 0 0 0 0 0 0 0 0 0 ... ## $ pricmed : int 1 1 1 1 1 1 1 1 1 1 ... ## $ prichigh : int 0 0 0 0 0 0 0 0 0 0 ... ## $ saledummy_B : int 1 1 0 0 0 0 0 0 0 1 ... ## $ saledummy_C : int 0 0 0 0 0 0 0 0 0 0 ... ## $ saledummy_S : int 0 0 0 0 0 0 0 0 0 0 ... ## $ promoweek : int 234 235 NA NA NA NA NA NA NA 245 ... We will use a slightly different data set to explore additional opportunities for regression analysis. Suppose, you wish to investigate the effect of the variable “store” on sales, which is a categorical variable that can only take two levels (i.e., 98 = store with ID 98, and 100 = store with ID 100). Categorical variables with two levels are also called binary predictors; in our example, however, they are not decoded into typical binary view (i.e., they are not 0 and 1). It is straightforward to include these variables in your model as “dummy” variables. Dummy variables are factor variables that can only take two values. For our “store” variable, we can create a new predictor variable that takes the form: \\[\\begin{equation} x_4 = \\begin{cases} 0 &amp; \\quad \\text{if } i \\text{th observation comes from store 98}\\\\ 1 &amp; \\quad \\text{if } i \\text{th observation comes from store 100} \\end{cases} \\tag{6.21} \\end{equation}\\] This new variable is then added to our regression equation from before, so that the equation becomes \\[\\begin{align} Sales =\\beta_0 &amp;+\\beta_1*price\\\\ &amp;+\\beta_2*bonus\\_buy\\\\ &amp;+\\beta_3*price\\_reduction\\\\ &amp;+\\beta_4*store+\\epsilon \\end{align}\\] where “store” represents the new dummy variable and is the coefficient associated with this variable. Estimating the model is straightforward - you just need to include the variable as an additional predictor variable. Note that the variable needs to be specified as a factor variable before including it in your model. If you haven’t converted it to a factor variable before, you could also use the wrapper function as.factor() within the equation. First, let’s reestimate the regression we had before (note that the result slightly changes because we are using a different data set - you can recall that with different samples, the estimation of true value changes). For the sake of easier interpretation, we use regular regression specification (i.e., not log-log transformed). multiple_regression_new &lt;- lm(move_ounce ~ price_ounce + sale_B + sale_S, data = categories) summary(multiple_regression_new) ## ## Call: ## lm(formula = move_ounce ~ price_ounce + sale_B + sale_S, data = categories) ## ## Residuals: ## Min 1Q Median 3Q Max ## -10570 -2915 -1414 407 56446 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 14706 458 32.08 &lt;0.0000000000000002 *** ## price_ounce -146658 5991 -24.48 &lt;0.0000000000000002 *** ## sale_B 601 457 1.32 0.19 ## sale_S -2340 3015 -0.78 0.44 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 7430 on 3190 degrees of freedom ## Multiple R-squared: 0.16, Adjusted R-squared: 0.159 ## F-statistic: 202 on 3 and 3190 DF, p-value: &lt;0.0000000000000002 Now, let’s add the store variable: multiple_regression_store &lt;- lm(move_ounce ~ price_ounce + sale_B + sale_S + store, data = categories) summary(multiple_regression_store) ## ## Call: ## lm(formula = move_ounce ~ price_ounce + sale_B + sale_S + store, ## data = categories) ## ## Residuals: ## Min 1Q Median 3Q Max ## -10710 -3135 -1317 678 55579 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 13844 474 29.22 &lt; 0.0000000000000002 *** ## price_ounce -147160 5952 -24.73 &lt; 0.0000000000000002 *** ## sale_B 666 454 1.47 0.14 ## sale_S -2428 2995 -0.81 0.42 ## store100 1725 261 6.60 0.000000000047 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 7380 on 3189 degrees of freedom ## Multiple R-squared: 0.171, Adjusted R-squared: 0.17 ## F-statistic: 165 on 4 and 3189 DF, p-value: &lt;0.0000000000000002 You can see that we now have an additional coefficient in the regression output, which tells us the effect of the dummy predictor. The dummy variable can generally be interpreted as the average difference in the dependent variable between the two groups, conditional on the other variables you have included in your model. In this case, the coefficient tells you the difference in sales between store 98 and 100 artists, and whether this difference is significant. Specifically, it means that sales in store 100 are on average 1,724.92 oz higher than in store 98, and this difference is significant (i.e., p &lt; 0.05). 6.1.3.2 More than two categories Predictors with more than two categories, like our “brand”” variable, can also be included in your model. However, in this case one dummy variable cannot represent all possible values, since there are many brands (i.e., 1 = Amstel, 2 = Budweiser, 3 = Corona, 4 = Fosters, 5 = Heineken, 6 = Old Milwaukee). Thus, we need to create additional dummy variables. For example, for our “brand” variable, we create five dummy variables as follows: \\[\\begin{equation} x_5 = \\begin{cases} 1 &amp; \\quad \\text{if } i \\text{th product is Budweiser}\\\\ 0 &amp; \\quad \\text{if } i \\text{th product is Amstel} \\end{cases} \\tag{6.22} \\end{equation}\\] \\[\\begin{equation} x_6 = \\begin{cases} 1 &amp; \\quad \\text{if } i \\text{th product is Corona}\\\\ 0 &amp; \\quad \\text{if } i \\text{th product is Amstel} \\end{cases} \\tag{6.23} \\end{equation}\\] and so on. We would then add these variables as additional predictors in the regression equation and obtain the following model \\[\\begin{align} Sales =\\beta_0 &amp;+\\beta_1*price\\\\ &amp;+\\beta_2*bonus\\_buy\\\\ &amp;+\\beta_3*price\\_reduction\\\\ &amp;+\\beta_4*store\\\\ &amp;+\\beta_5*Budweiser\\\\ &amp;+\\beta_6*Corona\\\\ &amp;+\\beta_7*Fosters\\\\ &amp;+\\beta_8*Heineken\\\\ &amp;+\\beta_9*Old\\_Milwaukee+\\epsilon \\end{align}\\] where “Budweiser”, “Corona”, “Fosters”, “Heineken”, and “Old Milwaukee” represent our new dummy variables, and refer to the associated regression coefficients. You don’t have to create the dummy variables manually as R will do this automatically when you add the variable to your equation. The interpretation of the coefficients is as follows: \\(\\beta_5\\) is the difference in average sales between the brands “Amstel” and “Budweiser”, \\(\\beta_6\\) is the difference in average sales between the brands “Amstel” and “Corona”, and so on. Note that the level for which no dummy variable is created is also referred to as the baseline. In our case, “Amstel” would be the baseline brand. This means that there will always be one fewer dummy variable than the number of levels. multiple_regression_ext &lt;- lm(move_ounce ~ price_ounce + sale_B + sale_S + store + brand, data = categories) summary(multiple_regression_ext) ## ## Call: ## lm(formula = move_ounce ~ price_ounce + sale_B + sale_S + store + ## brand, data = categories) ## ## Residuals: ## Min 1Q Median 3Q Max ## -13305 -1179 -381 1091 40849 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1614 1372 -1.18 0.2395 ## price_ounce 7220 15159 0.48 0.6339 ## sale_B 1235 307 4.03 0.00005819767165529 *** ## sale_S 1661 1496 1.11 0.2669 ## store100 1858 129 14.44 &lt; 0.0000000000000002 *** ## brandBudweiser 22299 619 36.05 &lt; 0.0000000000000002 *** ## brandCorona 2289 237 9.66 &lt; 0.0000000000000002 *** ## brandFosters -110 240 -0.46 0.6472 ## brandHeinekenBeer 1815 220 8.25 0.00000000000000022 *** ## brandOldMilwaukee 2584 838 3.08 0.0021 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3620 on 3184 degrees of freedom ## Multiple R-squared: 0.801, Adjusted R-squared: 0.8 ## F-statistic: 1.42e+03 on 9 and 3184 DF, p-value: &lt;0.0000000000000002 How can we interpret the coefficients? It is estimated based on our model that products from the “Budweiser” brand will on average sell 22,298.63 oz more than products from the “Amstel” brand, and that products from the “Corona” brand will sell on average 2,288.88 oz more than the products from the “Amstel” brand, etc. The p-value of both these and some other brand-variables is smaller than 0.05, suggesting that there is statistical evidence for a real difference in sales between the brands The level of the baseline category is arbitrary. As you have seen, R simply selects the first level as the baseline. If you would like to use a different baseline category, you can use the relevel() function and set the reference category using the ref argument. The following would estimate the same model using the second category as the baseline: multiple_regression_ext &lt;- lm(move_ounce ~ price_ounce + sale_B + sale_S + store + relevel(brand, ref = 2), data = categories) summary(multiple_regression_ext) ## ## Call: ## lm(formula = move_ounce ~ price_ounce + sale_B + sale_S + store + ## relevel(brand, ref = 2), data = categories) ## ## Residuals: ## Min 1Q Median 3Q Max ## -13305 -1179 -381 1091 40849 ## ## Coefficients: ## Estimate Std. Error t value ## (Intercept) 20684 811 25.51 ## price_ounce 7220 15159 0.48 ## sale_B 1235 307 4.03 ## sale_S 1661 1496 1.11 ## store100 1858 129 14.44 ## relevel(brand, ref = 2)Amstel -22299 619 -36.05 ## relevel(brand, ref = 2)Corona -20010 534 -37.50 ## relevel(brand, ref = 2)Fosters -22409 616 -36.36 ## relevel(brand, ref = 2)HeinekenBeer -20484 625 -32.77 ## relevel(brand, ref = 2)OldMilwaukee -19714 329 -59.88 ## Pr(&gt;|t|) ## (Intercept) &lt; 0.0000000000000002 *** ## price_ounce 0.63 ## sale_B 0.000058 *** ## sale_S 0.27 ## store100 &lt; 0.0000000000000002 *** ## relevel(brand, ref = 2)Amstel &lt; 0.0000000000000002 *** ## relevel(brand, ref = 2)Corona &lt; 0.0000000000000002 *** ## relevel(brand, ref = 2)Fosters &lt; 0.0000000000000002 *** ## relevel(brand, ref = 2)HeinekenBeer &lt; 0.0000000000000002 *** ## relevel(brand, ref = 2)OldMilwaukee &lt; 0.0000000000000002 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3620 on 3184 degrees of freedom ## Multiple R-squared: 0.801, Adjusted R-squared: 0.8 ## F-statistic: 1.42e+03 on 9 and 3184 DF, p-value: &lt;0.0000000000000002 Note that while your choice of the baseline category impacts the coefficients and the significance level, the prediction for each group will be the same regardless of this choice. 6.1.3.3 Non-linear relationships 6.1.3.3.1 Multiplicative model In many practical applications, linear relationship might not be the case. Let’s review the implications of a linear specification again: Constant marginal returns (e.g., an increase in ad-spend from 10€ to 11€ yields the same increase in sales as an increase from 100,000€ to 100,001€) Elasticities increase with X (e.g., advertising becomes relatively more effective; i.e., a relatively smaller change in advertising expenditure will yield the same return) In many marketing contexts, these might not be reasonable assumptions. Consider the case of advertising. It is unlikely that the return on advertising will not depend on the level of advertising expenditures. It is rather likely that saturation occurs at some level, meaning that the return from an additional Euro spend on advertising is decreasing with the level of advertising expenditures (i.e., decreasing marginal returns). In other words, at some point the advertising campaign has achieved a certain level of penetration and an additional Euro spend on advertising won’t yield the same return as in the beginning. Let’s use an example data set, containing the advertising expenditures of a company and the sales (in thousand units). non_linear_reg &lt;- read.table(&quot;https://raw.githubusercontent.com/IMSMWU/Teaching/master/MRDA2017/non_linear.dat&quot;, sep = &quot;\\t&quot;, header = TRUE) #read in data head(non_linear_reg) Now we inspect if a linear specification is appropriate by looking at the scatterplot: ggplot(data = non_linear_reg, aes(x = advertising, y = sales)) + geom_point(shape = 1) + geom_smooth(method = &quot;lm&quot;, fill = &quot;blue&quot;, alpha = 0.1) + theme_bw() Figure 4.9: Non-linear relationship It appears that a linear model might not represent the data well. It rather appears that the effect of an additional Euro spend on advertising is decreasing with increasing levels of advertising expenditures. Thus, we have decreasing marginal returns. We could put this to a test and estimate a linear model: linear_reg &lt;- lm(sales ~ advertising, data = non_linear_reg) summary(linear_reg) ## ## Call: ## lm(formula = sales ~ advertising, data = non_linear_reg) ## ## Residuals: ## Min 1Q Median 3Q Max ## -10.477 -2.389 -0.356 2.188 16.745 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 9.9575216 0.2251151 44.2 &lt;0.0000000000000002 *** ## advertising 0.0005024 0.0000156 32.2 &lt;0.0000000000000002 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.6 on 998 degrees of freedom ## Multiple R-squared: 0.509, Adjusted R-squared: 0.509 ## F-statistic: 1.04e+03 on 1 and 998 DF, p-value: &lt;0.0000000000000002 Advertising appears to be positively related to sales with an additional Euro that is spent on advertising resulting in 0.0005 additional sales. The R2 statistic suggests that approximately 51% of the total variation can be explained by the model To test if the linear specification is appropriate, let’s inspect some of the plots that are generated by R. We start by inspecting the residuals plot. plot(linear_reg, 1) Figure 6.3: Residuals vs. Fitted The plot suggests that the assumption of homoscedasticity is violated (i.e., the spread of values on the y-axis is different for different levels of the fitted values). In addition, the red line deviates from the dashed grey line, suggesting that the relationship might not be linear. Finally, the Q-Q plot of the residuals suggests that the residuals are not normally distributed. plot(linear_reg, 2) Figure 6.4: Q-Q plot To sum up, a linear specification might not be the best model for this data set. In this case, a multiplicative model might be a better representation of the data. The multiplicative model has the following formal representation: \\[\\begin{equation} Y =\\beta_0 *X_1^{\\beta_1}*X_2^{\\beta_2}*...*X_J^{\\beta_J}*\\epsilon \\tag{6.18} \\end{equation}\\] This functional form can be linearized by taking the logarithm of both sides of the equation: \\[\\begin{equation} log(Y) =log(\\beta_0) + \\beta_1*log(X_1) + \\beta_2*log(X_2) + ...+ \\beta_J*log(X_J) + log(\\epsilon) \\tag{6.19} \\end{equation}\\] This means that taking logarithms of both sides of the equation makes linear estimation possible. The above transformation follows from two logarithm rules that we apply here: the product rule states that \\(log(xy)=log(x)+log(y)\\); thus, when taking the logarithm of the right hand side of the multiplicative model, we can write \\(log(X_1) + log(X_2)... log(X_J)\\) instead of \\(log(X_1*X_2*...X_J)\\), and the power rule states that \\(log(x^y) = ylog(x)\\); thus, we can write \\(\\beta*log(X)\\) instead of \\(X^{\\beta}\\) Let’s test how the scatterplot would look like if we use the logarithm of our variables using the log() function instead of the original values. ggplot(data = non_linear_reg, aes(x = log(advertising), y = log(sales))) + geom_point(shape = 1) + geom_smooth(method = &quot;lm&quot;, fill = &quot;blue&quot;, alpha = 0.1) + theme_bw() Figure 4.10: Linearized effect It appears that now, with the log-transformed variables, a linear specification is a much better representation of the data. Hence, we can log-transform our variables and estimate the following equation: \\[\\begin{equation} log(sales) = log(\\beta_0) + \\beta_1*log(advertising) + log(\\epsilon) \\tag{6.20} \\end{equation}\\] This can be easily implemented in R by transforming the variables using the log() function: log_reg &lt;- lm(log(sales) ~ log(advertising), data = non_linear_reg) summary(log_reg) ## ## Call: ## lm(formula = log(sales) ~ log(advertising), data = non_linear_reg) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.666 -0.127 0.003 0.134 0.640 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.01493 0.05971 -0.25 0.8 ## log(advertising) 0.30077 0.00651 46.20 &lt;0.0000000000000002 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2 on 998 degrees of freedom ## Multiple R-squared: 0.681, Adjusted R-squared: 0.681 ## F-statistic: 2.13e+03 on 1 and 998 DF, p-value: &lt;0.0000000000000002 Note that this specification implies decreasing marginal returns (i.e., the returns of advertising are decreasing with the level of advertising), which appear to be more consistent with the data. The specification is also consistent with proportional changes in advertising being associated with proportional changes in sales (i.e., advertising does not become more effective with increasing levels). This has important implications on the interpretation of the coefficients. In our example, you would interpret the coefficient as follows: A 1% increase in advertising leads to a 0.3% increase in sales. Hence, the interpretation is in proportional terms and no longer in units. This means that the coefficients in a log-log model can be directly interpreted as elasticities, which also makes communication easier. We can generally also inspect the R2 statistic to see that the model fit has increased compared to the linear specification (i.e., R2 has increased to 0.681 from 0.509). However, please note that the variables are now measured on a different scale, which means that the model fit in theory is not directly comparable. Also, we could use the residuals plot to confirm that the revised specification is more appropriate: plot(log_reg, 1) Figure 6.5: Residuals plot plot(log_reg, 2) Figure 6.6: Q-Q plot Finally, we can plot the predicted values against the observed values to see that the results from the log-log model (red) provide a better prediction than the results from the linear model (blue). non_linear_reg$pred_lin_reg &lt;- predict(linear_reg) non_linear_reg$pred_log_reg &lt;- predict(log_reg) ggplot(data = non_linear_reg) + geom_point(aes(x = advertising, y = sales), shape = 1) + geom_line(data = non_linear_reg, aes(x = advertising, y = pred_lin_reg), color = &quot;blue&quot;, size = 1.05) + geom_line(data = non_linear_reg, aes(x = advertising, y = exp(pred_log_reg)), color = &quot;red&quot;, size = 1.05) + theme_bw() Figure 4.12: Comparison if model fit 6.2 Logistic regression 6.2.1 Motivation and intuition In the last section we saw how to predict continuous outcomes (e.g., sales) via linear regression models. Another interesting case is that of binary outcomes, i.e. when the variable we want to model can only take two values (yes or no, group 1 or group 2, dead or alive, etc.). To this end we would like to estimate how our predictor variables change the probability of a value being 0 or 1. In this case we can technically still use a linear model (e.g. OLS). However, its predictions will most likely not be particularly useful. A more useful method is the logistic regression. In particular we are going to have a look at the logit model. In the following dataset we are trying to predict whether a customer will churn (i.e., stop being our customer) any time soon. In the first step we are going to use only the “cash-back amount” index as a predictor. Later we are going to add more independent variables. library(ggplot2) library(gridExtra) churn_data &lt;- read.csv(&quot;https://raw.githubusercontent.com/WU-RDS/RMA2024/main/data/e_com_data.csv&quot;, sep = &quot;,&quot;, header = T) head(churn_data) str(churn_data) ## &#39;data.frame&#39;: 5630 obs. of 20 variables: ## $ CustomerID : int 50001 50002 50003 50004 50005 50006 50007 50008 50009 50010 ... ## $ Churn : int 1 1 1 1 1 1 1 1 1 1 ... ## $ Tenure : int 4 NA NA 0 0 0 NA NA 13 NA ... ## $ PreferredLoginDevice : chr &quot;Mobile Phone&quot; &quot;Phone&quot; &quot;Phone&quot; &quot;Phone&quot; ... ## $ CityTier : int 3 1 1 3 1 1 3 1 3 1 ... ## $ WarehouseToHome : int 6 8 30 15 12 22 11 6 9 31 ... ## $ PreferredPaymentMode : chr &quot;Debit Card&quot; &quot;UPI&quot; &quot;Debit Card&quot; &quot;Debit Card&quot; ... ## $ Gender : chr &quot;Female&quot; &quot;Male&quot; &quot;Male&quot; &quot;Male&quot; ... ## $ HourSpendOnApp : int 3 3 2 2 NA 3 2 3 NA 2 ... ## $ NumberOfDeviceRegistered : int 3 4 4 4 3 5 3 3 4 5 ... ## $ PreferedOrderCat : chr &quot;Laptop &amp; Accessory&quot; &quot;Mobile&quot; &quot;Mobile&quot; &quot;Laptop &amp; Accessory&quot; ... ## $ SatisfactionScore : int 2 3 3 5 5 5 2 2 3 3 ... ## $ MaritalStatus : chr &quot;Single&quot; &quot;Single&quot; &quot;Single&quot; &quot;Single&quot; ... ## $ NumberOfAddress : int 9 7 6 8 3 2 4 3 2 2 ... ## $ Complain : int 1 1 1 0 0 1 0 1 1 0 ... ## $ OrderAmountHikeFromlastYear: int 11 15 14 23 11 22 14 16 14 12 ... ## $ CouponUsed : int 1 0 0 0 1 4 0 2 0 1 ... ## $ OrderCount : int 1 1 1 1 1 6 1 2 1 1 ... ## $ DaySinceLastOrder : int 5 0 3 3 3 7 0 0 2 1 ... ## $ CashbackAmount : int 160 121 120 134 130 139 121 123 127 123 ... # Variable &#39;Churn&#39; is 1 if a customer left and 0 # else # correct the variables types churn_data$CustomerID &lt;- as.factor(churn_data$CustomerID) churn_data$Gender &lt;- as.factor(churn_data$Gender) churn_data$Tenure &lt;- as.factor(churn_data$Tenure) churn_data$PreferredLoginDevice &lt;- as.factor(churn_data$PreferredLoginDevice) churn_data$CityTier &lt;- as.factor(churn_data$CityTier) Below are two attempts to model the data. The left assumes a linear probability model (calculated with the same methods that we used in the last chapter), while the right model is a logistic regression model. As you can see, the linear probability model is able of producing probabilities that are above 1 and below 0 (see the lower right corner), which are not valid probabilities, while the logistic model stays between 0 and 1. Notice that songs with a higher cash-back value (on the right of the x-axis) seem to cluster more at \\(0\\) and those with a lower more at \\(1\\) so we expect a negative influence of cash-back value on the probability of churn. Figure 4.13: The same binary data explained by two models; A linear probability model (on the left) and a logistic regression model (on the right) A key insight at this point is that the connection between \\(\\mathbf{X}\\) and \\(Y\\) is non-linear in the logistic regression model. 6.2.2 Technical details of the model As the name suggests, the logistic function is an important component of the logistic regression model. It has the following form: \\[ f(\\mathbf{X}) = \\frac{1}{1 + e^{-\\mathbf{X}}} \\] This function transforms all real numbers into the range between 0 and 1. We need this to model probabilities, as probabilities can only be between 0 and 1. The logistic function on its own is not very useful yet, as we want to be able to determine how predictors influence the probability of a value to be equal to 1. To this end we replace the \\(\\mathbf{X}\\) in the function above with our familiar linear specification, i.e. \\[ \\mathbf{X} = \\beta_0 + \\beta_1 * x_{1,i} + \\beta_2 * x_{2,i} + ... +\\beta_m * x_{m,i}\\\\ f(\\mathbf{X}) = P(y_i = 1) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 * x_{1,i} + \\beta_2 * x_{2,i} + ... +\\beta_m * x_{m,i})}} \\] In our case we only have \\(\\beta_0\\) and \\(\\beta_1\\), the coefficient associated with cash-back. In general we now have a mathematical relationship between our predictor variables \\((x_1, ..., x_m)\\) and the probability of \\(y_i\\) being equal to one. The last step is to estimate the parameters of this model \\((\\beta_0, \\beta_1, ..., \\beta_m)\\) to determine the magnitude of the effects. 6.2.3 Estimation in R We are now going to show how to perform logistic regression in R. Instead of lm() we now use glm(Y~X, family=binomial(link = 'logit')) to use the logit model. We can still use the summary() command to inspect the output of the model. # Run the glm logit_model &lt;- glm(Churn ~ CashbackAmount, family = binomial(link = &quot;logit&quot;), data = churn_data) # Inspect model summary summary(logit_model) ## ## Call: ## glm(formula = Churn ~ CashbackAmount, family = binomial(link = &quot;logit&quot;), ## data = churn_data) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.189865 0.156571 1.21 0.23 ## CashbackAmount -0.010542 0.000936 -11.26 &lt;0.0000000000000002 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 5104.3 on 5629 degrees of freedom ## Residual deviance: 4950.3 on 5628 degrees of freedom ## AIC: 4954 ## ## Number of Fisher Scoring iterations: 5 Noticeably this output does not include an \\(R^2\\) value to asses model fit. Multiple “Pseudo \\(R^2\\)s”, similar to the one used in OLS, have been developed. There are packages that return the \\(R^2\\) given a logit model: library(DescTools) PseudoR2(logit_model, which = &quot;CoxSnell&quot;) # you can also use &#39;McFadden&#39;, &#39;McFaddenAdj&#39;, &#39;Nagelkerke&#39;, &#39;AldrichNelson&#39;, &#39;VeallZimmermann&#39;, &#39;Efron&#39;, &#39;McKelveyZavoina&#39;, &#39;Tjur&#39;, &#39;all&#39; ## CoxSnell ## 0.027 The coefficients of the model give the change in the log odds of the dependent variable due to a unit change in the regressor. This makes the exact interpretation of the coefficients difficult, but we can still interpret the signs and the p-values which will tell us if a variable has a significant positive or negative impact on the probability of the dependent variable being \\(1\\). In order to get the odds ratios we can simply take the exponent of the coefficients. exp(coef(logit_model)) ## (Intercept) CashbackAmount ## 1.21 0.99 This now gives the effect on the dependent variable: an additional dollar paid back as cash-back, on average, makes it \\(0.99\\) time more likely (= by a constant factor of 0.99 = 1% less) for a customer to churn. Note: In some cases, depending on the scales, the coefficient can be extremely large (e.g., due to the fact that the variable is constrained to values between \\(0\\) and \\(1\\) and the coefficients are for a unit change). We can make the “unit-change” interpretation more meaningful by multiplying the variable by \\(100\\), thus changing its scale. This linear transformation does not affect the model fit or the p-values. We observe that cash-back negatively affects the likelihood of churning. To get the confidence intervals for the coefficients we can use the same function as with OLS. confint(logit_model) ## 2.5 % 97.5 % ## (Intercept) -0.114 0.5000 ## CashbackAmount -0.012 -0.0087 To get the effect of an additional point at a specific value, we can calculate the odds ratio by predicting the probability at a value and at the value \\(+1\\). For example, if we are interested in how much more (or, in our case, less) likely a customer with cash-back value of 201 compared to 200 is to churn, we can simply calculate the following: # Probability of churn with a cashback amount of # 200 prob_200 &lt;- exp(-(-summary(logit_model)$coefficients[1, 1] - summary(logit_model)$coefficients[2, 1] * 200)) prob_200 ## [1] 0.15 # Probability of churn with a cashback amount of # 201 prob_201 &lt;- exp(-(-summary(logit_model)$coefficients[1, 1] - summary(logit_model)$coefficients[2, 1] * 201)) prob_201 ## [1] 0.15 # Odds ratio prob_201/prob_200 ## [1] 0.99 This is essentially what we got in the regression output earlier. So the odds are 1% lower at 201 than at 200 cash-back. 6.2.3.1 Logistic model with multiple predictors Of course we can also use multiple predictors in logistic regression as shown in the formula above. We might want to add order amount from last year, days since last order, warehouse-to-home distance, and order count. Again, the familiar formula interface can be used with the glm() function. All the model summaries shown above still work with multiple predictors. multiple_logit_model &lt;- glm(Churn ~ OrderAmountHikeFromlastYear + DaySinceLastOrder + WarehouseToHome + OrderCount + CashbackAmount, family = binomial(link = &quot;logit&quot;), data = churn_data) summary(multiple_logit_model) ## ## Call: ## glm(formula = Churn ~ OrderAmountHikeFromlastYear + DaySinceLastOrder + ## WarehouseToHome + OrderCount + CashbackAmount, family = binomial(link = &quot;logit&quot;), ## data = churn_data) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.16916 0.28264 -0.60 0.55 ## OrderAmountHikeFromlastYear 0.00679 0.01104 0.62 0.54 ## DaySinceLastOrder -0.16095 0.01667 -9.66 &lt; 0.0000000000000002 ## WarehouseToHome 0.02481 0.00459 5.41 0.0000000630563 ## OrderCount 0.10421 0.01959 5.32 0.0000001046673 ## CashbackAmount -0.00978 0.00140 -6.96 0.0000000000033 ## ## (Intercept) ## OrderAmountHikeFromlastYear ## DaySinceLastOrder *** ## WarehouseToHome *** ## OrderCount *** ## CashbackAmount *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 4162.4 on 4548 degrees of freedom ## Residual deviance: 3942.0 on 4543 degrees of freedom ## (1081 observations deleted due to missingness) ## AIC: 3954 ## ## Number of Fisher Scoring iterations: 5 PseudoR2(multiple_logit_model, which = &quot;CoxSnell&quot;) ## CoxSnell ## 0.047 Again, to properly interpret the coefficient, we extract odds ratio: exp(coef(multiple_logit_model)) ## (Intercept) OrderAmountHikeFromlastYear ## 0.84 1.01 ## DaySinceLastOrder WarehouseToHome ## 0.85 1.03 ## OrderCount CashbackAmount ## 1.11 0.99 confint(multiple_logit_model) ## Waiting for profiling to be done... ## 2.5 % 97.5 % ## (Intercept) -0.720 0.3878 ## OrderAmountHikeFromlastYear -0.015 0.0283 ## DaySinceLastOrder -0.194 -0.1287 ## WarehouseToHome 0.016 0.0338 ## OrderCount 0.065 0.1423 ## CashbackAmount -0.013 -0.0071 We can see that such variables as warehouse-to-home distance and order count increase the probability of churn, while days since last order and cash-back value decrease it. 6.2.3.2 Model selection The question remains, whether a variable should be added to the model. We will present two methods for model selection for logistic regression. The first is based on the Akaike Information Criterium (AIC). It is reported with the summary output for logit models. The value of the AIC is relative, meaning that it has no interpretation by itself. However, it can be used to compare and select models. The model with the lowest AIC value is the one that should be chosen. Note that the AIC does not indicate how well the model fits the data, but is merely used to compare models. For example, consider the following model, where we exclude the order count predictor. Seeing as it was able to contribute significantly to the explanatory power of the model, the AIC increases, indicating that the model including order count is better suited to explain the data. We always want the lowest possible AIC. multiple_logit_model2 &lt;- glm(Churn ~ OrderAmountHikeFromlastYear + DaySinceLastOrder + WarehouseToHome + CashbackAmount, family = binomial(link = &quot;logit&quot;), data = churn_data) summary(multiple_logit_model2) ## ## Call: ## glm(formula = Churn ~ OrderAmountHikeFromlastYear + DaySinceLastOrder + ## WarehouseToHome + CashbackAmount, family = binomial(link = &quot;logit&quot;), ## data = churn_data) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.12798 0.26542 -0.48 0.63 ## OrderAmountHikeFromlastYear 0.00281 0.01092 0.26 0.80 ## DaySinceLastOrder -0.11361 0.01390 -8.17 0.0000000000000003 *** ## WarehouseToHome 0.02556 0.00448 5.70 0.0000000119246808 *** ## CashbackAmount -0.00912 0.00126 -7.25 0.0000000000004033 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 4315.0 on 4806 degrees of freedom ## Residual deviance: 4100.4 on 4802 degrees of freedom ## (823 observations deleted due to missingness) ## AIC: 4110 ## ## Number of Fisher Scoring iterations: 5 As a second measure for variable selection, you can use the pseudo \\(R^2\\)s as shown above. The fit is worse according to all three values presented here, when excluding the order count. PseudoR2(multiple_logit_model2, which = &quot;CoxSnell&quot;) ## CoxSnell ## 0.044 6.2.3.3 Predictions We can predict the probability given an observation using the predict(my_logit, newdata = ..., type = \"response\") function. Replace ... with the observed values for which you would like to predict the outcome variable. # Prediction for one observation predict(multiple_logit_model, newdata = data.frame(OrderAmountHikeFromlastYear = 5, DaySinceLastOrder = 30, WarehouseToHome = 10, OrderCount = 10, CashbackAmount = 300), type = &quot;response&quot;) ## 1 ## 0.0013 The prediction indicates that a customer who made \\(5\\) orders last year, \\(10\\) orders in total, last order happened \\(30\\) days ago, warehouse-to-home distance is 10 km, and cash-back amount is 300 dollars, has a tiny (\\(0.2\\%\\) chance of churning. Learning check (LC6.1) What is a correlation coefficient? It describes the difference in means of two variables It describes the causal relation between two variables It is the standardized covariance It describes the degree to which the variation in one variable is related to the variation in another variable None of the above (LC6.2) Which line through a scatterplot produces the best fit in a linear regression model? The line associated with the steepest slope parameter The line that minimizes the sum of the squared deviations of the predicted values (regression line) from the observed values The line that minimizes the sum of the squared residuals The line that maximizes the sum of the squared residuals None of the above (LC6.3) Which of the following statements about the adjusted R-squared is TRUE? It is always larger than the regular \\(R^{2}\\) It increases with every additional variable It increases only with additional variables that add more explanatory power than pure chance It contains a “penalty” for including unnecessary variables None of the above (LC6.4) When do you use a logistic regression model? When the dependent variable is continuous When the independent and dependent variables are binary When the dependent variable is binary None of the above (LC6.5) What is the correct way to implement a linear regression model in R? (x = independent variable, y = dependent variable)? lm(y~x, data=data) lm(x~y + error, data=data) lm(x~y, data=data) lm(y~x + error, data=data) None of the above (LC6.6) Consider the output from a bivariate correlation below …lower prices cause higher sales. …lower prices are associated with higher sales and vice versa. None of the above (LC6.7) When interpreting the statistical significance of a regression coefficient, the p-value… …of 0.05 means that, if the null hypothesis is true (i.e., if the independent variable would NOT affect the outcome), the odds are 19 in 20 of getting a regression coefficient as large or larger than the estimated coefficient …of 0.05 means that, if the null hypothesis is true (i.e., if the independent variable would NOT affect the outcome), the odds are 1 in 20 of getting a regression coefficient as large or larger than the estimated coefficient …of 0.05 means that the effect is statistically significant at the 5% level. …does not tell you anything about the importance of the effect …will get smaller, the larger the calculated value of the test statistic (t-value). (LC6.8) In which setting(s) would a regression coefficient be interpreted as “statistically significant”? When the absolute value of the calculated test-statistic (e.g., t-value) exceeds the critical value of the test statistic at your specified significance level (e.g., 0.05) When the test-statistic (e.g., t-value) is lower than the critical value of the test statistic at your specified significance level (e.g., 0.05) When the confidence interval associated with the test does not contain zero When the p-value is smaller than your specified significance level (e.g., 0.05) (LC6.9) When interpreting the significance of the coefficients in a regression model, what is the relationship between the test statistic (e.g., t-value) and the p-value? The lower the absolute value of the test statistic, the lower the p-value The higher the absolute value of the test statistic, the higher the p-value There is no connection between the test statistic and the p-value The higher the absolute value of the test statistic, the lower the p-value (LC6.10) What does the term overfitting refer to? A regression model that fits to a specific data set so poorly, that it will not generalize to other samples A regression model that fits to a specific data set so well, that it will generalize to other samples particularly well A regression model that has too many predictor variables A regression model that fits to a specific data set so well, that it will only predict well within the sample but not generalize to other samples References Field, A., Miles J., &amp; Field, Z. (2012): Discovering Statistics Using R. Sage Publications (chapters 6, 7, 8). James, G., Witten, D., Hastie, T., &amp; Tibshirani, R. (2013): An Introduction to Statistical Learning with Applications in R, Springer (chapter 3) "],["unsupervised-learning.html", "7 Unsupervised learning 7.1 Principal component analysis 7.2 Cluster analysis Learning check References", " 7 Unsupervised learning 7.1 Principal component analysis You can download the corresponding R-Code here 7.1.1 Introduction In this chapter, we will focus on exploratory factor analysis. Generally, factor analysis is a class of procedures used for data reduction or summary. It is an interdependence technique, meaning that there is no distinction between dependent and independent variables and all variables are considered simultaneously. In exploratory factor analysis, specific hypotheses about how many factors will emerge, and what items these factors will comprise are not requires (as opposed to confirmatory factor analysis). Principal Components Analysis (PCA) is one of the most frequently used techniques. The goals are … To identify underlying dimensions, or factors, that explain the correlations among a set of variables To identify a new, smaller set of uncorrelated variables to replace the original set of correlated variables in subsequent multivariate analysis (e.g., regression analysis, t-test, etc.) To see what this means, let’s use a simple example. Say, you wanted to explain the motives underlying the purchasing of toothpaste. You come up with six items that represent different motives of purchasing toothpaste: Item 1: It is important to buy toothpaste that prevents cavities. Item 2: I like a toothpaste that gives shiny teeth. Item 3: A toothpaste should strengthen your gums. Item 4: I prefer a toothpaste that freshens breath. Item 5: Prevention of tooth decay should not be an important benefit offered by a toothpaste. Item 6: The most important consideration in buying a toothpaste is attractive teeth. Let’s assume you collect data from 30 respondents and you use 7-point itemized rating scales to measure the extent of agreement to each of these statements. This is the data that you have collected: factor_analysis &lt;- read.table(&quot;https://raw.githubusercontent.com/IMSMWU/Teaching/master/MRDA2017/toothpaste.dat&quot;, sep = &quot;\\t&quot;, header = TRUE) #read in data str(factor_analysis) #inspect data ## &#39;data.frame&#39;: 30 obs. of 6 variables: ## $ prevent_cavities: int 7 1 6 4 1 6 5 6 3 2 ... ## $ shiny_teeth : int 3 3 2 5 2 3 3 4 4 6 ... ## $ strengthen_gum : int 6 2 7 4 2 6 6 7 2 2 ... ## $ fresh_breath : int 4 4 4 6 3 4 3 4 3 6 ... ## $ prevent_decay : int 6 3 7 6 2 6 4 7 2 1 ... ## $ attract_teeth : int 4 4 3 5 2 4 3 4 3 6 ... head(factor_analysis) #inspect data A construct is a specific type of concept that exists at a higher level of abstraction than everyday concepts. In this example, the perceived benefits of toothpaste represent the construct that we would like to measure. The construct is unobservable (‘latent’) but it can be inferred from other measurable variables (items) that together comprise a scale (latent construct). A multi-item scale consists of multiple items, where an item is a single question or statement to be evaluated. In the above example, we use six items to measure the perceived benefits of toothpaste. If several items correlate highly, they might measure aspects of a common underlying dimension (a.k.a. factors). That is, specific patterns in the correlation matrix signal the existence of one or more factors underlying the data. Let’s inspect the correlation matrix using the rcorr() function from the Hmisc package. library(&quot;Hmisc&quot;) rcorr(as.matrix(factor_analysis)) ## prevent_cavities shiny_teeth strengthen_gum fresh_breath ## prevent_cavities 1.00 -0.05 0.87 -0.09 ## shiny_teeth -0.05 1.00 -0.16 0.57 ## strengthen_gum 0.87 -0.16 1.00 -0.25 ## fresh_breath -0.09 0.57 -0.25 1.00 ## prevent_decay 0.86 -0.02 0.78 0.01 ## attract_teeth 0.00 0.64 -0.02 0.64 ## prevent_decay attract_teeth ## prevent_cavities 0.86 0.00 ## shiny_teeth -0.02 0.64 ## strengthen_gum 0.78 -0.02 ## fresh_breath 0.01 0.64 ## prevent_decay 1.00 0.14 ## attract_teeth 0.14 1.00 ## ## n= 30 ## ## ## P ## prevent_cavities shiny_teeth strengthen_gum fresh_breath ## prevent_cavities 0.7800 0.0000 0.6508 ## shiny_teeth 0.7800 0.4134 0.0010 ## strengthen_gum 0.0000 0.4134 0.1868 ## fresh_breath 0.6508 0.0010 0.1868 ## prevent_decay 0.0000 0.9175 0.0000 0.9725 ## attract_teeth 0.9826 0.0001 0.9245 0.0001 ## prevent_decay attract_teeth ## prevent_cavities 0.0000 0.9826 ## shiny_teeth 0.9175 0.0001 ## strengthen_gum 0.0000 0.9245 ## fresh_breath 0.9725 0.0001 ## prevent_decay 0.4723 ## attract_teeth 0.4723 You can see that some of the items correlate highly, while others don’t. Specifically, there appear to be two groups of items that correlate highly and that might represent underlying dimensions of the construct: Factor 1: Items 1, 3, 5 Factor 2: Items 2, 4, 6 Going back to the specific wording of the items you can see that the first group of items (i.e., items 1,3,5) refer to the health benefits, while the second item group (i.e., items 2,4,6) refer to the social benefits. Imagine now, for example, you would like to include the above variables as explanatory variables in a regression model. Due to the high degree of correlation among the items, you are likely to run into problems of multicollinearity. Instead of omitting some of the items, you might try to combine highly correlated items into one variable. Another application could be when you are developing a new measurement scale for a construct and you wish to explore the underlying dimensions of this construct. In these applications, you need to make sure that the questions that you are asking actually relate to the construct that you intend to measure. The goal of factor analysis is to explain the maximum amount of total variance in a correlation matrix by transforming the original variables into linear components. This means that the correlation matrix is broken down into a smaller set of dimensions. The generalized formal representation of the linear relationship between a latent factor Y and the set of variables can be written as: \\[\\begin{equation} Y_i=b_1X_{1i} + b_2X_{2i} + b_nX_{ni}+\\epsilon_i \\tag{7.1} \\end{equation}\\] where Xn represents the data that we have collected for the different variables. To make it more explicit, the equation could also be written as \\[\\begin{equation} Factor_i=b_1Variable_{1i} + b_2Variable_{2i} + b_nVariable_{ni}+\\epsilon_i \\tag{7.2} \\end{equation}\\] where the dependent variable “Factor” refers to the factor score of person i on the underlying dimensions. In our case, the initial inspection suggested two underlying factors (i.e., health benefits and social benefits), so that we can construct two equations that describe both factors in terms of the variables that we have measured: \\[\\begin{equation} Health_i=b_1X_{1i} + b_2X_{2i} + b_3X_{3i}+ b_4X_{4i}+ b_5X_{5i}+ b_6X_{6i}+\\epsilon_i \\tag{7.3} \\end{equation}\\] \\[\\begin{equation} Social_i=b_1X_{1i} + b_2X_{2i} + b_3X_{3i}+ b_4X_{4i}+ b_5X_{5i}+ b_6X_{6i}+\\epsilon_i \\tag{7.4} \\end{equation}\\] where the b’s in each equation represent the factor loadings. You can see that both equations include the same set of predictors. However, their values in each equation will be different, depending on the importance of each variable to the respective factor. Once the factor loadings have been computed (we will see how this is done below), we can summarize them in a component matrix, which is usually denoted as A: \\[\\mathbf{A} = \\left[\\begin{array} {rrr} 0.93 &amp; 0.25 \\\\ -0.30 &amp; 0.80 \\\\ 0.94 &amp; 0.13 \\\\ -0.34 &amp; 0.79 \\\\ 0.87 &amp; 0.35 \\\\ -0.18 &amp; 0.87 \\end{array}\\right]\\] The linear relation between the factors and the factor loadings can also be shown in a graph, where each axis represents a factor and the variables are placed on the coordinates according to the strength of the relationship between the variable and each factor. The greater the loading of variables on a factor, the more that factor explains relationships between those variables. You can also think of the factor loadings as the correlations between a factor and a variable. Figure 1.4: Factor loadings The factor loadings may then be used to compute the two new variables (i.e., factor scores) representing the two underlying dimensions. Using a rather simplistic approach, the factor scores for person i could be computed by \\[\\begin{equation} \\begin{split} Health_i=&amp; 0.93*preventCavities_{i} -0.3*shinyTeeth_{i} + 0.94*strengthenGum_{i}\\\\ &amp;-0.34*freshBreath_{i} + 0.87*preventDecay_{i} - 0.18*attractTeeth_{i} \\end{split} \\tag{7.5} \\end{equation}\\] \\[\\begin{equation} \\begin{split} Social_i=&amp; 0.25*preventCavities_{i} + 0.80*shinyTeeth_{i} + 0.13*strengthenGum_{i}\\\\ &amp;+ 0.79*freshBreath_{i}+ 0.35*preventDecay_{i}+ 0.87*attractTeeth_{i} \\end{split} \\tag{7.6} \\end{equation}\\] where the variable names are replaced by the values that were observed for respondent i to compute the factor scores for respondent i. This means that we have reduced the number of variables from six to two. Note that this is a rather simple approach that is intended to explain the underlying logic. However, that the resulting factor scores will depend on the measurement scales of the variables. If different measurement scales would be used, the resulting factor scores for different factors could not be compared. Thus, R will compute the factor scores using more sophisticated methods as we will see below. 7.1.2 Steps in factor analysis Now that we have a broad understanding of how factor analysis works, let’s use another example to go through the process of deriving factors step by step. In this section, we will use the R anxiety questionnaire from the book by Andy Field et al.. The questionnaire is intended to measure the various aspects of student’s anxiety towards learning R. It includes 23 items for which respondents are asked to indicate on a five-point Likert scale to what extent they agree with the respective statements. The questionnaire is shown in the following figure: The R anxiety questionnaire (source: Field, A. et al. (2012): Discovering Statistics Using R, p. 768) Let’s assume, we have collected data from 2,571 respondents and stored the results in the data set “raq.dat”. raq_data &lt;- read.table(&quot;https://raw.githubusercontent.com/IMSMWU/Teaching/master/MRDA2017/raq.dat&quot;, sep = &quot;\\t&quot;, header = TRUE) #read in data 7.1.2.1 Are the assumptions satisfied? Since PCA is based the correlation between variables, the first step is to inspect the correlation matrix, which can be created using the cor() function. raq_matrix &lt;- cor(raq_data) round(raq_matrix, 3) ## Q01 Q02 Q03 Q04 Q05 Q06 Q07 Q08 Q09 Q10 ## Q01 1.000 -0.099 -0.337 0.436 0.402 0.217 0.305 0.331 -0.092 0.214 ## Q02 -0.099 1.000 0.318 -0.112 -0.119 -0.074 -0.159 -0.050 0.315 -0.084 ## Q03 -0.337 0.318 1.000 -0.380 -0.310 -0.227 -0.382 -0.259 0.300 -0.193 ## Q04 0.436 -0.112 -0.380 1.000 0.401 0.278 0.409 0.349 -0.125 0.216 ## Q05 0.402 -0.119 -0.310 0.401 1.000 0.257 0.339 0.269 -0.096 0.258 ## Q06 0.217 -0.074 -0.227 0.278 0.257 1.000 0.514 0.223 -0.113 0.322 ## Q07 0.305 -0.159 -0.382 0.409 0.339 0.514 1.000 0.297 -0.128 0.284 ## Q08 0.331 -0.050 -0.259 0.349 0.269 0.223 0.297 1.000 0.016 0.159 ## Q09 -0.092 0.315 0.300 -0.125 -0.096 -0.113 -0.128 0.016 1.000 -0.134 ## Q10 0.214 -0.084 -0.193 0.216 0.258 0.322 0.284 0.159 -0.134 1.000 ## Q11 0.357 -0.144 -0.351 0.369 0.298 0.328 0.345 0.629 -0.116 0.271 ## Q12 0.345 -0.195 -0.410 0.442 0.347 0.313 0.423 0.252 -0.167 0.246 ## Q13 0.355 -0.143 -0.318 0.344 0.302 0.466 0.442 0.314 -0.167 0.302 ## Q14 0.338 -0.165 -0.371 0.351 0.315 0.402 0.441 0.281 -0.122 0.255 ## Q15 0.246 -0.165 -0.312 0.334 0.261 0.360 0.391 0.300 -0.187 0.295 ## Q16 0.499 -0.168 -0.419 0.416 0.395 0.244 0.389 0.321 -0.189 0.291 ## Q17 0.371 -0.087 -0.327 0.383 0.310 0.282 0.391 0.590 -0.037 0.218 ## Q18 0.347 -0.164 -0.375 0.382 0.322 0.513 0.501 0.280 -0.150 0.293 ## Q19 -0.189 0.203 0.342 -0.186 -0.165 -0.167 -0.269 -0.159 0.249 -0.127 ## Q20 0.214 -0.202 -0.325 0.243 0.200 0.101 0.221 0.175 -0.159 0.084 ## Q21 0.329 -0.205 -0.417 0.410 0.335 0.272 0.483 0.296 -0.136 0.193 ## Q22 -0.104 0.231 0.204 -0.098 -0.133 -0.165 -0.168 -0.079 0.257 -0.131 ## Q23 -0.004 0.100 0.150 -0.034 -0.042 -0.069 -0.070 -0.050 0.171 -0.062 ## Q11 Q12 Q13 Q14 Q15 Q16 Q17 Q18 Q19 Q20 ## Q01 0.357 0.345 0.355 0.338 0.246 0.499 0.371 0.347 -0.189 0.214 ## Q02 -0.144 -0.195 -0.143 -0.165 -0.165 -0.168 -0.087 -0.164 0.203 -0.202 ## Q03 -0.351 -0.410 -0.318 -0.371 -0.312 -0.419 -0.327 -0.375 0.342 -0.325 ## Q04 0.369 0.442 0.344 0.351 0.334 0.416 0.383 0.382 -0.186 0.243 ## Q05 0.298 0.347 0.302 0.315 0.261 0.395 0.310 0.322 -0.165 0.200 ## Q06 0.328 0.313 0.466 0.402 0.360 0.244 0.282 0.513 -0.167 0.101 ## Q07 0.345 0.423 0.442 0.441 0.391 0.389 0.391 0.501 -0.269 0.221 ## Q08 0.629 0.252 0.314 0.281 0.300 0.321 0.590 0.280 -0.159 0.175 ## Q09 -0.116 -0.167 -0.167 -0.122 -0.187 -0.189 -0.037 -0.150 0.249 -0.159 ## Q10 0.271 0.246 0.302 0.255 0.295 0.291 0.218 0.293 -0.127 0.084 ## Q11 1.000 0.335 0.423 0.325 0.365 0.369 0.587 0.373 -0.200 0.255 ## Q12 0.335 1.000 0.489 0.433 0.332 0.408 0.333 0.493 -0.267 0.298 ## Q13 0.423 0.489 1.000 0.450 0.342 0.358 0.408 0.533 -0.227 0.204 ## Q14 0.325 0.433 0.450 1.000 0.380 0.418 0.354 0.498 -0.254 0.226 ## Q15 0.365 0.332 0.342 0.380 1.000 0.454 0.373 0.343 -0.210 0.206 ## Q16 0.369 0.408 0.358 0.418 0.454 1.000 0.410 0.422 -0.267 0.265 ## Q17 0.587 0.333 0.408 0.354 0.373 0.410 1.000 0.376 -0.163 0.205 ## Q18 0.373 0.493 0.533 0.498 0.343 0.422 0.376 1.000 -0.257 0.235 ## Q19 -0.200 -0.267 -0.227 -0.254 -0.210 -0.267 -0.163 -0.257 1.000 -0.249 ## Q20 0.255 0.298 0.204 0.226 0.206 0.265 0.205 0.235 -0.249 1.000 ## Q21 0.346 0.441 0.374 0.399 0.300 0.421 0.363 0.430 -0.275 0.468 ## Q22 -0.162 -0.167 -0.195 -0.170 -0.168 -0.156 -0.126 -0.160 0.234 -0.100 ## Q23 -0.086 -0.046 -0.053 -0.048 -0.062 -0.082 -0.092 -0.080 0.122 -0.035 ## Q21 Q22 Q23 ## Q01 0.329 -0.104 -0.004 ## Q02 -0.205 0.231 0.100 ## Q03 -0.417 0.204 0.150 ## Q04 0.410 -0.098 -0.034 ## Q05 0.335 -0.133 -0.042 ## Q06 0.272 -0.165 -0.069 ## Q07 0.483 -0.168 -0.070 ## Q08 0.296 -0.079 -0.050 ## Q09 -0.136 0.257 0.171 ## Q10 0.193 -0.131 -0.062 ## Q11 0.346 -0.162 -0.086 ## Q12 0.441 -0.167 -0.046 ## Q13 0.374 -0.195 -0.053 ## Q14 0.399 -0.170 -0.048 ## Q15 0.300 -0.168 -0.062 ## Q16 0.421 -0.156 -0.082 ## Q17 0.363 -0.126 -0.092 ## Q18 0.430 -0.160 -0.080 ## Q19 -0.275 0.234 0.122 ## Q20 0.468 -0.100 -0.035 ## Q21 1.000 -0.129 -0.068 ## Q22 -0.129 1.000 0.230 ## Q23 -0.068 0.230 1.000 If the variables measure the same construct, we would expect to see a certain degree of correlation between the variables. Even if the variables turn out to measure different dimensions of the same underlying construct, we would still expect to see some degree of correlation. So the first problem that could occur is that the correlations are not high enough. A first approach would be to scan the correlation matrix for correlations lower than about 0.3 and identify variables that have many correlations below this threshold. If your data set contains many variables, this task can be quite tedious. To make the task a little easier, you could proceed as follows. Create a dataframe from the correlation matrix and set the diagonal elements to missing since these are always 1: correlations &lt;- as.data.frame(raq_matrix) diag(correlations) &lt;- NA Now we can use the apply() function to count the number of correlations for each variable that are below a certain threshold (say, 0.3). The apply() function is very useful as it lets you apply function by the rows or columns in your dataframe. In the following example abs(correlations) &lt; 0.3 returns a logical value for the correlation matrix that returns TRUE if the statement is true. The second argument 1 means that the function should be applied to the rows (2 would apply it to the columns). The third argument states the function that should be applied. In our case, we would like to count the number of absolute correlations below 0.3 so we apply the sum function, which sums the number of TRUE occurrences by row. The final argument na.rm = TRUE simply tells R to neglect the missing values that we have created for the diagonals of the matrix. apply(abs(correlations) &lt; 0.3, 1, sum, na.rm = TRUE) ## Q01 Q02 Q03 Q04 Q05 Q06 Q07 Q08 Q09 Q10 Q11 Q12 Q13 Q14 Q15 Q16 Q17 Q18 Q19 Q20 ## 9 20 6 8 11 14 8 16 21 20 8 8 6 8 11 8 8 8 21 20 ## Q21 Q22 Q23 ## 9 22 22 The output shows you the number of correlations below the threshold for each variable. In a similar way, it would also be possible to compute the mean correlation for each variable. apply(abs(correlations), 1, mean, na.rm = TRUE) ## Q01 Q02 Q03 Q04 Q05 Q06 Q07 Q08 ## 0.2786461 0.1590732 0.3193955 0.3044131 0.2671130 0.2685230 0.3341477 0.2581210 ## Q09 Q10 Q11 Q12 Q13 Q14 Q15 Q16 ## 0.1555957 0.2105303 0.3197517 0.3263889 0.3277661 0.3179673 0.2902898 0.3345197 ## Q17 Q18 Q19 Q20 Q21 Q22 Q23 ## 0.3084564 0.3421177 0.2173012 0.2121990 0.3222743 0.1621558 0.0798525 Another way to make the correlations more salient is to plot the correlation matrix using different colors that indicate the strength of the correlations. This can be done using the corPolot() function from the psych package. corPlot(correlations, numbers = TRUE, upper = FALSE, diag = FALSE, main = &quot;Correlations between variables&quot;) Figure 1.9: Correlation matrix You will, however, notice that this is a rather subjective approach. The Bartlett’s test is a statistical test that can be used to test whether all the off-diagonal elements in the population correlation matrix are zero (i.e., whether the population correlation matrix resembles an identify matrix). Thus, it tests whether the correlations are overall too small. If the matrix is an identify matrix, it means that all variables are independent. Thus, a significant test statistic (i.e., p &lt; 0.05) indicates that there is some relationship between variables. The test can be implemented using the cortest.bartlett() function from the psych package: library(psych) cortest.bartlett(raq_matrix, n = nrow(raq_data)) ## $chisq ## [1] 19334.49 ## ## $p.value ## [1] 0 ## ## $df ## [1] 253 In our example, the p-value is less than 0.05, which is good news since it confirms that overall the correlation between variables is different from zero. The other problem that might occur is that the correlations are too high. Actually, a certain degree of multicollinearity is not a problem in PCA. However, it is important to avoid extreme multicollinearity (i.e., variables are highly correlated) and singularity (i.e., variables are perfectly correlated). Multicollinearity causes problems, because it becomes difficult to determine the unique contribution of a variable (as was the case in linear regression analysis). Again, inspecting the entire correlation matrix when there are many variables will be a tedious task. . apply(abs(correlations) &gt; 0.8, 1, sum, na.rm = TRUE) ## Q01 Q02 Q03 Q04 Q05 Q06 Q07 Q08 Q09 Q10 Q11 Q12 Q13 Q14 Q15 Q16 Q17 Q18 Q19 Q20 ## 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ## Q21 Q22 Q23 ## 0 0 0 The results do not suggest any extreme or perfect correlations. Again, there is a more objective measure that could be applied. The determinant tells us whether the correlation matrix is singular (determinant = 0), or if all variables are completely unrelated (determinant = 1), or somewhere in between. As a rule of thumb, the determinant should be greater than 0.00001. The det() function can be used to compute the determinant: det(raq_matrix) ## [1] 0.0005271037 det(raq_matrix) &gt; 0.00001 ## [1] TRUE As you can see, the determinant is larger than the threshold, indicating that the overall correlation between variables is not too strong. Finally, you should test if the correlation pattern in the matrix is appropriate for factor analysis using the Kaiser-Meyer-Olkin (KMO) measure of sampling adequacy. This statistic is a measure of the proportion of variance among variables that might be common variance. \\[\\begin{equation} MSA_j=\\frac{\\sum_{k\\ne j}^{}{r^2_{jk}}}{\\sum_{k\\ne j}^{}{r^2_{jk}}+\\sum_{k\\ne j}^{}{p^2_{jk}}} \\tag{7.7} \\end{equation}\\] where \\(r_{jk}\\) is the correlation between two variables of interest and \\(p_{jk}\\) is their partial correlation. The partial correlation measures the degree of association between the two variables, when the effect of the remaining variables is controlled for. It can takes values between 0 (bad) and 1 (good), where a value of 0 indicates that the sum of partial correlations is large relative to the sum of correlations. If the remaining correlation between two variables remains high if you control for all the other variables, this provides an indication that the correlation is fairly concentrated and on a subset of the variables and factor analysis is likely to be inappropriate. In contrast, a value close to 1 means that the sum of the partial correlations is fairly is low, indicating a more compact pattern of correlations between a larger set of variables. Values should at least exceed 0.50, with the thresholds: &lt;.50 = unacceptable &gt;.50 = miserable &gt;.60 = mediocre &gt;.70 = middling &gt;.80 = meritorious &gt;.90 = marvelous The KMO measure of sampling adequacy can be computed using the KMO() function from the psych package: KMO(raq_data) ## Kaiser-Meyer-Olkin factor adequacy ## Call: KMO(r = raq_data) ## Overall MSA = 0.93 ## MSA for each item = ## Q01 Q02 Q03 Q04 Q05 Q06 Q07 Q08 Q09 Q10 Q11 Q12 Q13 Q14 Q15 Q16 ## 0.93 0.87 0.95 0.96 0.96 0.89 0.94 0.87 0.83 0.95 0.91 0.95 0.95 0.97 0.94 0.93 ## Q17 Q18 Q19 Q20 Q21 Q22 Q23 ## 0.93 0.95 0.94 0.89 0.93 0.88 0.77 You can see that the statistic is calculated for the entire matrix and for each variable individually. In our example, the values for all variables as well as the overall matrix is above 0.5, suggesting that factor analysis is appropriate. 7.1.2.2 Deriving factors After testing that the data can be used for PCA, we can move on to conducting PCA. Conducting PCA is fairly simple in R using the pricipal() function from the psych package. However, before the apply the function to our data, it is useful to reflect on the goals of PCA again. The goal is to explain the maximum amount of total variance in a correlation matrix by transforming the original variables into a smaller set of linear components (factors). So the first decision we have to make is how many factors we should extract. There are different methods that can be used to decide on the appropriate number of factors, including: A priori determination: Requires prior knowledge Determination based on percentage of variance: When cumulative percentage of variance extracted by the factors reaches a satisfactory level (e.g., factors extracted should account for at least 60% of the variance) Eigenvalues: Eigenvalues refer to the total variance that is explained by each factor. Factors with eigenvalues greater than 1.0 are retained (factors with a variance less than 1.0 are no better than a single variable) Scree plot: Is a plot of the eigenvalues against the number of factors in order of extraction. Assumption: the point at which the scree begins denotes the true number of factors. Often, the decision is made based on a combination of different criteria. By extracting as many factors as there are variables we can inspect their eigenvalues and make decisions about which factors to extract. Since we have 23 variables, we set the argument nfactors to 23. pc1 &lt;- principal(raq_data, nfactors = 23, rotate = &quot;none&quot;) pc1 ## Principal Components Analysis ## Call: principal(r = raq_data, nfactors = 23, rotate = &quot;none&quot;) ## Standardized loadings (pattern matrix) based upon correlation matrix ## PC1 PC2 PC3 PC4 PC5 PC6 PC7 PC8 PC9 PC10 PC11 PC12 ## Q01 0.59 0.18 -0.22 0.12 -0.40 -0.11 -0.22 -0.08 0.01 -0.10 0.11 -0.12 ## Q02 -0.30 0.55 0.15 0.01 -0.03 -0.38 0.19 -0.39 0.01 -0.12 0.30 0.27 ## Q03 -0.63 0.29 0.21 -0.07 0.02 0.00 0.01 -0.05 0.20 0.10 0.15 0.03 ## Q04 0.63 0.14 -0.15 0.15 -0.20 -0.12 -0.06 0.11 -0.11 -0.01 -0.03 0.34 ## Q05 0.56 0.10 -0.07 0.14 -0.42 -0.17 -0.06 0.11 0.24 0.09 -0.30 0.16 ## Q06 0.56 0.10 0.57 -0.05 0.17 0.01 0.00 0.05 0.00 0.00 -0.13 0.20 ## Q07 0.69 0.04 0.25 0.10 0.17 -0.08 0.05 0.03 -0.08 0.13 -0.27 0.20 ## Q08 0.55 0.40 -0.32 -0.42 0.15 0.10 -0.07 -0.04 0.01 -0.05 -0.09 0.03 ## Q09 -0.28 0.63 -0.01 0.10 0.17 -0.27 -0.01 -0.03 0.16 0.32 -0.22 -0.37 ## Q10 0.44 0.03 0.36 -0.10 -0.34 0.22 0.44 -0.03 0.37 -0.22 -0.11 -0.21 ## Q11 0.65 0.25 -0.21 -0.40 0.13 0.18 -0.01 0.03 0.10 -0.14 0.00 0.03 ## Q12 0.67 -0.05 0.05 0.25 0.04 -0.08 -0.14 0.08 0.01 -0.11 0.19 -0.07 ## Q13 0.67 0.08 0.28 -0.01 0.13 0.03 -0.21 0.05 0.08 -0.22 0.24 -0.08 ## Q14 0.66 0.02 0.20 0.14 0.08 -0.03 -0.10 -0.06 -0.14 0.16 0.08 -0.29 ## Q15 0.59 0.01 0.12 -0.11 -0.07 0.29 0.32 -0.12 -0.27 0.41 0.15 0.09 ## Q16 0.68 0.01 -0.14 0.08 -0.32 0.00 0.12 -0.14 -0.19 0.15 0.16 -0.19 ## Q17 0.64 0.33 -0.21 -0.34 0.10 0.05 -0.02 0.03 -0.04 0.02 0.01 -0.03 ## Q18 0.70 0.03 0.30 0.13 0.15 -0.09 -0.10 0.06 -0.06 -0.12 0.05 -0.11 ## Q19 -0.43 0.39 0.10 -0.01 -0.15 0.07 0.05 0.68 0.02 0.16 0.29 0.04 ## Q20 0.44 -0.21 -0.40 0.30 0.33 -0.01 0.34 0.03 0.33 0.02 0.21 0.04 ## Q21 0.66 -0.06 -0.19 0.28 0.24 -0.15 0.18 0.10 0.12 0.08 -0.02 0.04 ## Q22 -0.30 0.47 -0.12 0.38 0.07 0.12 0.31 0.12 -0.41 -0.39 -0.19 -0.10 ## Q23 -0.14 0.37 -0.02 0.51 0.02 0.62 -0.28 -0.22 0.18 0.08 0.00 0.13 ## PC13 PC14 PC15 PC16 PC17 PC18 PC19 PC20 PC21 PC22 PC23 h2 ## Q01 0.30 -0.25 0.18 0.12 -0.05 -0.17 0.16 -0.01 -0.21 0.05 0.01 1 ## Q02 -0.02 0.01 -0.24 -0.05 -0.08 0.00 0.01 -0.02 -0.02 0.03 0.02 1 ## Q03 0.10 0.13 0.40 -0.06 0.43 0.08 0.09 0.05 0.01 0.00 0.05 1 ## Q04 -0.32 -0.17 0.12 0.31 0.19 0.05 -0.21 0.04 0.09 -0.02 0.02 1 ## Q05 0.12 0.48 -0.07 -0.08 -0.04 0.01 -0.04 0.00 -0.02 0.02 0.01 1 ## Q06 0.24 -0.03 0.08 0.20 -0.14 0.05 0.09 -0.07 0.04 -0.32 -0.11 1 ## Q07 0.04 -0.22 0.00 -0.23 0.03 -0.15 0.20 0.16 0.14 0.24 0.09 1 ## Q08 -0.01 0.04 -0.04 0.03 0.10 0.07 0.12 -0.15 0.06 0.16 -0.36 1 ## Q09 -0.17 -0.07 0.12 0.11 -0.19 -0.02 -0.08 -0.03 0.04 -0.01 0.03 1 ## Q10 -0.17 -0.15 -0.07 0.03 0.07 -0.01 0.00 0.04 -0.03 0.02 -0.04 1 ## Q11 0.02 0.03 -0.02 0.07 -0.05 0.07 0.07 -0.18 0.06 0.00 0.41 1 ## Q12 -0.45 0.17 0.09 -0.10 -0.08 0.04 0.36 0.00 -0.04 -0.10 -0.02 1 ## Q13 0.01 0.12 0.14 -0.11 -0.06 -0.32 -0.30 -0.06 0.16 0.08 -0.05 1 ## Q14 0.07 0.14 -0.37 0.25 0.34 -0.09 0.06 0.02 0.03 -0.01 0.05 1 ## Q15 -0.09 0.16 0.16 0.06 -0.12 -0.10 -0.04 -0.07 -0.19 0.10 0.00 1 ## Q16 0.12 -0.08 0.06 -0.22 -0.03 0.22 -0.02 -0.04 0.35 -0.12 -0.01 1 ## Q17 -0.01 -0.01 -0.05 -0.18 0.04 -0.04 -0.10 0.42 -0.15 -0.23 -0.01 1 ## Q18 0.09 0.00 0.03 -0.01 -0.06 0.45 -0.15 0.08 -0.18 0.23 0.01 1 ## Q19 0.06 -0.09 -0.16 -0.03 -0.06 0.01 0.05 -0.02 0.02 0.04 -0.02 1 ## Q20 0.17 0.07 0.05 0.22 -0.09 0.00 0.04 0.18 0.10 0.06 -0.04 1 ## Q21 0.03 -0.15 -0.04 -0.27 0.20 -0.03 -0.11 -0.31 -0.20 -0.13 -0.01 1 ## Q22 0.08 0.15 0.09 0.01 0.04 -0.06 0.02 0.00 0.01 -0.01 0.01 1 ## Q23 -0.01 -0.07 -0.12 -0.06 -0.03 0.05 -0.03 0.01 -0.01 -0.02 0.00 1 ## u2 com ## Q01 -0.00000000000000111 6.0 ## Q02 -0.00000000000000377 6.1 ## Q03 0.00000000000000067 4.4 ## Q04 -0.00000000000000111 4.9 ## Q05 -0.00000000000000067 5.2 ## Q06 -0.00000000000000044 4.4 ## Q07 -0.00000000000000044 4.1 ## Q08 -0.00000000000000133 5.7 ## Q09 -0.00000000000000133 5.0 ## Q10 0.00000000000000022 7.7 ## Q11 -0.00000000000000133 4.1 ## Q12 -0.00000000000000155 3.8 ## Q13 -0.00000000000000155 4.2 ## Q14 -0.00000000000000111 4.3 ## Q15 -0.00000000000000067 5.6 ## Q16 -0.00000000000000178 4.0 ## Q17 -0.00000000000000155 4.3 ## Q18 -0.00000000000000111 3.4 ## Q19 -0.00000000000000089 3.5 ## Q20 0.00000000000000044 8.7 ## Q21 -0.00000000000000022 4.6 ## Q22 0.00000000000000000 7.2 ## Q23 0.00000000000000011 4.2 ## ## PC1 PC2 PC3 PC4 PC5 PC6 PC7 PC8 PC9 PC10 PC11 ## SS loadings 7.29 1.74 1.32 1.23 0.99 0.90 0.81 0.78 0.75 0.72 0.68 ## Proportion Var 0.32 0.08 0.06 0.05 0.04 0.04 0.04 0.03 0.03 0.03 0.03 ## Cumulative Var 0.32 0.39 0.45 0.50 0.55 0.59 0.62 0.65 0.69 0.72 0.75 ## Proportion Explained 0.32 0.08 0.06 0.05 0.04 0.04 0.04 0.03 0.03 0.03 0.03 ## Cumulative Proportion 0.32 0.39 0.45 0.50 0.55 0.59 0.62 0.65 0.69 0.72 0.75 ## PC12 PC13 PC14 PC15 PC16 PC17 PC18 PC19 PC20 PC21 PC22 ## SS loadings 0.67 0.61 0.58 0.55 0.52 0.51 0.46 0.42 0.41 0.38 0.36 ## Proportion Var 0.03 0.03 0.03 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 ## Cumulative Var 0.78 0.80 0.83 0.85 0.88 0.90 0.92 0.94 0.95 0.97 0.99 ## Proportion Explained 0.03 0.03 0.03 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 ## Cumulative Proportion 0.78 0.80 0.83 0.85 0.88 0.90 0.92 0.94 0.95 0.97 0.99 ## PC23 ## SS loadings 0.33 ## Proportion Var 0.01 ## Cumulative Var 1.00 ## Proportion Explained 0.01 ## Cumulative Proportion 1.00 ## ## Mean item complexity = 5 ## Test of the hypothesis that 23 components are sufficient. ## ## The root mean square of the residuals (RMSR) is 0 ## with the empirical chi square 0 with prob &lt; NA ## ## Fit based upon off diagonal values = 1 The output is quite complex, but we will focus only on the SS loadings for now, which are the Eigenvalues (a.k.a. sum of squared loadings). One common rule is to retain factors with eigenvalues greater than 1.0. So based on this rule, we would extract four factors (i.e., the SS loadings for the fifth factor is &lt; 1). You can also plot the eigenvalues against the number of factors in order of extraction using a so-called Scree plot: plot(pc1$values, type = &quot;b&quot;) abline(h = 1, lty = 2) Figure 1.14: Scree plot The dashed line is simply a visualization of the rule that we will retain factors with Eigenvalues &gt; 1 (suggesting four factors). Another criterion based on this plot would be to find the point where the curve flattens (point of inflection). If the largest few eigenvalues in the covariance matrix dominate in magnitude, then the scree plot will exhibit an “elbow”. From that point onward, the incremental gain in explained variance is rather low. Also according to this criterion, we would extract four factors. Taken together, the results suggest that we should extract four factors. Now that we know how many components we want to extract, we can rerun the analysis, specifying that number: pc2 &lt;- principal(raq_data, nfactors = 4, rotate = &quot;none&quot;) pc2 ## Principal Components Analysis ## Call: principal(r = raq_data, nfactors = 4, rotate = &quot;none&quot;) ## Standardized loadings (pattern matrix) based upon correlation matrix ## PC1 PC2 PC3 PC4 h2 u2 com ## Q01 0.59 0.18 -0.22 0.12 0.43 0.57 1.6 ## Q02 -0.30 0.55 0.15 0.01 0.41 0.59 1.7 ## Q03 -0.63 0.29 0.21 -0.07 0.53 0.47 1.7 ## Q04 0.63 0.14 -0.15 0.15 0.47 0.53 1.3 ## Q05 0.56 0.10 -0.07 0.14 0.34 0.66 1.2 ## Q06 0.56 0.10 0.57 -0.05 0.65 0.35 2.1 ## Q07 0.69 0.04 0.25 0.10 0.55 0.45 1.3 ## Q08 0.55 0.40 -0.32 -0.42 0.74 0.26 3.5 ## Q09 -0.28 0.63 -0.01 0.10 0.48 0.52 1.5 ## Q10 0.44 0.03 0.36 -0.10 0.33 0.67 2.1 ## Q11 0.65 0.25 -0.21 -0.40 0.69 0.31 2.2 ## Q12 0.67 -0.05 0.05 0.25 0.51 0.49 1.3 ## Q13 0.67 0.08 0.28 -0.01 0.54 0.46 1.4 ## Q14 0.66 0.02 0.20 0.14 0.49 0.51 1.3 ## Q15 0.59 0.01 0.12 -0.11 0.38 0.62 1.2 ## Q16 0.68 0.01 -0.14 0.08 0.49 0.51 1.1 ## Q17 0.64 0.33 -0.21 -0.34 0.68 0.32 2.4 ## Q18 0.70 0.03 0.30 0.13 0.60 0.40 1.4 ## Q19 -0.43 0.39 0.10 -0.01 0.34 0.66 2.1 ## Q20 0.44 -0.21 -0.40 0.30 0.48 0.52 3.2 ## Q21 0.66 -0.06 -0.19 0.28 0.55 0.45 1.6 ## Q22 -0.30 0.47 -0.12 0.38 0.46 0.54 2.8 ## Q23 -0.14 0.37 -0.02 0.51 0.41 0.59 2.0 ## ## PC1 PC2 PC3 PC4 ## SS loadings 7.29 1.74 1.32 1.23 ## Proportion Var 0.32 0.08 0.06 0.05 ## Cumulative Var 0.32 0.39 0.45 0.50 ## Proportion Explained 0.63 0.15 0.11 0.11 ## Cumulative Proportion 0.63 0.78 0.89 1.00 ## ## Mean item complexity = 1.8 ## Test of the hypothesis that 4 components are sufficient. ## ## The root mean square of the residuals (RMSR) is 0.06 ## with the empirical chi square 4006.15 with prob &lt; 0 ## ## Fit based upon off diagonal values = 0.96 Now that the output is less complex, we can inspect the remaining statistics. The first part of the output is the factor matrix, which contains the factor loadings of all the variables on the four extracted factors (i.e., PC1, PC2, PC3, PC4). As we have said before, the factor loadings are the correlations between the variables and the factors. The factor matrix also contains the columns “h2” and “u2”. “h2” refers to the communality, which is the proportion of variance a variable shares with all the other variables being considered. This is also the proportion of variance explained by the common factors. In contrast, “u2” refers to the unique variance, which is the proportion of the variance that is unique to a particular variable. In PCA, we are primarily interested in the common variance. When the communality is very low (say &lt;.30), a variable is “quite unique” and should be removed, as it is definitely measuring “something else”. In our example, all communalities (i.e., h2) are above 0.3 so that we retain all variables. Note that there is a difference between PCA and common factor analysis. PCA focuses on the variance and aims to reproduce the total variable variance. This means that the components reflect both common and unique variance of the variables. Factor analysis, in contrast, focuses on the correlation and aims to reproduce the correlations among variables. Here, the factors only represent the common variance that variable share and do not include the unique variance. In other words, while factor analysis focuses on explaining the off-diagonal terms in the correlation matrix (i.e., shared co-variance), PCA focuses on explaining the diagonal terms (i.e., the variances). However, although PCA aims to reproduce the on-diagonal terms in the correlation matrix, it also tends to fit the off-diagonal correlations quite well. Hence, the results are often comparable. See also here. You should also take a closer look at the residuals in order to check whether you have extracted the correct number of factors. The difference between the reproduced and the actual correlation matrices are the residuals. We can extract the residuals from our model using the factor.residuals() function from the psych package. It takes the original correlation matrix and the factor loadings as arguments: residuals &lt;- factor.residuals(raq_matrix, pc2$loadings) round(residuals, 3) ## Q01 Q02 Q03 Q04 Q05 Q06 Q07 Q08 Q09 Q10 ## Q01 0.565 0.013 0.035 -0.011 0.027 -0.001 -0.061 -0.081 -0.050 0.042 ## Q02 0.013 0.586 -0.062 0.022 0.003 -0.041 -0.011 -0.052 -0.115 -0.023 ## Q03 0.035 -0.062 0.470 0.019 0.035 -0.027 -0.009 0.011 -0.052 -0.013 ## Q04 -0.011 0.022 0.019 0.531 0.002 0.000 -0.010 -0.041 -0.051 0.003 ## Q05 0.027 0.003 0.035 0.002 0.657 -0.016 -0.041 -0.044 -0.016 0.053 ## Q06 -0.001 -0.041 -0.027 0.000 -0.016 0.346 -0.014 0.040 -0.005 -0.139 ## Q07 -0.061 -0.011 -0.009 -0.010 -0.041 -0.014 0.455 0.030 0.033 -0.098 ## Q08 -0.081 -0.052 0.011 -0.041 -0.044 0.040 0.030 0.261 -0.039 -0.021 ## Q09 -0.050 -0.115 -0.052 -0.051 -0.016 -0.005 0.033 -0.039 0.516 -0.018 ## Q10 0.042 -0.023 -0.013 0.003 0.053 -0.139 -0.098 -0.021 -0.018 0.665 ## Q11 -0.066 -0.046 0.006 -0.051 -0.050 0.038 -0.018 -0.061 -0.045 0.012 ## Q12 -0.057 0.024 0.030 -0.006 -0.050 -0.076 -0.072 0.024 0.027 -0.038 ## Q13 0.008 -0.021 0.024 -0.051 -0.058 -0.078 -0.091 0.001 -0.021 -0.096 ## Q14 -0.024 -0.009 0.002 -0.060 -0.055 -0.075 -0.074 0.032 0.038 -0.091 ## Q15 -0.065 -0.007 0.025 -0.009 -0.045 -0.047 -0.033 -0.039 -0.012 -0.018 ## Q16 0.059 0.050 0.039 -0.050 -0.005 -0.056 -0.051 -0.068 -0.014 0.052 ## Q17 -0.069 -0.039 0.003 -0.052 -0.049 -0.008 0.025 -0.105 -0.027 -0.033 ## Q18 -0.020 -0.015 0.001 -0.042 -0.066 -0.048 -0.069 0.030 0.018 -0.110 ## Q19 0.015 -0.153 -0.061 0.045 0.041 -0.020 -0.015 -0.056 -0.114 0.010 ## Q20 -0.128 0.099 0.115 -0.110 -0.092 0.122 0.002 0.011 0.060 0.078 ## Q21 -0.120 0.049 0.071 -0.070 -0.078 0.029 0.053 0.014 0.055 0.005 ## Q22 -0.079 -0.102 -0.071 -0.049 -0.072 0.043 0.010 0.020 -0.161 0.066 ## Q23 -0.049 -0.147 -0.008 -0.076 -0.070 0.013 -0.033 0.086 -0.152 0.048 ## Q11 Q12 Q13 Q14 Q15 Q16 Q17 Q18 Q19 Q20 ## Q01 -0.066 -0.057 0.008 -0.024 -0.065 0.059 -0.069 -0.020 0.015 -0.128 ## Q02 -0.046 0.024 -0.021 -0.009 -0.007 0.050 -0.039 -0.015 -0.153 0.099 ## Q03 0.006 0.030 0.024 0.002 0.025 0.039 0.003 0.001 -0.061 0.115 ## Q04 -0.051 -0.006 -0.051 -0.060 -0.009 -0.050 -0.052 -0.042 0.045 -0.110 ## Q05 -0.050 -0.050 -0.058 -0.055 -0.045 -0.005 -0.049 -0.066 0.041 -0.092 ## Q06 0.038 -0.076 -0.078 -0.075 -0.047 -0.056 -0.008 -0.048 -0.020 0.122 ## Q07 -0.018 -0.072 -0.091 -0.074 -0.033 -0.051 0.025 -0.069 -0.015 0.002 ## Q08 -0.061 0.024 0.001 0.032 -0.039 -0.068 -0.105 0.030 -0.056 0.011 ## Q09 -0.045 0.027 -0.021 0.038 -0.012 -0.014 -0.027 0.018 -0.114 0.060 ## Q10 0.012 -0.038 -0.096 -0.091 -0.018 0.052 -0.033 -0.110 0.010 0.078 ## Q11 0.310 0.020 0.020 -0.013 -0.045 -0.075 -0.094 0.020 -0.002 0.056 ## Q12 0.020 0.487 0.030 -0.048 -0.042 -0.058 0.014 -0.020 0.036 -0.056 ## Q13 0.020 0.030 0.464 -0.047 -0.091 -0.061 0.006 -0.023 0.004 0.041 ## Q14 -0.013 -0.048 -0.047 0.512 -0.017 -0.011 0.012 -0.038 0.000 -0.015 ## Q15 -0.045 -0.042 -0.091 -0.017 0.622 0.077 -0.025 -0.094 0.027 0.031 ## Q16 -0.075 -0.058 -0.061 -0.011 0.077 0.513 -0.034 -0.024 0.032 -0.108 ## Q17 -0.094 0.014 0.006 0.012 -0.025 -0.034 0.317 0.019 -0.001 0.009 ## Q18 0.020 -0.020 -0.023 -0.038 -0.094 -0.024 0.019 0.403 0.003 0.020 ## Q19 -0.002 0.036 0.004 0.000 0.027 0.032 -0.001 0.003 0.657 0.060 ## Q20 0.056 -0.056 0.041 -0.015 0.031 -0.108 0.009 0.020 0.060 0.516 ## Q21 0.005 -0.062 -0.010 -0.032 -0.036 -0.074 0.016 -0.009 0.049 0.010 ## Q22 0.047 -0.031 0.007 -0.011 0.062 -0.004 0.019 0.023 -0.060 -0.032 ## Q23 0.116 -0.057 0.026 -0.027 0.079 -0.032 0.049 -0.049 -0.073 -0.056 ## Q21 Q22 Q23 ## Q01 -0.120 -0.079 -0.049 ## Q02 0.049 -0.102 -0.147 ## Q03 0.071 -0.071 -0.008 ## Q04 -0.070 -0.049 -0.076 ## Q05 -0.078 -0.072 -0.070 ## Q06 0.029 0.043 0.013 ## Q07 0.053 0.010 -0.033 ## Q08 0.014 0.020 0.086 ## Q09 0.055 -0.161 -0.152 ## Q10 0.005 0.066 0.048 ## Q11 0.005 0.047 0.116 ## Q12 -0.062 -0.031 -0.057 ## Q13 -0.010 0.007 0.026 ## Q14 -0.032 -0.011 -0.027 ## Q15 -0.036 0.062 0.079 ## Q16 -0.074 -0.004 -0.032 ## Q17 0.016 0.019 0.049 ## Q18 -0.009 0.023 -0.049 ## Q19 0.049 -0.060 -0.073 ## Q20 0.010 -0.032 -0.056 ## Q21 0.450 -0.033 -0.100 ## Q22 -0.033 0.536 -0.177 ## Q23 -0.100 -0.177 0.588 Note that the diagonal elements in the residual matrix correspond to the unique variance in each variable that cannot be explained by the factors (i.e., “u2” in the output above). For example, the proportion of unique variance for question 1 is 0.57, which is reflected in the first cell in the residual matrix. The off-diagonal elements represent the difference between the actual correlations and the correlation based on the reproduced correlation matrix for all variable pairs. To see this, the reproduced correlation matrix could be generated by using the factor.model() function: reproduced_matrix &lt;- factor.model(pc2$loadings) round(reproduced_matrix, 3) ## Q01 Q02 Q03 Q04 Q05 Q06 Q07 Q08 Q09 Q10 ## Q01 0.435 -0.112 -0.372 0.447 0.376 0.218 0.366 0.412 -0.042 0.172 ## Q02 -0.112 0.414 0.380 -0.134 -0.122 -0.033 -0.148 0.002 0.430 -0.061 ## Q03 -0.372 0.380 0.530 -0.399 -0.345 -0.200 -0.373 -0.270 0.352 -0.181 ## Q04 0.447 -0.134 -0.399 0.469 0.399 0.278 0.419 0.390 -0.073 0.212 ## Q05 0.376 -0.122 -0.345 0.399 0.343 0.273 0.380 0.312 -0.080 0.205 ## Q06 0.218 -0.033 -0.200 0.278 0.273 0.654 0.528 0.183 -0.108 0.461 ## Q07 0.366 -0.148 -0.373 0.419 0.380 0.528 0.545 0.267 -0.161 0.382 ## Q08 0.412 0.002 -0.270 0.390 0.312 0.183 0.267 0.739 0.055 0.180 ## Q09 -0.042 0.430 0.352 -0.073 -0.080 -0.108 -0.161 0.055 0.484 -0.116 ## Q10 0.172 -0.061 -0.181 0.212 0.205 0.461 0.382 0.180 -0.116 0.335 ## Q11 0.423 -0.097 -0.357 0.419 0.348 0.290 0.363 0.691 -0.071 0.259 ## Q12 0.402 -0.219 -0.440 0.448 0.397 0.388 0.495 0.228 -0.195 0.283 ## Q13 0.347 -0.122 -0.342 0.395 0.360 0.545 0.533 0.313 -0.147 0.398 ## Q14 0.362 -0.155 -0.373 0.411 0.370 0.477 0.514 0.249 -0.159 0.345 ## Q15 0.311 -0.158 -0.337 0.343 0.306 0.406 0.425 0.339 -0.174 0.314 ## Q16 0.440 -0.217 -0.458 0.466 0.400 0.300 0.439 0.390 -0.175 0.239 ## Q17 0.439 -0.048 -0.331 0.434 0.359 0.290 0.365 0.695 -0.009 0.252 ## Q18 0.368 -0.149 -0.376 0.424 0.388 0.562 0.570 0.250 -0.168 0.403 ## Q19 -0.204 0.357 0.403 -0.231 -0.207 -0.147 -0.254 -0.104 0.363 -0.137 ## Q20 0.342 -0.301 -0.440 0.353 0.292 -0.021 0.219 0.164 -0.218 0.006 ## Q21 0.449 -0.254 -0.488 0.480 0.412 0.244 0.430 0.282 -0.191 0.188 ## Q22 -0.025 0.333 0.275 -0.050 -0.060 -0.209 -0.179 -0.099 0.417 -0.197 ## Q23 0.045 0.246 0.158 0.042 0.028 -0.082 -0.037 -0.136 0.323 -0.110 ## Q11 Q12 Q13 Q14 Q15 Q16 Q17 Q18 Q19 Q20 ## Q01 0.423 0.402 0.347 0.362 0.311 0.440 0.439 0.368 -0.204 0.342 ## Q02 -0.097 -0.219 -0.122 -0.155 -0.158 -0.217 -0.048 -0.149 0.357 -0.301 ## Q03 -0.357 -0.440 -0.342 -0.373 -0.337 -0.458 -0.331 -0.376 0.403 -0.440 ## Q04 0.419 0.448 0.395 0.411 0.343 0.466 0.434 0.424 -0.231 0.353 ## Q05 0.348 0.397 0.360 0.370 0.306 0.400 0.359 0.388 -0.207 0.292 ## Q06 0.290 0.388 0.545 0.477 0.406 0.300 0.290 0.562 -0.147 -0.021 ## Q07 0.363 0.495 0.533 0.514 0.425 0.439 0.365 0.570 -0.254 0.219 ## Q08 0.691 0.228 0.313 0.249 0.339 0.390 0.695 0.250 -0.104 0.164 ## Q09 -0.071 -0.195 -0.147 -0.159 -0.174 -0.175 -0.009 -0.168 0.363 -0.218 ## Q10 0.259 0.283 0.398 0.345 0.314 0.239 0.252 0.403 -0.137 0.006 ## Q11 0.690 0.315 0.403 0.338 0.410 0.444 0.681 0.353 -0.198 0.200 ## Q12 0.315 0.513 0.459 0.481 0.374 0.466 0.319 0.513 -0.302 0.354 ## Q13 0.403 0.459 0.536 0.497 0.433 0.419 0.402 0.556 -0.231 0.163 ## Q14 0.338 0.481 0.497 0.488 0.397 0.429 0.342 0.537 -0.254 0.241 ## Q15 0.410 0.374 0.433 0.397 0.378 0.378 0.399 0.437 -0.236 0.175 ## Q16 0.444 0.466 0.419 0.429 0.378 0.487 0.443 0.446 -0.299 0.373 ## Q17 0.681 0.319 0.402 0.342 0.399 0.443 0.683 0.357 -0.162 0.196 ## Q18 0.353 0.513 0.556 0.537 0.437 0.446 0.357 0.597 -0.259 0.215 ## Q19 -0.198 -0.302 -0.231 -0.254 -0.236 -0.299 -0.162 -0.259 0.343 -0.308 ## Q20 0.200 0.354 0.163 0.241 0.175 0.373 0.196 0.215 -0.308 0.484 ## Q21 0.342 0.503 0.384 0.431 0.336 0.494 0.347 0.439 -0.324 0.457 ## Q22 -0.209 -0.136 -0.203 -0.159 -0.230 -0.152 -0.145 -0.183 0.294 -0.068 ## Q23 -0.202 0.011 -0.079 -0.022 -0.141 -0.049 -0.140 -0.032 0.196 0.021 ## Q21 Q22 Q23 ## Q01 0.449 -0.025 0.045 ## Q02 -0.254 0.333 0.246 ## Q03 -0.488 0.275 0.158 ## Q04 0.480 -0.050 0.042 ## Q05 0.412 -0.060 0.028 ## Q06 0.244 -0.209 -0.082 ## Q07 0.430 -0.179 -0.037 ## Q08 0.282 -0.099 -0.136 ## Q09 -0.191 0.417 0.323 ## Q10 0.188 -0.197 -0.110 ## Q11 0.342 -0.209 -0.202 ## Q12 0.503 -0.136 0.011 ## Q13 0.384 -0.203 -0.079 ## Q14 0.431 -0.159 -0.022 ## Q15 0.336 -0.230 -0.141 ## Q16 0.494 -0.152 -0.049 ## Q17 0.347 -0.145 -0.140 ## Q18 0.439 -0.183 -0.032 ## Q19 -0.324 0.294 0.196 ## Q20 0.457 -0.068 0.021 ## Q21 0.550 -0.096 0.032 ## Q22 -0.096 0.464 0.408 ## Q23 0.032 0.408 0.412 You can see that the reproduced correlation between the first and second variable is -0.112. From the correlation table from the beginning we know, however, that the observed correlation was -0.099. Hence, the difference between the observed and reproduced correlation is: (-0.099)-(-0.112) = 0.013, which corresponds to the residual of this variable pair in the residual matrix. Note that the diagonal elements in the reproduced matrix correspond to the communalities in the model summary above (i.e., “h2”). A measure of fit can now be computed based on the size of the residuals. In the worst case, the residuals would be as large as the correlations in the original matrix, which would be the case if we extracted no factors at all. A measure of fit could therefore be the sum of the squared residuals divided by the sum of the squared correlations. We square the residuals to account for positive and negative deviations. Values &gt;0.90 are considered indicators of good fit. From the output above, you can see that: “Fit based upon off diagonal values = 0.96”. Thus, we conclude that the model fit is sufficient. You could also manually compute this statistic by summing over the squared residuals and correlations, take their ratio and subtract it from one (note that we use the upper.tri() function to use the upper triangle of the matrix only; this has the effect of discarding the diagonal elements and the elements below the diagonal). ssr &lt;- (sum(residuals[upper.tri((residuals))]^2)) #sum of squared residuals ssc &lt;- (sum(raq_matrix[upper.tri((raq_matrix))]^2)) #sum of squared correlations 1 - (ssr/ssc) #model fit ## [1] 0.9645252 In a next step, we check the size of the residuals. If fewer residuals than 50% have absolute values greater than 0.05 the model is a good fit. This can be tested using the following code. We first convert the residuals to a matrix and select the upper triangular again to avoid duplicates. Finally, we count all occurrences where the absolute value is larger than 0.05 and divide it by the number of total observations to get the proportion. residuals &lt;- as.matrix(residuals[upper.tri((residuals))]) large_res &lt;- abs(residuals) &gt; 0.05 sum(large_res) ## [1] 91 sum(large_res)/nrow(residuals) ## [1] 0.3596838 In our example, we can confirm that the proportion of residuals &gt; 0.05 is less than 50%. Another way to evaluate the residuals is by looking at their mean value (rather, we square the residuals first to account for positive and negative values, compute the mean and then take the square root). sqrt(mean(residuals^2)) ## [1] 0.05549286 This means that our mean residual is 0.055 and this value should be as low as possible. Finally, we need to validate if the residuals are approximately normally distributed, which we do by using a histogram, a Q-Q plot and the Shapiro test. hist(residuals) Figure 1.21: Hinstogram of residuals qqnorm(residuals) qqline(residuals) Figure 1.22: Q-Q plot shapiro.test(residuals) ## ## Shapiro-Wilk normality test ## ## data: residuals ## W = 0.99436, p-value = 0.4691 All of the tests suggest that the distribution of the residuals is approximately normal. 7.1.2.3 Factor interpretation To aid interpretation, it is possible to maximize the loading of a variable on one factor while minimizing its loading on all other factors. This is known as factor rotation. There are two types of factor rotation: orthogonal (assumes that factors are uncorrelated) oblique (assumes that factors are intercorrelated) To carry out a orthogonal rotation, we change the rotate option in the principal() function from “none” to “varimax” (we could also exclude it altogether because varimax is the default if the option is not specified): pc3 &lt;- principal(raq_data, nfactors = 4, rotate = &quot;varimax&quot;) pc3 ## Principal Components Analysis ## Call: principal(r = raq_data, nfactors = 4, rotate = &quot;varimax&quot;) ## Standardized loadings (pattern matrix) based upon correlation matrix ## RC3 RC1 RC4 RC2 h2 u2 com ## Q01 0.24 0.50 0.36 0.06 0.43 0.57 2.4 ## Q02 -0.01 -0.34 0.07 0.54 0.41 0.59 1.7 ## Q03 -0.20 -0.57 -0.18 0.37 0.53 0.47 2.3 ## Q04 0.32 0.52 0.31 0.04 0.47 0.53 2.4 ## Q05 0.32 0.43 0.24 0.01 0.34 0.66 2.5 ## Q06 0.80 -0.01 0.10 -0.07 0.65 0.35 1.0 ## Q07 0.64 0.33 0.16 -0.08 0.55 0.45 1.7 ## Q08 0.13 0.17 0.83 0.01 0.74 0.26 1.1 ## Q09 -0.09 -0.20 0.12 0.65 0.48 0.52 1.3 ## Q10 0.55 0.00 0.13 -0.12 0.33 0.67 1.2 ## Q11 0.26 0.21 0.75 -0.14 0.69 0.31 1.5 ## Q12 0.47 0.52 0.09 -0.08 0.51 0.49 2.1 ## Q13 0.65 0.23 0.23 -0.10 0.54 0.46 1.6 ## Q14 0.58 0.36 0.14 -0.07 0.49 0.51 1.8 ## Q15 0.46 0.22 0.29 -0.19 0.38 0.62 2.6 ## Q16 0.33 0.51 0.31 -0.12 0.49 0.51 2.6 ## Q17 0.27 0.22 0.75 -0.04 0.68 0.32 1.5 ## Q18 0.68 0.33 0.13 -0.08 0.60 0.40 1.5 ## Q19 -0.15 -0.37 -0.03 0.43 0.34 0.66 2.2 ## Q20 -0.04 0.68 0.07 -0.14 0.48 0.52 1.1 ## Q21 0.29 0.66 0.16 -0.07 0.55 0.45 1.5 ## Q22 -0.19 0.03 -0.10 0.65 0.46 0.54 1.2 ## Q23 -0.02 0.17 -0.20 0.59 0.41 0.59 1.4 ## ## RC3 RC1 RC4 RC2 ## SS loadings 3.73 3.34 2.55 1.95 ## Proportion Var 0.16 0.15 0.11 0.08 ## Cumulative Var 0.16 0.31 0.42 0.50 ## Proportion Explained 0.32 0.29 0.22 0.17 ## Cumulative Proportion 0.32 0.61 0.83 1.00 ## ## Mean item complexity = 1.8 ## Test of the hypothesis that 4 components are sufficient. ## ## The root mean square of the residuals (RMSR) is 0.06 ## with the empirical chi square 4006.15 with prob &lt; 0 ## ## Fit based upon off diagonal values = 0.96 Interpreting the factor loading matrix is a little complex, so we can make it easier by using the print.psych() function, which we can use to exclude loading below a cutoff from the display and order the variables by their loading within each factor. In the following, we will only display loadings that exceed the value 0.3. print.psych(pc3, cut = 0.3, sort = TRUE) ## Principal Components Analysis ## Call: principal(r = raq_data, nfactors = 4, rotate = &quot;varimax&quot;) ## Standardized loadings (pattern matrix) based upon correlation matrix ## item RC3 RC1 RC4 RC2 h2 u2 com ## Q06 6 0.80 0.65 0.35 1.0 ## Q18 18 0.68 0.33 0.60 0.40 1.5 ## Q13 13 0.65 0.54 0.46 1.6 ## Q07 7 0.64 0.33 0.55 0.45 1.7 ## Q14 14 0.58 0.36 0.49 0.51 1.8 ## Q10 10 0.55 0.33 0.67 1.2 ## Q15 15 0.46 0.38 0.62 2.6 ## Q20 20 0.68 0.48 0.52 1.1 ## Q21 21 0.66 0.55 0.45 1.5 ## Q03 3 -0.57 0.37 0.53 0.47 2.3 ## Q12 12 0.47 0.52 0.51 0.49 2.1 ## Q04 4 0.32 0.52 0.31 0.47 0.53 2.4 ## Q16 16 0.33 0.51 0.31 0.49 0.51 2.6 ## Q01 1 0.50 0.36 0.43 0.57 2.4 ## Q05 5 0.32 0.43 0.34 0.66 2.5 ## Q08 8 0.83 0.74 0.26 1.1 ## Q17 17 0.75 0.68 0.32 1.5 ## Q11 11 0.75 0.69 0.31 1.5 ## Q09 9 0.65 0.48 0.52 1.3 ## Q22 22 0.65 0.46 0.54 1.2 ## Q23 23 0.59 0.41 0.59 1.4 ## Q02 2 -0.34 0.54 0.41 0.59 1.7 ## Q19 19 -0.37 0.43 0.34 0.66 2.2 ## ## RC3 RC1 RC4 RC2 ## SS loadings 3.73 3.34 2.55 1.95 ## Proportion Var 0.16 0.15 0.11 0.08 ## Cumulative Var 0.16 0.31 0.42 0.50 ## Proportion Explained 0.32 0.29 0.22 0.17 ## Cumulative Proportion 0.32 0.61 0.83 1.00 ## ## Mean item complexity = 1.8 ## Test of the hypothesis that 4 components are sufficient. ## ## The root mean square of the residuals (RMSR) is 0.06 ## with the empirical chi square 4006.15 with prob &lt; 0 ## ## Fit based upon off diagonal values = 0.96 After obtaining the rotated matrix, variables with high loading are used for interpreting (=naming) the factor. Note that factor loading can be positive or negative (depends on scaling of the variable), thus take care when interpreting! Look for simple structure: each variable (hopefully) loads high on 1 factor and low on other factors. As an example, we could name our factors as follows: Factor 1: fear of computers Factor 2: fear of statistics Factor 3: fear of maths Factor 4: Peer evaluation The previous type of rotation (i.e., “varimax”) assumed that the the factors are independent. Oblique rotation is another type of rotation that can handle correlation between the factors. The command for an oblique rotation is very similar to that for an orthogonal rotation – we just change the rotate option from “varimax” to “oblimin”. pc4 &lt;- principal(raq_data, nfactors = 4, rotate = &quot;oblimin&quot;, scores = TRUE) print.psych(pc4, cut = 0.3, sort = TRUE) ## Principal Components Analysis ## Call: principal(r = raq_data, nfactors = 4, rotate = &quot;oblimin&quot;, scores = TRUE) ## Standardized loadings (pattern matrix) based upon correlation matrix ## item TC1 TC4 TC3 TC2 h2 u2 com ## Q06 6 0.87 0.65 0.35 1.1 ## Q18 18 0.70 0.60 0.40 1.1 ## Q07 7 0.64 0.55 0.45 1.2 ## Q13 13 0.64 0.54 0.46 1.1 ## Q10 10 0.57 0.33 0.67 1.2 ## Q14 14 0.57 0.49 0.51 1.3 ## Q12 12 0.45 0.43 0.51 0.49 2.0 ## Q15 15 0.40 0.38 0.62 1.9 ## Q08 8 0.90 0.74 0.26 1.0 ## Q11 11 0.78 0.69 0.31 1.0 ## Q17 17 0.78 0.68 0.32 1.0 ## Q20 20 0.71 0.48 0.52 1.1 ## Q21 21 0.60 0.55 0.45 1.3 ## Q03 3 -0.51 0.53 0.47 1.8 ## Q04 4 0.41 0.47 0.53 2.6 ## Q16 16 0.41 0.49 0.51 2.4 ## Q01 1 0.33 0.40 0.43 0.57 2.4 ## Q05 5 0.34 0.34 0.66 2.7 ## Q22 22 0.65 0.46 0.54 1.2 ## Q09 9 0.63 0.48 0.52 1.4 ## Q23 23 0.61 0.41 0.59 1.6 ## Q02 2 -0.36 0.51 0.41 0.59 1.9 ## Q19 19 -0.35 0.38 0.34 0.66 2.1 ## ## TC1 TC4 TC3 TC2 ## SS loadings 3.90 2.88 2.94 1.85 ## Proportion Var 0.17 0.13 0.13 0.08 ## Cumulative Var 0.17 0.29 0.42 0.50 ## Proportion Explained 0.34 0.25 0.25 0.16 ## Cumulative Proportion 0.34 0.59 0.84 1.00 ## ## With component correlations of ## TC1 TC4 TC3 TC2 ## TC1 1.00 0.44 0.36 -0.18 ## TC4 0.44 1.00 0.31 -0.10 ## TC3 0.36 0.31 1.00 -0.17 ## TC2 -0.18 -0.10 -0.17 1.00 ## ## Mean item complexity = 1.6 ## Test of the hypothesis that 4 components are sufficient. ## ## The root mean square of the residuals (RMSR) is 0.06 ## with the empirical chi square 4006.15 with prob &lt; 0 ## ## Fit based upon off diagonal values = 0.96 The component correlations indicate that the factors might indeed be correlated, so oblique rotation might actually be more appropriate in this case. 7.1.2.4 Creating new variables Once we have decided on the final model, we can calculate the new variables as the weighted sum of the variables that form a factor. This means, we estimate a person’s score on a factor based on their scores on the items that constitute the measurement scales. These scores are also referred to as the factor scores. Because we have used the scores = TRUE argument in the previous command, the factor scores have already been created for us. By default, R uses the regression method to compute the factor scores, which controls for differences in the units of measurement. You can access the residuals as follows: head(pc4$scores) ## TC1 TC4 TC3 TC2 ## [1,] 0.37296709 1.8808424 0.95979596 0.3910711 ## [2,] 0.63334164 0.2374679 0.29090777 -0.3504080 ## [3,] 0.39712768 -0.1056263 -0.09333769 0.9249353 ## [4,] -0.78741595 0.2956628 -0.77703307 0.2605666 ## [5,] 0.04425942 0.6815179 0.59786611 -0.6912687 ## [6,] -1.70018648 0.2091685 0.02784164 0.6653081 We can also use the cbind() function to add the computed factor scores to the existing data set: raq_data &lt;- cbind(raq_data, pc4$scores) This way, it is easier to use the new variables in subsequent analysis (e.g., t-tests, regression, ANOVA, cluster analysis). 7.1.2.5 Reliability analysis When you are using multi-item scales to measure a latent construct (e.g., the output from the PCA above), it is useful to check the reliability of your scale. Reliability means that our items should consistently reflect the construct that they are intended to measure. In other words, individual items should produce results consistent with the overall scale. This means that for a scale to be reliable, the score of a person on one half of the items should be similar to the score derived based on the other half of the items (split-half reliability). The problem is that there are several ways in which data can be split. A measure that reflects this underlying intuition is Cronbach’s alpha, which is approximately equal to the average of all possible split-half reliabilities. It is computed as follows: \\[\\begin{equation} \\alpha=\\frac{N^2\\overline{Cov}}{\\sum{s^2_{item}}+\\sum{Cov_{item}}} \\tag{7.8} \\end{equation}\\] The share of the items common variance (inter-correlation) in the total variance is supposed to be as high as possible across all items. The thresholds are as follows: &gt;0.7 reasonable for practical application/exploratory research &gt;0.8 necessary for fundamental research &gt;0.9 desirable for applied research To see if the subscales that were derived from the previous PCA exhibit a sufficient degree of reliability, we first create subsets of our data set that contain the respective items for each of the factors (we use the results from the oblimin rotation here): computer_fear &lt;- raq_data[, c(6, 7, 10, 13, 14, 15, 18)] statistics_fear &lt;- raq_data[, c(1, 3, 4, 5, 12, 16, 20, 21)] math_fear &lt;- raq_data[, c(8, 11, 17)] peer_evaluation &lt;- raq_data[, c(2, 9, 19, 22, 23)] Now we can use the alpha() function from the psych package to test the reliability. Note that the keys argument may be used to indicate reverse coded items. In the example below, the second item of the “statistics_fear” factor is reverse coded as indicated by the -1, meaning that it is phrased in a “positive” way, while the remaining items belonging to this factor are phrased in a “negative” way. This is often done to check if respondents are giving consistent answers. psych::alpha(computer_fear) ## ## Reliability analysis ## Call: psych::alpha(x = computer_fear) ## ## raw_alpha std.alpha G6(smc) average_r S/N ase mean sd median_r ## 0.82 0.82 0.81 0.4 4.6 0.0052 3.4 0.71 0.39 ## ## 95% confidence boundaries ## lower alpha upper ## Feldt 0.81 0.82 0.83 ## Duhachek 0.81 0.82 0.83 ## ## Reliability if an item is dropped: ## raw_alpha std.alpha G6(smc) average_r S/N alpha se var.r med.r ## Q06 0.79 0.79 0.77 0.38 3.7 0.0063 0.0081 0.38 ## Q07 0.79 0.79 0.77 0.38 3.7 0.0063 0.0079 0.36 ## Q10 0.82 0.82 0.80 0.44 4.7 0.0053 0.0043 0.44 ## Q13 0.79 0.79 0.77 0.39 3.8 0.0062 0.0081 0.38 ## Q14 0.80 0.80 0.77 0.39 3.9 0.0060 0.0085 0.36 ## Q15 0.81 0.81 0.79 0.41 4.2 0.0056 0.0095 0.44 ## Q18 0.79 0.78 0.76 0.38 3.6 0.0064 0.0058 0.38 ## ## Item statistics ## n raw.r std.r r.cor r.drop mean sd ## Q06 2571 0.75 0.74 0.68 0.62 3.8 1.12 ## Q07 2571 0.75 0.73 0.68 0.62 3.1 1.10 ## Q10 2571 0.54 0.57 0.44 0.40 3.7 0.88 ## Q13 2571 0.72 0.73 0.67 0.61 3.6 0.95 ## Q14 2571 0.70 0.70 0.64 0.58 3.1 1.00 ## Q15 2571 0.64 0.64 0.54 0.49 3.2 1.01 ## Q18 2571 0.76 0.76 0.72 0.65 3.4 1.05 ## ## Non missing response frequency for each item ## 1 2 3 4 5 miss ## Q06 0.06 0.10 0.13 0.44 0.27 0 ## Q07 0.09 0.24 0.26 0.34 0.07 0 ## Q10 0.02 0.10 0.18 0.57 0.14 0 ## Q13 0.03 0.12 0.25 0.48 0.12 0 ## Q14 0.07 0.18 0.38 0.31 0.06 0 ## Q15 0.06 0.18 0.30 0.39 0.07 0 ## Q18 0.06 0.12 0.31 0.37 0.14 0 psych::alpha(statistics_fear, keys = c(1, -1, 1, 1, 1, 1, 1, 1)) ## ## Reliability analysis ## Call: psych::alpha(x = statistics_fear, keys = c(1, -1, 1, 1, 1, 1, ## 1, 1)) ## ## raw_alpha std.alpha G6(smc) average_r S/N ase mean sd median_r ## 0.82 0.82 0.81 0.37 4.7 0.0053 3 0.64 0.4 ## ## 95% confidence boundaries ## lower alpha upper ## Feldt 0.81 0.82 0.83 ## Duhachek 0.81 0.82 0.83 ## ## Reliability if an item is dropped: ## raw_alpha std.alpha G6(smc) average_r S/N alpha se var.r med.r ## Q01 0.80 0.80 0.79 0.37 4.1 0.0060 0.0052 0.40 ## Q03- 0.80 0.80 0.79 0.37 4.1 0.0061 0.0070 0.40 ## Q04 0.80 0.80 0.78 0.36 4.0 0.0062 0.0061 0.35 ## Q05 0.81 0.81 0.80 0.38 4.2 0.0058 0.0058 0.41 ## Q12 0.80 0.80 0.79 0.36 4.0 0.0061 0.0067 0.39 ## Q16 0.79 0.80 0.78 0.36 3.9 0.0062 0.0057 0.35 ## Q20 0.82 0.82 0.80 0.40 4.6 0.0055 0.0022 0.41 ## Q21 0.79 0.80 0.78 0.36 3.9 0.0063 0.0063 0.38 ## ## Item statistics ## n raw.r std.r r.cor r.drop mean sd ## Q01 2571 0.65 0.67 0.60 0.54 3.6 0.83 ## Q03- 2571 0.69 0.67 0.60 0.55 2.6 1.08 ## Q04 2571 0.69 0.70 0.64 0.58 3.2 0.95 ## Q05 2571 0.63 0.63 0.55 0.49 3.3 0.96 ## Q12 2571 0.69 0.69 0.63 0.57 2.8 0.92 ## Q16 2571 0.71 0.71 0.67 0.60 3.1 0.92 ## Q20 2571 0.58 0.56 0.47 0.42 2.4 1.04 ## Q21 2571 0.72 0.71 0.67 0.61 2.8 0.98 ## ## Non missing response frequency for each item ## 1 2 3 4 5 miss ## Q01 0.02 0.07 0.29 0.52 0.11 0 ## Q03 0.03 0.17 0.34 0.26 0.19 0 ## Q04 0.05 0.17 0.36 0.37 0.05 0 ## Q05 0.04 0.18 0.29 0.43 0.06 0 ## Q12 0.09 0.23 0.46 0.20 0.02 0 ## Q16 0.06 0.16 0.42 0.33 0.04 0 ## Q20 0.22 0.37 0.25 0.15 0.02 0 ## Q21 0.09 0.29 0.34 0.26 0.02 0 psych::alpha(math_fear) ## ## Reliability analysis ## Call: psych::alpha(x = math_fear) ## ## raw_alpha std.alpha G6(smc) average_r S/N ase mean sd median_r ## 0.82 0.82 0.75 0.6 4.5 0.0062 3.7 0.75 0.59 ## ## 95% confidence boundaries ## lower alpha upper ## Feldt 0.81 0.82 0.83 ## Duhachek 0.81 0.82 0.83 ## ## Reliability if an item is dropped: ## raw_alpha std.alpha G6(smc) average_r S/N alpha se var.r med.r ## Q08 0.74 0.74 0.59 0.59 2.8 0.010 NA 0.59 ## Q11 0.74 0.74 0.59 0.59 2.9 0.010 NA 0.59 ## Q17 0.77 0.77 0.63 0.63 3.4 0.009 NA 0.63 ## ## Item statistics ## n raw.r std.r r.cor r.drop mean sd ## Q08 2571 0.86 0.86 0.76 0.68 3.8 0.87 ## Q11 2571 0.86 0.86 0.75 0.68 3.7 0.88 ## Q17 2571 0.85 0.85 0.72 0.65 3.5 0.88 ## ## Non missing response frequency for each item ## 1 2 3 4 5 miss ## Q08 0.03 0.06 0.19 0.58 0.15 0 ## Q11 0.02 0.06 0.22 0.53 0.16 0 ## Q17 0.03 0.10 0.27 0.52 0.08 0 psych::alpha(peer_evaluation) ## ## Reliability analysis ## Call: psych::alpha(x = peer_evaluation) ## ## raw_alpha std.alpha G6(smc) average_r S/N ase mean sd median_r ## 0.57 0.57 0.53 0.21 1.3 0.013 3.4 0.65 0.23 ## ## 95% confidence boundaries ## lower alpha upper ## Feldt 0.54 0.57 0.6 ## Duhachek 0.54 0.57 0.6 ## ## Reliability if an item is dropped: ## raw_alpha std.alpha G6(smc) average_r S/N alpha se var.r med.r ## Q02 0.52 0.52 0.45 0.21 1.07 0.015 0.0028 0.23 ## Q09 0.48 0.48 0.41 0.19 0.92 0.017 0.0036 0.22 ## Q19 0.52 0.53 0.46 0.22 1.11 0.015 0.0055 0.23 ## Q22 0.49 0.49 0.43 0.19 0.96 0.016 0.0065 0.19 ## Q23 0.56 0.57 0.50 0.25 1.32 0.014 0.0014 0.24 ## ## Item statistics ## n raw.r std.r r.cor r.drop mean sd ## Q02 2571 0.56 0.61 0.45 0.34 4.4 0.85 ## Q09 2571 0.70 0.66 0.53 0.39 3.2 1.26 ## Q19 2571 0.61 0.60 0.42 0.32 3.7 1.10 ## Q22 2571 0.64 0.64 0.50 0.38 3.1 1.04 ## Q23 2571 0.53 0.53 0.31 0.24 2.6 1.04 ## ## Non missing response frequency for each item ## 1 2 3 4 5 miss ## Q02 0.01 0.04 0.08 0.31 0.56 0 ## Q09 0.08 0.28 0.23 0.20 0.20 0 ## Q19 0.02 0.15 0.22 0.33 0.29 0 ## Q22 0.05 0.26 0.34 0.26 0.10 0 ## Q23 0.12 0.42 0.27 0.12 0.06 0 The above output would lead us to conclude that the fear of computers, fear of statistics and fear of maths subscales of the RAQ all had sufficiently high levels of reliability (i.e., Cronbach’s alpha &gt; 0.70). However, the fear of negative peer evaluation subscale had relatively low reliability (Cronbach’s alpha = 0.57). As the output under “Reliability if an item is dropped” suggests, the alpha score would also not increase if an item was dropped from the scale. 7.2 Cluster analysis In the previous chapter on factor analysis we tried to reduce the number of variables or columns by identifying underlying dimensions. In order to do so we exploited the fact that some items are highly correlated and therefore might represent the same underlying concept (e.g., health benefits or social benefits). Similarly, in cluster analysis we again do not distinguish between dependent and independent variables. However, in the case of cluster analysis we do not try to reduce the number of variables but the number of observations by grouping similar ones into “clusters”. What exactly defines “similarity” depends on the use case. In the case of retailing, we can consider customer segmentation that can, for example, support the recommender systems, used in e-commerce. First, to give an illustrative example of how clusters might look, let’s tale a step back to music data. Let’s try to create a recommendation system using song features. In our data we have the ISRC, name of the track, name of the artist and audio features of the track. We are going to use the audio features to cluster tracks together such that given one track we can easily identify similar tracks by looking at which cluster it belongs to. library(NbClust) load(url(&quot;https://github.com/WU-RDS/MRDA2021/raw/main/trackfeatures.RData&quot;)) # remove duplicates tracks &lt;- na.omit(tracks[!duplicated(tracks$isrc), ]) To get an idea of how clustering might work let’s first take a look at just with two variables, energy and acousticness, and two artists, Robin Schulz and Adele. We immediately see that Adele’s songs are more to the top left (high acousticness, low energy) whereas Robin Schulz’s songs are mostly on the bottom right (low acousticness, high energy). library(tidyverse) ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ── ## ✔ dplyr 1.1.3 ✔ readr 2.1.4 ## ✔ forcats 1.0.0 ✔ tibble 3.2.1 ## ✔ lubridate 1.9.3 ✔ tidyr 1.3.0 ## ✔ purrr 1.0.2 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ psych::%+%() masks ggplot2::%+%() ## ✖ psych::alpha() masks ggplot2::alpha() ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## ✖ dplyr::src() masks Hmisc::src() ## ✖ dplyr::summarize() masks Hmisc::summarize() ## ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors library(stringr) robin_schulz &lt;- tracks[str_detect(tracks$artistName, &quot;Robin Schulz&quot;), ] robin_schulz$artist &lt;- &quot;Robin Schulz&quot; adele &lt;- tracks[str_detect(tracks$artistName, &quot;Adele&quot;), ] adele$artist &lt;- &quot;Adele&quot; example_tracks &lt;- rbind(robin_schulz, adele) ggplot(example_tracks, aes(x = energy, y = acousticness, color = artist)) + geom_point() + theme_bw() 7.2.1 K-Means One of the most popular algorithms for clustering is the K-means algorithm. The “K” stands for the number of clusters that are specified as a hyperparameter (more on how to set that parameter later). The algorithm then tries to separate the observations into K clusters such that the within-cluster sum of squared differences form the cluster mean of the features (e.g., our audio features) is minimized. Therefore, it is important to scale all variables before performing clustering such that they all contribute equally to the distance between the observations. Minimizing the within-cluster sum of squares is equivalent to minimizing the sum of the squared deviations of observations pairwise in the same cluster and maximizing the sum of squared deviations of observations in different clusters. Intuitively the algorithm groups observations by iteratively calculating the mean or center of each cluster, assigning each observation to the cluster with the closest mean and re-calculating the mean… The algorithm has “converged” (i.e., is done) when the assignments no longer change. Let’s try it out on customer data. In order to perform clustering we first have to remove all missing values from the used variables as for those we cannot calculate distances. cluster &lt;- read.csv2(&quot;data/Mall_Customers.csv&quot;, sep = &quot;,&quot;) str(cluster) ## &#39;data.frame&#39;: 200 obs. of 5 variables: ## $ CustomerID : int 1 2 3 4 5 6 7 8 9 10 ... ## $ Gender : chr &quot;Male&quot; &quot;Male&quot; &quot;Female&quot; &quot;Female&quot; ... ## $ Age : int 19 21 20 23 31 22 35 23 64 30 ... ## $ Annual.Income..k.. : int 15 15 16 16 17 17 18 18 19 19 ... ## $ Spending.Score..1.100.: int 39 81 6 77 40 76 6 94 3 72 ... cluster &lt;- cluster %&gt;% rename(Annual_Income = &quot;Annual.Income..k..&quot;, Spending_Score = &quot;Spending.Score..1.100.&quot;) cluster &lt;- na.omit(cluster[!duplicated(cluster$CustomerID), ]) If we use all observations in our data, the best value for “K” is not immediately obvious. Hence, we can use the NbClust package to determine the best number of clusters according to various indices (see ?NbClust). First we scale the variables that can be used for K-means clustering, i.e., interval- or ratio-scaled variables. Then we count how many indices would choose a certain number of clusters. The best candidate is 2 clusters, chosen by 5 indices; all other options are worse (e.g., 5, 6, or 10 clusters have 4 indices, 3 have 3 indices, etc.). # Scale the data cluster_scale &lt;- scale(cluster[, 3:5]) set.seed(123) # get the recommended number of clusters opt_K &lt;- NbClust(cluster_scale, method = &quot;kmeans&quot;, max.nc = 10) ## *** : The Hubert index is a graphical method of determining the number of clusters. ## In the plot of Hubert index, we seek a significant knee that corresponds to a ## significant increase of the value of the measure i.e the significant peak in Hubert ## index second differences plot. ## ## *** : The D index is a graphical method of determining the number of clusters. ## In the plot of D index, we seek a significant knee (the significant peak in Dindex ## second differences plot) that corresponds to a significant increase of the value of ## the measure. ## ## ******************************************************************* ## * Among all indices: ## * 5 proposed 2 as the best number of clusters ## * 3 proposed 3 as the best number of clusters ## * 2 proposed 4 as the best number of clusters ## * 4 proposed 5 as the best number of clusters ## * 4 proposed 6 as the best number of clusters ## * 1 proposed 9 as the best number of clusters ## * 4 proposed 10 as the best number of clusters ## ## ***** Conclusion ***** ## ## * According to the majority rule, the best number of clusters is 2 ## ## ## ******************************************************************* table(opt_K$Best.nc[&quot;Number_clusters&quot;, ]) ## ## 0 1 2 3 4 5 6 9 10 ## 2 1 5 3 2 4 4 1 4 Once done, we can plug in the obtained number of clusters into kmeans function: kmeans_clusters &lt;- kmeans(cluster_scale, 2) #specify the number of clusters to be created kmeans_clusters$centers ## Age Annual_Income Spending_Score ## 1 0.7071480 -0.002469258 -0.6976405 ## 2 -0.7508891 0.002621995 0.7407935 The kmeans function returns, among other statistics, the centers of each cluster and a cluster identifier for each observation which we can add to our original data. In our case one cluster’s center is rather low in age and high spending and the second one show a different pattern. To get a quick overview of the centers we can user a radar plot. This allows us to quickly observe similarities and distinguishing features of clusters. library(ggiraph) library(ggiraphExtra) centers &lt;- data.frame(kmeans_clusters$centers) centers$cluster &lt;- 1:2 ggRadar(centers, aes(color = cluster), rescale = FALSE) + ggtitle(&quot;Centers&quot;) + theme_minimal() We can use a barplot to visualize the number of customers in each cluster and color them by one of the other characteristics we know (here it’s gender): cluster$cluster_num &lt;- as.factor(kmeans_clusters$cluster) ggplot(cluster, aes(y = cluster_num, fill = Gender)) + geom_bar() + theme_minimal() table(cluster$Gender, cluster$cluster_num) ## ## 1 2 ## Female 55 57 ## Male 48 40 We can use the fviz_cluster function from the factoextra library to get a partial picture. If there are more than 2 variables used for clustering, the package performs a PCA and uses the first two principal components for the visualization. library(factoextra) fviz_cluster(kmeans_clusters, data = cluster_scale, palette = hcl.colors(2, palette = &quot;Dynamic&quot;), geom = &quot;point&quot;, ellipse.type = &quot;convex&quot;, ggtheme = theme_minimal()) We could use the clustering to identify customers similar to each other to recommend them goods or services, chosen by their statistical “twins”. For example, we want to find customers similar to the one with ID 19: customer &lt;- cluster %&gt;% filter(CustomerID == 19) %&gt;% distinct(cluster_num, .keep_all = TRUE) %&gt;% select(cluster_num) customer similar_customers &lt;- cluster %&gt;% filter(cluster_num == 1) %&gt;% select(CustomerID, Gender) head(similar_customers, 20) See how differently customers from clusters 1 and 2 locate relative to each other: random_customers &lt;- cluster %&gt;% filter(CustomerID == 19 | CustomerID == 20 | CustomerID == 21) ggplot(random_customers, aes(Annual_Income, Spending_Score, color = cluster_num)) + geom_point() + geom_label(aes(label = CustomerID), hjust = &quot;inward&quot;) + theme_bw() Learning check (LC7.1) The goals of PCA are… …to identify underlying dimensions that explain correlations among variables. …to identify multiplicative effects in a linear regression. …to identify a smaller set of uncorrelated variables. …to identify interaction terms in a linear regression. None of the above (LC7.2) What are typical hypotheses in PCA concerning how many factors will emerge? A reduction greater than 50% of the input variables Between a third and a fourth of the input variables A reduction smaller than 50% of the input variables None of the above (LC7.3) What assumptions have to be fulfilled for using PCA? Variables must be in interval or ratio scale Existence of some underlying factor structure The correlation matrix must have sufficient number of correlations Variables must be measured using ordinal scales None of the above (LC7.4) What is the correct interpretation of the b-values in the following mathematical representation concerning exploratory factor analysis (EFA)? \\(Factor_i=b_1*Variable_1 + b_2*Variable_2+…+b_nVariable_n\\) Regression coefficients Correlations between the variables Weights of a variable on a factor Factor loadings None of the above (LC7.5) What is the null hypothesis of the Bartlett’s test of sphericity? All variables are correlated in the population The correlation matrix is singular All variables are uncorrelated in the population The correlation matrix is an identity matrix None of the above (LC7.6) Before conducting PCA, how can you test the sampling adequacy of your data (i.e., how suited your data is for Factor Analysis)? Kaiser-Meyer-Olkin (KMO) test with scores &lt;0.5 Kaiser-Meyer-Olkin (KMO) test with scores &gt;0.5 By inspecting the scree plot Cronbach’s alpha test with scores &gt;0.7 None of the above (LC7.7) What is communality? Proportion of common variance in a variable Variance that is unique to a particular variable Proportion of unique variance in a variable Covariance between two factors None of the above (LC7.8) Orthogonal factor rotation assumes: Inter-correlated factors Uncorrelated factors Outer-correlated factors None of the above (LC7.9) Imagine you want to conduct a PCA on 10 variables without factor rotation and in a first step, you wish to find out how many components you should extract. How would the corresponding R Code look? principal(data, nfactors = 10, rotate = \"none\") principal(data, nfactors = “varimax”, rotate = 10) principal(data, nfactors = 10, rotate = \"oblimin\") principal(data, nfactors = 10, rotate = \"varimax\") None of the above (LC7.10) How many factors should be extracted based on the Scree Plot and the Eigenvalues below? 1 2 3 4 5 15 23 (LC7.11) Which of the following statements regarding cluster analysis are TRUE? The goal is to reduce the number of columns in a data set The goal is to reduce the number of rows in a data set H-Means is a popular clustering method The goal is to establish a relationship between a dependent variable and an independent variable The goal is to group observations into clusters such that those in the same cluster are more “similar” than those of other clusters It is an unsupervised learning method (LC7.12) What is the main objective of k-means clustering in data analysis? To find the best-fitting linear regression model. To identify underlying factors that explain correlations among variables. To partition data into clusters based on similarity. To calculate the correlation between variables. None of the above (LC7.13) In the k-means algorithm, what does “k” represent? The number of variables in the dataset. The number of clusters to be formed. The number of iterations to converge. The number of outliers in the data. None of the above (LC7.14) What is the primary limitation of the k-means clustering algorithm? It is computationally expensive. It requires specifying the number of clusters in advance. It cannot handle categorical data. It always produces spherical clusters. None of the above References Field, A., Miles J., &amp; Field, Z. (2012): Discovering Statistics Using R. Sage Publications, chapter 17 Malhotra, N. K.(2010). Marketing Research: An Applied Orientation (6th. ed.). Prentice Hall. chapter 19 "],["r-markdown.html", "8 R Markdown 8.1 Introduction to R Markdown", " 8 R Markdown 8.1 Introduction to R Markdown You can download the example markdown file here This page will guide you through creating and editing R Markdown documents. This is a useful tool for reporting your analysis (e.g. for homework assignments). Of course, there is also a cheat sheet for R-Markdown and this book contains a comprehensive discussion of the format. The following video contains a short introduction to the R Markdown format. Creating a new R Markdown document In addition to the video, the following text contains a short description of the most important formatting options. Let’s start to go through the steps of creating and .Rmd file and outputting the content to an HTML file. If an R-Markdown file was provided to you, open it with R-Studio and skip to step 4 after adding your answers. Open R-Studio Create a new R-Markdown document Save with appropriate name 3.1. Add your answers 3.2. Save again “Knit” to HTML Hand in appropriate file (ending in .html) on learn@WU Text and Equations R-Markdown documents are plain text files that include both text and R-code. Using RStudio they can be converted (‘knitted’) to HTML or PDF files that include both the text and the results of the R-code. In fact this website is written using R-Markdown and RStudio. In order for RStudio to be able to interpret the document you have to use certain characters or combinations of characters when formatting text and including R-code to be evaluated. By default the document starts with the options for the text part. You can change the title, date, author and a few more advanced options. First lines of an R-Markdown document The default is text mode, meaning that lines in an Rmd document will be interpreted as text, unless specified otherwise. Headings Usually you want to include some kind of heading to structure your text. A heading is created using # signs. A single # creates a first level heading, two ## a second level and so on. It is important to note here that the # symbol means something different within the code chunks as opposed to outside of them. If you continue to put a # in front of all your regular text, it will all be interpreted as a first level heading, making your text very large. Lists Bullet point lists are created using *, + or -. Sub-items are created by indenting the item using 4 spaces or 2 tabs. * First Item * Second Item + first sub-item - first sub-sub-item + second sub-item First Item Second Item first sub-item first sub-sub-item second sub-item Ordered lists can be created using numbers and letters. If you need sub-sub-items use A) instead of A. on the third level. 1. First item a. first sub-item A) first sub-sub-item b. second sub-item 2. Second item First item first sub-item first sub-sub-item second sub-item Second item Text formatting Text can be formatted in italics (*italics*) or bold (**bold**). In addition, you can ad block quotes with &gt; &gt; Lorem ipsum dolor amet chillwave lomo ramps, four loko green juice messenger bag raclette forage offal shoreditch chartreuse austin. Slow-carb poutine meggings swag blog, pop-up salvia taxidermy bushwick freegan ugh poke. Lorem ipsum dolor amet chillwave lomo ramps, four loko green juice messenger bag raclette forage offal shoreditch chartreuse austin. Slow-carb poutine meggings swag blog, pop-up salvia taxidermy bushwick freegan ugh poke. R-Code R-code is contained in so called “chunks”. These chunks always start with three backticks and r in curly braces ({r} ) and end with three backticks ( ). Optionally, parameters can be added after the r to influence how a chunk behaves. Additionally, you can also give each chunk a name. Note that these have to be unique, otherwise R will refuse to knit your document. Global and chunk options The first chunk always looks as follows ```{r setup, include = FALSE} knitr::opts_chunk$set(echo = TRUE) ``` It is added to the document automatically and sets options for all the following chunks. These options can be overwritten on a per-chunk basis. Keep knitr::opts_chunk$set(echo = TRUE) to print your code to the document you will hand in. Changing it to knitr::opts_chunk$set(echo = FALSE) will not print your code by default. This can be changed on a per-chunk basis. ```{r cars, echo = FALSE} summary(cars) plot(dist~speed, cars) ``` ## speed dist ## Min. : 4.0 Min. : 2.00 ## 1st Qu.:12.0 1st Qu.: 26.00 ## Median :15.0 Median : 36.00 ## Mean :15.4 Mean : 42.98 ## 3rd Qu.:19.0 3rd Qu.: 56.00 ## Max. :25.0 Max. :120.00 ```{r cars2, echo = TRUE} summary(cars) plot(dist~speed, cars) ``` summary(cars) ## speed dist ## Min. : 4.0 Min. : 2.00 ## 1st Qu.:12.0 1st Qu.: 26.00 ## Median :15.0 Median : 36.00 ## Mean :15.4 Mean : 42.98 ## 3rd Qu.:19.0 3rd Qu.: 56.00 ## Max. :25.0 Max. :120.00 plot(dist ~ speed, cars) A good overview of all available global/chunk options can be found here. LaTeX Math Writing well formatted mathematical formulas is done the same way as in LaTeX. Math mode is started and ended using $$. $$ f_1(\\omega) = \\frac{\\sigma^2}{2 \\pi},\\ \\omega \\in[-\\pi, \\pi] $$ \\[ f_1(\\omega) = \\frac{\\sigma^2}{2 \\pi},\\ \\omega \\in[-\\pi, \\pi] \\] (for those interested this is the spectral density of white noise) Including inline mathematical notation is done with a single $ symbol. ${2\\over3}$ of my code is inline. \\({2\\over3}\\) of my code is inline. Take a look at this wikibook on Mathematics in LaTeX and this list of Greek letters and mathematical symbols if you are not familiar with LaTeX. In order to write multi-line equations in the same math environment, use \\\\ after every line. In order to insert a space use a single \\. To render text inside a math environment use \\text{here is the text}. In order to align equations start with \\begin{align} and place an &amp; in each line at the point around which it should be aligned. Finally end with \\end{align} $$ \\begin{align} \\text{First equation: }\\ Y &amp;= X \\beta + \\epsilon_y,\\ \\forall X \\\\ \\text{Second equation: }\\ X &amp;= Z \\gamma + \\epsilon_x \\end{align} $$ \\[ \\begin{align} \\text{First equation: }\\ Y &amp;= X \\beta + \\epsilon_y,\\ \\forall X \\\\ \\text{Second equation: }\\ X &amp;= Z \\gamma + \\epsilon_x \\end{align} \\] Important symbols Symbol Code \\(a^{2} + b\\) a^{2} + b \\(a^{2+b}\\) a^{2+b} \\(a_{1}\\) a_{1} \\(a \\leq b\\) a \\leq b \\(a \\geq b\\) a \\geq b \\(a \\neq b\\) a \\neq b \\(a \\approx b\\) a \\approx b \\(a \\in (0,1)\\) a \\in (0,1) \\(a \\rightarrow \\infty\\) a \\rightarrow \\infty \\(\\frac{a}{b}\\) \\frac{a}{b} \\(\\frac{\\partial a}{\\partial b}\\) \\frac{\\partial a}{\\partial b} \\(\\sqrt{a}\\) \\sqrt{a} \\(\\sum_{i = 1}^{b} a_i\\) \\sum_{i = 1}^{b} a_i \\(\\int_{a}^b f(c) dc\\) \\int_{a}^b f(c) dc \\(\\prod_{i = 0}^b a_i\\) \\prod_{i = 0}^b a_i \\(c \\left( \\sum_{i=1}^b a_i \\right)\\) c \\left( \\sum_{i=1}^b a_i \\right) The {} after _ and ^ are not strictly necessary if there is only one character in the sub-/superscript. However, in order to place multiple characters in the sub-/superscript they are necessary. e.g. Symbol Code \\(a^b = a^{b}\\) a^b = a^{b} \\(a^b+c \\neq a^{b+c}\\) a^b+c \\neq a^{b+c} \\(\\sum_i a_i = \\sum_{i} a_{i}\\) \\sum_i a_i = \\sum_{i} a_{i} \\(\\sum_{i=1}^{b+c} a_i \\neq \\sum_i=1^b+c a_i\\) \\sum_{i=1}^{b+c} a_i \\neq \\sum_i=1^b+c a_i Greek letters Greek letters are preceded by a \\ followed by their name ($\\beta$ = \\(\\beta\\)). In order to capitalize them simply capitalize the first letter of the name ($\\Gamma$ = \\(\\Gamma\\)). "],["assignments-solutions.html", "9 Assignments: Solutions 9.1 Assignment 1", " 9 Assignments: Solutions 9.1 Assignment 1 This is the technical solution for the first assignment (i.e., it does not include any interpretations while your solutions should have some for each task). Also, plase note that there are multiple possible ways of solving the assignment. 9.1.1 Load data library(tidyverse) library(magrittr) sales_data &lt;- read.csv2(&quot;https://raw.githubusercontent.com/WU-RDS/RMA2024/main/data/Sales-2019-2020_A1.csv&quot;, sep = &quot;,&quot;, header = TRUE) sales_data %&lt;&gt;% mutate(date = as.Date(date, format = &quot;%m/%d/%Y&quot;)) sales_data$year &lt;- format(sales_data$date, &quot;%Y&quot;) sales_data$order_value_EUR &lt;- str_remove(sales_data$order_value_EUR, &quot;,&quot;) str(sales_data) ## &#39;data.frame&#39;: 1000 obs. of 11 variables: ## $ country : chr &quot;Sweden&quot; &quot;Finland&quot; &quot;Portugal&quot; &quot;Portugal&quot; ... ## $ order_value_EUR: chr &quot;17524.02&quot; &quot;116563.40&quot; &quot;296465.56&quot; &quot;74532.02&quot; ... ## $ cost : chr &quot;14122.61&quot; &quot;92807.78&quot; &quot;257480.34&quot; &quot;59752.32&quot; ... ## $ date : Date, format: &quot;2020-02-12&quot; &quot;2019-09-26&quot; ... ## $ category : chr &quot;Books&quot; &quot;Games&quot; &quot;Clothing&quot; &quot;Beauty&quot; ... ## $ customer_name : chr &quot;Goldner-Dibbert&quot; &quot;Hilll-Vandervort&quot; &quot;Larkin-Collier&quot; &quot;Hessel-Stiedemann&quot; ... ## $ sales_manager : chr &quot;Maxie Marrow&quot; &quot;Hube Corey&quot; &quot;Celine Tumasian&quot; &quot;Celine Tumasian&quot; ... ## $ sales_rep : chr &quot;Madelon Bront&quot; &quot;Wat Bowkley&quot; &quot;Smitty Culverhouse&quot; &quot;Aurelie Wren&quot; ... ## $ device_type : chr &quot;Mobile&quot; &quot;Mobile&quot; &quot;PC&quot; &quot;PC&quot; ... ## $ order_id : chr &quot;70-0511466&quot; &quot;28-6585323&quot; &quot;58-7703341&quot; &quot;14-6700183&quot; ... ## $ year : chr &quot;2020&quot; &quot;2019&quot; &quot;2019&quot; &quot;2020&quot; ... # head(sales_data) First, you should check the data and make sure all variables are recorded correctly (correct variable types). From the output above you can see that order_value_EUR and cost, which are measured in ratio scales, are not numeric. Hence, you need to fix this: sales_data$order_value_EUR = as.numeric(sales_data$order_value_EUR) sales_data$cost = as.numeric(sales_data$cost) head(sales_data, 2) 9.1.2 Q1 To solve the first task of Question 1, ypu could simply use the table() and arrange() functions. By storing the table as data frame, you allow for using it in the bar plot as well: prop &lt;- as.data.frame(table(sales_data$country)) prop &lt;- prop %&gt;% arrange(desc(Freq)) prop ggplot(prop, aes(x = Var1, y = Freq)) + geom_col(aes(x = fct_reorder(Var1, -Freq), fill = Freq)) + ylab(&quot;Number of transactions&quot;) + xlab(&quot;Market&quot;) + theme_minimal() + theme(axis.text.x = element_text(angle = 45, vjust = 1.1, hjust = 1)) For the next task, it was enough to use conditional frequencies by year (because this way, the audience sees the dynamics of transactions shares between markets in Y1 and Y2), or you could show the percentage changes for each market from Y1 to Y2. round(prop.table(table(select(sales_data, country, year)), 2), 3) * 100 ## year ## country 2019 2020 ## Austria 0.0 0.4 ## Belgium 0.4 0.8 ## Bulgaria 3.1 2.9 ## Denmark 0.8 1.4 ## Finland 4.5 4.3 ## France 26.1 20.4 ## Germany 2.2 2.5 ## Ireland 3.1 5.5 ## Italy 0.8 1.2 ## Luxembourg 2.7 1.8 ## Netherlands 2.2 3.3 ## Portugal 21.0 26.7 ## Spain 3.1 2.2 ## Sweden 19.2 17.3 ## UK 10.8 9.4 9.1.3 Q2 In this task, you should simply perform grouping and summarizing. The only difference is, in task 1, you only needed one grouping variable (year), while in task 2, you had to use country as well. s1 &lt;- sales_data %&gt;% group_by(year) %&gt;% summarize(annual_revenue = sum(order_value_EUR), avg_revenue = mean(order_value_EUR)) s1 s2 &lt;- sales_data %&gt;% group_by(country, year) %&gt;% summarize(annual_revenue = sum(order_value_EUR), avg_revenue = mean(order_value_EUR)) s2 &lt;- as.data.frame(s2) s2 sales_data %&gt;% filter(year == 2020) %&gt;% group_by(country) %&gt;% ggplot(aes(x = fct_reorder(country, order_value_EUR), y = order_value_EUR/1000)) + # to have nicely readable axis points geom_boxplot(coef = 3) + labs(x = &quot;Country&quot;, y = &quot;Sales (in tsd)&quot;) + theme_minimal() + theme(axis.text.x = element_text(angle = 45, vjust = 1.1, hjust = 1), plot.title = element_text(hjust = 0.5, color = &quot;#666666&quot;), legend.position = &quot;none&quot;) 9.1.4 Q3 Here are the two ways to solve Question 3: one with mutate() and another with summarize(). You can see that the second way is slightly more parsimonious. Reminder: ROI is calculated as (( total revenue - total cost) / total cost ) * 100. roi_mut &lt;- sales_data %&gt;% group_by(country) %&gt;% mutate(roi_index = (sum(order_value_EUR) - sum(cost))/sum(cost) * 100) %&gt;% arrange(desc(roi_index)) %&gt;% select(country, roi_index) %&gt;% unique() roi_mut roi_sum &lt;- sales_data %&gt;% group_by(country) %&gt;% summarize(roi_index = (sum(order_value_EUR) - sum(cost))/sum(cost) * 100) %&gt;% arrange(desc(roi_index)) roi_sum head(roi_mut, 3) tail(roi_mut, 3) 9.1.5 Q4 This task is quite similar to the prevoius one and could be solved the same way: categories &lt;- sales_data %&gt;% group_by(category) %&gt;% mutate(profit = (sum(order_value_EUR) - sum(cost))) %&gt;% arrange(desc(profit)) %&gt;% select(category, profit) %&gt;% unique() %&gt;% head(5) # could also be done with summarize() like in the previous task categories 9.1.6 Q5 The tricky part in this task could be with “share of sales” - remember that share of sales is a share of revenue, gained by the device type, against the total revenue. This is not counting (i.e., prop.table() would not be the right way to solve this). Instead, you should use the sums of revenues per device type to obtain the shares: device_sales &lt;- sales_data %&gt;% group_by(device_type) %&gt;% summarise(sales = sum(order_value_EUR)) device_sales %&gt;% mutate(share_of_sales_percentage = round(sales/sum(sales), 4) * 100) "],["group-project-1.html", "10 Group project 10.1 Data analysis 10.2 Load data 10.3 Question 1 10.4 Question 2 10.5 Question 3 10.6 Question 4", " 10 Group project Die Informationen zum Gruppenprojekt entnehmen Sie bitte der .Rmd-Datei in dem entsprechenden Assignment auf Canvas. Verwenden Sie R, um die Aufgaben zu loesen. Wenn Sie mit der Bearbeitung fertig sind, klicken Sie auf den “Knit to HTML” Button oben in der Menueleiste in R. Dadurch wird ein HTML-Dokument in dem Ordner erstellt, in dem die group_project.Rmd Datei gespeichert ist. Oeffnen Sie diese Datei im Browser um zu testen, ob der Inhalt korrekt dargestellt wird. Wenn der Output korrekt ist, reichen Sie die .html Datei ueber Learn ein. Den Dateinamen sollten sie wie folgt waehlen: “group_project_groupID.html”. Bitte denken Sie daran, die Schritte in Ihren Analysen zu beschreiben und zu begründen, sowie die Ergebnisse zu interpretieren. Es ist nicht ausreichend, die Analysen unkommentiert auszuführen. As a marketing manager of a music streaming service, you are set the task to derive insights from data using different quantitative analyses. The following variables are available to you: playlist_id: unique ID per playlist playlist_owner: curator of playlist playlist_name: name of playlist playlist_follower: number of playlist follower spotify_pl: 1 if playlist is curated by Spotify, 0 else major_pl: 1 if playlist is curated by major label, 0 else artist_pl: 1 if playlist is artist specific (e.g., “this is…” playlists), 0 else german_pl: 1 if playlist contains high share of German content, 0 else context_pl: 1 if playlist is curated according to a context (e.g., running, cooking, commuting, etc.), 0 else content_pl: 1 if playlist is curated according to content (e.g., genre), 0 else ranking_pl: 1 if playlist is ranking-based (e.g., top 100 DE charts), 0 else artist_fame: avg. artist popularity of artists on playlist avg_song_age: avg. week since release of songs on playlist n_artists_playlist: avg. number of unique artists on playlist n_songs_playlist: avg. number of unique songs on playlist new_songs_30d: avg. number of new song listings per playlist over the past 30days new_song_share: avg. share of new song listings per playlist over the past 30days n_genres: avg. number of genres that songs on a playlist are associated with major_share: avg. share of major label content n_country_rep: avg. number of countries where the songs on a given playlist come from avg_song_similariy: measure of how similar the songs on a playlist are (higher values = more similar) avg_danceability: avg. value of song feature “danceability” avg_speechiness: avg. value of song feature “speechiness” avg_valence: avg. value of song feature “valence” avg_energy: avg. value of song feature “energy” avg_loudness: avg. value of song feature “loudness” avg_liveness: avg. value of song feature “liveness” avg_acousticness: avg. value of song feature “acousticness” avg_instrumentalness: avg. value of song feature “instrumentalness” avg_tempo: avg. value of song feature “tempo” min_date: first observation date (not relevant) Please conduct the following analyses: You are interested to understand what curation decisions drive the success of playlists. TO this end, build and estimate a regression model to explain the success of playlist curators in terms of their followers. This means, the variable “playlist_follower” is your dependent variable. As independent (explanatory) variables, you may choose from the remaining variables. Decide based on appropriate criteria between a linear model specification and a non-linear model specification (i.e., using log-transformation). After you have decided for the model specification, please formally state the regression equation, visualize the relationships between the dependent variable and independent variables using appropriate plots and interpret the model coefficients. Which model variables have a significant influence on the dependent variable? How do you judge the fit of the model? Build and estimate a classification model (i.e., logistic regression) that helps you to classify different playlist types, particularly, to differentiate between playlists that are managed by Spotify and playlists that are managed by other curators. This means, the variable “spotify_pl” is your dependent variable. As independent (explanatory) variables, you can choose from the remaining variables. Please formally state the regression equation, visualize the relationship between the top10 variable and the independent variables using appropriate plots and interpret the model coefficients. Which of the independent variable help you to differentiate between the playlist types? You are interested to learn more about whether the independent variables may be reduced to a smaller set of variables. Conduct a PCA using the following variables:major_share, avg_song_age, avg_danceability, avg_speechiness, avg_valence, avg_energy, avg_loudness, avg_liveness, avg_acousticness, avg_tempo. How many components emerge and how can these be interpreted? Please conduct the tests discussed in class to see if PCA is appropriate for this data. You would like to investigate the similarity between playlists and how you could group songs into clusters using the variables: avg_song_age, avg_danceability, avg_speechiness,avg_valence,avg_energy,avg_loudness, avg_liveness, avg_acousticness, avg_tempo, avg_instrumentalness, avg_song_similarity. Conduct a cluster analysis for the playlists. Determine the optimal number of clusters first, then run the cluster analysis using k-means clustering and visualize and interpret the results accordingly. Imagine you would like to recommend a playlist to a user that is most similar to the playlist “Superior Study Playlist”. Which playlist would you recommend? When you are done with your analysis, click on “Knit to HTML” button above the code editor. This will create a HTML document of your results in the folder where the “group_project.Rmd” file is stored. Open this file in your Internet browser to see if the output is correct. If the output is correct, submit the HTML file via Canvas. The file name should be “group_assignment_group_id.html”. 10.1 Data analysis 10.2 Load data library(ggplot2) library(psych) library(dplyr) library(ggiraph) library(ggiraphExtra) library(NbClust) library(factoextra) library(GPArotation) library(data.table) options(scipen = 999) set.seed(123) playlist_data &lt;- fread(&quot;https://raw.githubusercontent.com/WU-RDS/RMA2023/main/data/playlist_data_fin.csv&quot;) names(playlist_data) ## [1] &quot;playlist_id&quot; &quot;playlist_owner&quot; &quot;playlist_name&quot; ## [4] &quot;spotify_pl&quot; &quot;major_pl&quot; &quot;artist_pl&quot; ## [7] &quot;german_pl&quot; &quot;context_pl&quot; &quot;content_pl&quot; ## [10] &quot;ranking_pl&quot; &quot;playlist_follower&quot; &quot;artist_fame&quot; ## [13] &quot;avg_song_age&quot; &quot;n_artists_playlist&quot; &quot;n_songs_playlist&quot; ## [16] &quot;new_songs_30d&quot; &quot;new_song_share&quot; &quot;n_genres&quot; ## [19] &quot;major_share&quot; &quot;n_country_rep&quot; &quot;avg_song_similarity&quot; ## [22] &quot;avg_danceability&quot; &quot;avg_speechiness&quot; &quot;avg_valence&quot; ## [25] &quot;avg_energy&quot; &quot;avg_loudness&quot; &quot;avg_liveness&quot; ## [28] &quot;avg_acousticness&quot; &quot;avg_instrumentalness&quot; &quot;avg_tempo&quot; ## [31] &quot;min_date&quot; 10.3 Question 1 Provide a description of your steps here! # provide your code here Interpret the results here! 10.4 Question 2 Provide a description of your steps here! # provide your code here Interpret the results here! 10.5 Question 3 Provide a description of your steps here! # provide your code here Interpret the results here! 10.6 Question 4 Provide a description of your steps here! # provide your code here Interpret the results here! "],["faq.html", "11 FAQ 11.1 Common error messages 11.2 Installation of R packages 11.3 Issues with statistics and data 11.4 Errors related to specific methods 11.5 General settings and options 11.6 Data visualization/output issues 11.7 Issues with functions and function arguments 11.8 Issues with R Markdown 11.9 New questions", " 11 FAQ In this section, we provide answers to questions that students of previous cohorts encountered. We grouped the answers by topic and hope that you will find the answers useful. 11.1 Common error messages A general note on error messages We usually load data into a data.frame in our R Session (e.g., from a CSV file using data &lt;- read.csv(\"file.csv\")). It is important to note that this data.frame is not the original data file, but just a copy of the file that is stored on the hard drive. This means that any changes we make to the data.frame are not persistent/permanent and are not written to the original file (unless it is overwritten explicitly by using e.g., write.csv(data, \"file.csv\"), which we usually don’t do). Therefore, it is important to write all commands in an R/Rmd file such that we can re-run the analysis the next time we open R and reproduce the results. This also means that if you cannot solve an issue using the suggested solutions to specific error messages mentioned on this page, it is completely safe to restart R or delete variables from the Global Environment. You just have to re-run our code to get the variables and results back. Therefore, your code files should always be fully reproducible using only the R/Rmd and data files. In addition your R/Rmd files should run linearly from the first to the last line and should not depend on “jumping” back and forth. The files that you obtain from us from this course are examples of reproducible files and in case you a stuck with a problem at a certain point, you can just save the code file and run it again up until the point where you were before the error occurred. This means that a general procedure for dealing with errors that cannot be solved in any other way would be as follows: Save your code file and restart your R Session (Session -&gt; Restart R in RStudio) Go back to the beginning of your code file and run it line by line (Ctrl-Enter in RStudio) If your error persists check the affected line for typos/differences in spelling. If the error occurs in a function make sure you are passing the arguments correctly (see help file for the function using ?FUNCTIONNAME) Look at all the variables in your Global Environment and make sure they are in the format you expect them to be (e.g., if a file you expect to be a data frame is really a specified as a data frame). See the list of common error messages for more explanations below Nothing helped: Ask in the forum. If possible with a screenshot that explains your issue, or - better yet - a minimal reproducible example. In the following capitalized words are stand-ins for specific calls/symbols/functions. Error in file(file, “rt”): cannot open the connection This error message sometimes has an additional warning: In addition: Warning message: In file(file, &quot;rt&quot;) : cannot open file &#39;FILE&#39;: No such file or directory This error occurs either when a file name is not spelled correctly or the file is not in the directory where R is looking for it. You can check the directory R is looking at by executing the function getwd(). To set a new directory use setwd(DIRECTORY). Note that you cannot just paste a path from Windows Explorer to setwd since the directory has to be in the format: setwd(\"C:/Users/USERNAME/Documents\") but Windows Explorer uses C:\\Users\\USERNAME\\Documents (i.e., change \\ to / in R when specifying the path) Alternatively, you can set the directory in RStudio under Session -&gt; Set Working Directory. Here “To Source File Location” will set the directory to wherever the currently open R file is stored. Error: unexpected ‘SYMBOL’ in “CALL” Usually the unexpectes SYMBOL message is due to parentheses not being matched but it could also be any other symbol that R cannot interpret in the given context. Please check the line in which the error occurred for typos (especially too many/ too few symbols). Some common examples are: print(&quot;hello&quot;)) ## Error: &lt;text&gt;:1:15: unexpected &#39;)&#39; ## 1: print(&quot;hello&quot;)) ## ^ There is one too many closing parenthesis here. 1 +/ 2 # Too many symbols ## Error: &lt;text&gt;:1:4: unexpected &#39;/&#39; ## 1: 1 +/ ## ^ The “/” symbol may not follow the “+” symbol without any additional objects. 1 2 # Missing symbol ## Error: &lt;text&gt;:1:3: unexpected numeric constant ## 1: 1 2 ## ^ The sequence with a space between numbers is not recognized by R. x &lt;- 3 2x # Missing symbol ## Error: &lt;text&gt;:2:2: unexpected symbol ## 1: x &lt;- 3 ## 2: 2x ## ^ If you wanted to multiply x by 2 you would need to a the “*” symbol, as the following example shows. 2*x ## [1] 6 Error in CALL : object of type ‘closure’ is not subsettable This error occurs usually when one tries to subset a function (either with fun$element or fun[1] where fun is a function). Check your variable (especially data.frames) names for typos! This happens when you run the following code, for example, since mean is a function: # Does not work: means &lt;- data.frame(value = c(1,2,3)) mean$value ## Error in mean$value: object of type &#39;closure&#39; is not subsettable instead of (i.e., correcting for the missing “s” to identify the data frame by its name) # Works: means &lt;- data.frame(value = c(1,2,3)) means$value ## [1] 1 2 3 or we give variables the same name as a function (which should generally be avoided) but have not created that variable yet: summary(aov)[[1]] ## Error in object[[i]]: object of type &#39;closure&#39; is not subsettable Make sure all the relevant code is run first: dat &lt;- data.frame(value = c(1,2,3), group = c(&quot;a&quot;, &quot;b&quot;, &quot;b&quot;)) aov &lt;- aov(value~group, dat) summary(aov)[[1]] Error in CALL_WITH_$: $ operator is invalid for atomic vectors This error occurs when we try to subset a vector using the $ operator. Usually this occurs when we think an object is a data.frame with the variable in it but it is really a vector. x &lt;- c(1,2,3) x$a ## Error in x$a: $ operator is invalid for atomic vectors xdf &lt;- data.frame(a = x) xdf$a ## [1] 1 2 3 Note that this error can also occur as part of function calls when some variables are NA: library(psych) xdf$group &lt;- NA mean(xdf$a, xdf$group) ## Error in mean.default(xdf$a, xdf$group): &#39;trim&#39; must be numeric of length one When you get this error make sure your data is in the format you expect it to be (e.g., using the str function). And that missing values (i.e., NA) are handles appropriately. str(xdf) ## &#39;data.frame&#39;: 3 obs. of 2 variables: ## $ a : int 1 2 3 ## $ group: logi NA NA NA Error in CALL: object ‘NAME’ not found This error occurs whenever you pass a variable name that is not assigned to some function. The error is for example: Error in plot(x): object 'x' not found If you just enter a variable name that is not assigned it looks like this: Error: object 'NAME' not found Check your code for typos and make sure you have run all the relevant lines of code before the one in which the error occurs! Error in CALL: could not find function “FUNCTION” This error occurs if a function name is either misspelled or some packages have not been loaded into the current session (library(PACKAGENAME)). You have to re-load all packages every time you restart R. For example if you did not load the ggplot2 library but try to use the ggplot function: ## Error in detach(package:ggplot2): invalid &#39;name&#39; argument ggplot(data) ## Error in ggplot(data): could not find function &quot;ggplot&quot; If you are not sure which package provides a given function, try running: ??FUNCTION with two ?? this will search the help files of all installed packages for FUNCTION (e.g., ??ggplot). Error in CALL: incorrect number of dimension This error occurs when subsetting an object with the wrong number of dimension. For example if we have a vector x and try to get an element in the second dimension: x &lt;- c(1,2,3) x[1,1] ## Error in x[1, 1]: incorrect number of dimensions x[1] ## [1] 1 Note that data.frames have two dimensions (each variable is a column, each observation a row) even if there is only one variable: x &lt;- c(1,2,3) data.frame(x)[1, 1] ## [1] 1 For multidimensional objects you can always check the size of each dimension using the dim function: dim(data.frame(x)) ## [1] 3 1 For vectors dim will return NULL. 11.2 Installation of R packages What are the different ways to install R packages? There are multiple ways to install a package: Enter install.packages(\"PACKAGENAME\") in the console (attention: the name of the package needs to be in quotation marks) Go to the “Packages” pane in RStudio (lower right by default), then click on “Install”, then enter the package name Using the two methods above would load the package from the official R server, the so-called Comprehensive R Archive Network (CRAN). There may be instances when you would like to install packages from other sources. This could be the case, for example, when a package is not available for the version of R that you are using. Sometimes an new version of R is released and some packages may require updating to be compatible with this new version. The updating process on the official server may take some time and usually the most recent version of a package are available from other sources, such as GitHub. Using the devtools package, you can install packages from GitHub directly: devtools::install_github(repo = \"USERNAME/PACKAGENAME\"). Of course, this requires the devtools package to be installed already; i.e., you need to run install.packages(\"devtools\") first, if the devtools package is not installed yet. I cannot install packages due to “Error in contrib.url(repos,”source”)” or “Warning message: package ‘PACKAGENAME’ is not available for this version of R” Try adding the repo argument to the install.packages command as in the following example: install.packages(&quot;PACKAGENAME&quot;, repo=&quot;https://cloud.r-project.org/&quot;) Try to install the package from GitHub directly using the devtools package. For example to get the devtools package and install the ggstatsplot package from the GitHub-user IndrajeetPatil run the following code: install.packages(&quot;devtools&quot;) devtools::install_github( repo = &quot;IndrajeetPatil/ggstatsplot&quot;, # package path on GitHub (&quot;username/packagename&quot;) dependencies = TRUE, # installs packages which ggstatsplot depends on upgrade_dependencies = TRUE # updates any out of date dependencies ) In case you are using knitting process of a R Markdown file, you should not install any packages from within the markdown file. Instead, install the packages first using a plain R script file and then load the package within the markdown file before clicking knit to compile the document. Some libraries with graphical output (e.g., summarytools, magick) fail to install/load properly on MacOS Some libraries require the XQuartz window system for MacOS. After installing XQuartz please restart your computer. If you get an error message including a message about the magick package try install.packages(\"magick\") and if that fails devtools::install_github(\"ropensci/magick\", dependencies = TRUE) might help. I cannot install some packages on MacOS When asked whether R should try to install a package from sources a package which needs compilation, enter “no” or “n”. If this doesn’t solve the issue, try installing the free XCode package from the Apple Appstore, open a Terminal and enter “xcode-select –install”. After that try to install the package again (answering “yes” to the question above). 11.3 Issues with statistics and data Why does a multi-item scale lead to increased reliability? “When combining several items into a scale, random error that is inherent in every item is averaged out, which leads to increased levels of reliability”. There could, for example, be individual-level differences in the interpretation of certain items. When you have multiple items measuring the same underlying construct, these differences will average out. See Diamantopoulos, Sarstedt, Fuchs, et al. (2012) Why can demeaning/standardization lead to missing values? Calculating statistics (e.g., mean, sd) using variables that include NAs will return an NA by default. There are a couple of options to address this problem. The missing values can be deleted from a variable using the na.omit function. Alternatively many functions offer the na.rm argument which will calculate the statistic disregarding NAs. For example, the following will result in NA: x &lt;- c(1,2,3, 5, NA) mean(x) ## [1] NA While the following disregards the NA values when computing the mean of the numeric vector: x &lt;- c(1,2,3, 5, NA) mean(x, na.rm = TRUE) ## [1] 2.75 Note that the scale function automatically omits missing values when calculating the mean and standard deviation to standardize a variable. See missings.R for a sample script. The confidence interval (CI) of the mean seems very small compared to the dispersion of my sample. Can this be correct? The confidence interval of the mean depends on both the variance of the variable and the the sample size: \\[ \\sigma_{\\bar x} = \\frac{\\sigma}{\\sqrt{n}} \\] Therefore even if the standard deviation of the data (\\(\\sigma\\)) is large, we can get a narrow CI if we have a relatively large sample size. See confidenceinterval.R for a simulation study. 11.4 Errors related to specific methods Logistic regression When using logistic regression: Error in eval(family$initialize) : y values must be 0 &lt;= y &lt;= 1 When using logistic regression make sure all the values in the dependent variable (left hand side) are between \\(0\\) and \\(1\\) dat &lt;- data.frame(y = c(2,0,1), x = c(1,2,3)) glm(y ~ x , data = dat, family=binomial()) ## Error in eval(family$initialize): y values must be 0 &lt;= y &lt;= 1 When using logistic regression: Error in weights * y : non-numeric argument to binary operator This error occurs if a variable that is supposed to be numeric is a character dat$char_y &lt;- c(&quot;1&quot;, &quot;0&quot;, &quot;1&quot;) glm(char_y ~ x , data = dat, family=binomial()) ## Error in weights * y: non-numeric argument to binary operator 11.5 General settings and options Numbers are formatted weirdly By default R uses scientific notation for very large and very small numbers. We can control this behavior using options(scipen=...) where larger positive numbers will result in a wider range of values being printed in fixed notation (i.e., all digits) and negative numbers will result in more numbers being printed in scientific notation. From ?options we get: scipen: integer. A penalty to be applied when deciding to print numeric values in fixed or exponential notation. Positive values bias towards fixed and negative towards scientific notation: fixed notation will be preferred unless it is more than scipen digits wider. Scientific notation follows the following rule: \\(VeD \\Rightarrow V \\times 10^D\\). Therefore, options(scipen=-10) would result in: options(scipen=-10) 29.3749592384 ## [1] 2.937496e+01 And options(scipen=10) would result in: options(scipen=10) 29.3749592384 ## [1] 29.37496 See also scientificnotation.R for some examples. Note that options(digits=...) also allows you to control the number of digits to be displayed for numeric values: options(digits = 12) 29.3749592384 ## [1] 29.3749592384 11.6 Data visualization/output issues How can the geom colors in a ggplot be changed? In general, there are two types of colors that can be changed. The color argument changes the line or border color (e.g., in a bar chart). The fill argument changes the filling color of a plot that has a rectangle-like area (e.g., barplot, histogram, boxplot) that can be filled but does nothing for e.g., line plots. Colors can be specified either as an argument to the ggplot or geom* call directly as in ggplot(data, aes(x = Genre, y = Freq), color = c(\"red\", \"green\",...)) or as part of the aesthetics (aes). In the latter case, colors will automatically be assigned and a legend added if a categorical variable is provided as in ggplot(table_plot_rel, aes(x = Genre,y = Freq, fill = Genre)) + geom_col(). This is not to be confused with setting background/text colors as part of a theme. Themes can either be provided by a package (e.g., library(ggthemes)) or created by hand. See ggplotcolors.R for a sample script. Why are some histograms displayed differently? When plotting a histogram, there is an important parameter called binwidth which controls the range over which the number of observations are counted in each bin. If it is set to a too low value, each bin will only have very few observations and we get a large number of bins. If it is set to a value that is too high, we lump many observations together and get very few bins (in the extreme case only one). You may have to play around with different values to find the appropriate binwidth for your plot. See histogrambins.R for some examples. Some labels in plots are cut off. How can I extend the plot margins? If axis labels (e.g. names) are too long, they are cut off by the default margins of R plots. You can set margins manually in ggplot2 as part of the theme settings in the following order: top, right, bottom, left. For example, to add 2cm margin to each side, we can use: my_ggplot + theme(plot.margin = margin(2, 2, 2, 2, &quot;cm&quot;)) In addition you can try to change the height and width when saving a ggplot (this usually works better): ggsave(&quot;myggplot.png&quot;, width = 10, height = 10, units = &quot;cm&quot;) Within an Rmd document you can set the with and height as part of the code chunk options using e.g., fig.width=10, fig.height=10 (see also here) 11.7 Issues with functions and function arguments Generally, if you face an issue relating to a particular function, it is a good idea to check the details of a function, by typing ?FUNCTION (e.g., ?mean) and read the help file. Problems with factor and as.factor 1. Common mistake: some groups are not named in levels and labels \\(\\Rightarrow\\) results in NA for omitted group like in the following example: x &lt;- c(0,0,0,1,0,2,0,1) x &lt;- factor(x, levels = c(0,1), labels = c(&quot;no&quot;,&quot;yes&quot;)) x ## [1] no no no yes no &lt;NA&gt; no yes ## Levels: no yes In this example, you need to also consider “2” as a factor level to avoid setting the value of this observation to NA: x &lt;- c(0,0,0,1,0,2,0,1) x &lt;- factor(x, levels = c(0,1,2), labels = c(&quot;no&quot;,&quot;yes&quot;,&quot;maybe&quot;)) x ## [1] no no no yes no maybe no yes ## Levels: no yes maybe 2. Common mistake: the code creating the factor is run twice overwriting the original variable \\(\\Rightarrow\\) results in NA for all values like in the following example: x &lt;- c(0,0,0,1,0,2,0,1) x &lt;- factor(x, levels = c(0,1,2), labels = c(&quot;no&quot;,&quot;yes&quot;,&quot;maybe&quot;)) x &lt;- factor(x, levels = c(0,1,2), labels = c(&quot;no&quot;,&quot;yes&quot;,&quot;maybe&quot;)) x ## [1] &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## Levels: no yes maybe As can be seen, by running the line of code specifying the factor variable twice, we first specify the factor variable correctly and then incorrectly overwrite this variable again. The second time we run the code, the values are set to NA because R looks from levels of 0,1, and 2 again, but these had already been replaced by the labels when the code was run for the first time. Hence, since there are no elements with the values of 0,1,2 anymore, these values are replaced by missing values. Note that this is usually a result of “jumping” back and forth in the code. Run your script from the top and make sure you do not create the factor twice. The second time the original levels do not exist anymore and thus all resulting values are missing without warning or error. Possible remedy: name the factor variable you create differently from the source variable, e.g., x &lt;- c(0,0,0,1,0,2,0,1) y &lt;- factor(x, levels = c(0,1,2), labels = c(&quot;no&quot;,&quot;yes&quot;,&quot;maybe&quot;)) y ## [1] no no no yes no maybe no yes ## Levels: no yes maybe In this case, you can always go back and re-run the code creating the factor variable from its original source if you have overwritten it accidentally. Otherwise you would need to re-run the entire code to get the original formating of the variable back. 3. Common mistake: converting from factor to integer/numeric directly: Internally factors are stored in increasing integers starting at \\(1\\) each attached with a label. If we create a factor from an integer variable and then convert it back, this behavior might be surprising. Possible remedy: convert to character first since this will use the labels as values See factors.R for examples of each of the mistakes and remedies. How can I find an explanation of the output of a function? See the Value section of the help file for the function. You can get the help file by calling: ?FUNCTION e.g., ?lm or ?mean If the function is provided by a package you have to load the package first using the library(...) function. e.g., library(Hmisc) ?rcorr What does the MARGIN argument do? Some functions such as apply and prop.table take a MARGIN argument. This argument specifies over which dimension (e.g., rows = 1, columns = 2) a function should be applied. This is especially useful for multidimensional arrays such as matrices. m &lt;- matrix(1:9, nrow=3) m ## [,1] [,2] [,3] ## [1,] 1 4 7 ## [2,] 2 5 8 ## [3,] 3 6 9 e.g. we could get the max of each row with apply(m, 1, max) ## [1] 7 8 9 and the max of each column with apply(m, 2, max) ## [1] 3 6 9 See margins.R for more examples. 11.8 Issues with R Markdown I get an error when knitting to PDF but it works for HTML Try installing the tinytex library as follows before kniting your document: install.packages(&#39;tinytex&#39;) tinytex::install_tinytex() I am not sure where R-code, LaTeX math, and text go In an Rmd document, there are 3 different environments, 1. R-code is enclosed in three ticks followed by {r, chunk-options} where the chunk options can include configuration for printing code and output as well as figures e.g. print(&quot;Hello R!&quot;) ## [1] &quot;Hello R!&quot; 2. LaTeX math can either be enclosed in single dollar signs $x^2$ \\(\\Rightarrow x^2\\) for in-line math or in double dollar signs to put the math output on its own line $$ x^2 $$ \\[ x^2 \\] For aligned multi-line equations we can add \\begin{aligned} and \\end{aligend}. The equations will be aligend at the &amp; and a line is ended with \\\\. $$ \\begin{aligned} x &amp;= 1 \\\\ y &amp;= 2 \\\\ z = &amp;3 \\end{aligned} $$ \\[ \\begin{aligned} x &amp;= 1 \\\\ y &amp;= 2 \\\\ z = &amp;3 \\end{aligned} \\] 3. Regular text goes anywhere between those environments 11.9 New questions Couldn’t find an answer to your question? In this case, you may use the forum on Learn@wu to ask your question. We regularly update this section of the website and will include answers to new questions as they come up. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
